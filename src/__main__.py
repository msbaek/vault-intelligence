#!/usr/bin/env python3
"""
Vault Intelligence System V2 - Main Entry Point

CLI ì¸í„°í˜ì´ìŠ¤ì™€ ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import argparse
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.core.sentence_transformer_engine import SentenceTransformerEngine
    from src.core.embedding_cache import EmbeddingCache
    from src.core.vault_processor import VaultProcessor
    from src.features.advanced_search import AdvancedSearchEngine, SearchQuery
    from src.features.duplicate_detector import DuplicateDetector
    from src.features.topic_collector import TopicCollector
    from src.features.topic_analyzer import TopicAnalyzer
    import yaml
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    print("í•„ìˆ˜ ì˜ì¡´ì„±ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    DEPENDENCIES_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_config(config_path: str = None) -> dict:
    """ì„¤ì • íŒŒì¼ ë¡œë”©"""
    if config_path is None:
        config_path = project_root / "config" / "settings.yaml"
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"ì„¤ì • íŒŒì¼ ë¡œë”© ì™„ë£Œ: {config_path}")
        return config
    except Exception as e:
        logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        return {}


def check_dependencies() -> bool:
    """í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸"""
    print("ğŸ” ì˜ì¡´ì„± í™•ì¸ ì¤‘...")
    
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ í•µì‹¬ ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    missing_deps = []
    
    # ê¸°íƒ€ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    try:
        import numpy
        print(f"âœ… NumPy: {numpy.__version__}")
    except ImportError:
        missing_deps.append("numpy")
    
    try:
        import sklearn
        print(f"âœ… Scikit-learn: {sklearn.__version__}")
    except ImportError:
        missing_deps.append("scikit-learn")
    
    try:
        import yaml
        print(f"âœ… PyYAML ì‚¬ìš© ê°€ëŠ¥")
    except ImportError:
        missing_deps.append("pyyaml")
    
    if missing_deps:
        print(f"âŒ ëˆ„ë½ëœ ì˜ì¡´ì„±: {', '.join(missing_deps)}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print(f"pip install {' '.join(missing_deps)}")
        return False
    
    print("âœ… ëª¨ë“  ì˜ì¡´ì„±ì´ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
    return True


def initialize_system(vault_path: str, config: dict) -> bool:
    """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    print("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    
    try:
        # Vault ê²½ë¡œ í™•ì¸
        vault_path = Path(vault_path)
        if not vault_path.exists():
            print(f"âŒ Vault ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {vault_path}")
            return False
        
        print(f"ğŸ“ Vault ê²½ë¡œ: {vault_path}")
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        cache_dir = project_root / "cache"
        cache_dir.mkdir(exist_ok=True)
        print(f"ğŸ’¾ ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
        models_dir = project_root / "models"
        models_dir.mkdir(exist_ok=True)
        print(f"ğŸ¤– ëª¨ë¸ ë””ë ‰í† ë¦¬: {models_dir}")
        
        # Sentence Transformer ì—”ì§„ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        print("ğŸ§  Sentence Transformer ì—”ì§„ í…ŒìŠ¤íŠ¸ ì¤‘...")
        engine = SentenceTransformerEngine(
            model_name=config.get('model', {}).get('name', 'paraphrase-multilingual-mpnet-base-v2'),
            cache_dir=str(models_dir),
            device=config.get('model', {}).get('device')
        )
        
        # í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        test_text = "í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"
        test_embedding = engine.encode_text(test_text)
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›: {len(test_embedding)}")
        
        # ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("ğŸ’¾ ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        cache = EmbeddingCache(str(cache_dir))
        cache_stats = cache.get_statistics()
        print(f"ğŸ“Š ìºì‹œ í†µê³„: {cache_stats['total_embeddings']}ê°œ ì„ë² ë”©")
        
        # Vault í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        print("ğŸ“š Vault í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì¤‘...")
        processor = VaultProcessor(
            str(vault_path),
            excluded_dirs=config.get('vault', {}).get('excluded_dirs'),
            file_extensions=config.get('vault', {}).get('file_extensions')
        )
        
        vault_stats = processor.get_vault_statistics()
        print(f"ğŸ“ˆ Vault í†µê³„: {vault_stats['total_files']}ê°œ íŒŒì¼, "
              f"{vault_stats['total_size_mb']}MB")
        
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.exception("ì´ˆê¸°í™” ì¤‘ ìƒì„¸ ì˜¤ë¥˜:")
        return False


def run_tests():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    success_count = 0
    total_tests = 3
    
    # ì—”ì§„ í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£ Sentence Transformer ì—”ì§„ í…ŒìŠ¤íŠ¸")
    try:
        from src.core.sentence_transformer_engine import test_engine
        if test_engine():
            success_count += 1
    except Exception as e:
        print(f"ì—”ì§„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ìºì‹œ í…ŒìŠ¤íŠ¸
    print("\n2ï¸âƒ£ ì„ë² ë”© ìºì‹œ í…ŒìŠ¤íŠ¸")
    try:
        from src.core.embedding_cache import test_cache
        if test_cache():
            success_count += 1
    except Exception as e:
        print(f"ìºì‹œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ Vault í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸")
    try:
        from src.core.vault_processor import test_processor
        if test_processor():
            success_count += 1
    except Exception as e:
        print(f"í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {success_count}/{total_tests} í†µê³¼")
    return success_count == total_tests


def show_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ"""
    print("â„¹ï¸ Vault Intelligence System V2")
    print("=" * 50)
    print(f"í”„ë¡œì íŠ¸ ê²½ë¡œ: {project_root}")
    print(f"Python ë²„ì „: {sys.version}")
    
    try:
        import torch
        device = "CUDA" if torch.cuda.is_available() else "CPU"
        print(f"PyTorch ì¥ì¹˜: {device}")
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    except:
        print("PyTorch: ì‚¬ìš© ë¶ˆê°€ (TF-IDF ëª¨ë“œ)")
    
    print("\nğŸ“‹ Phase 2 ì™„ë£Œ ê¸°ëŠ¥:")
    print("- TF-IDF ê¸°ë°˜ ì„ë² ë”© (ì„ì‹œ êµ¬í˜„)")
    print("- ê³ ê¸‰ ê²€ìƒ‰ (ì˜ë¯¸ì /í‚¤ì›Œë“œ/í•˜ì´ë¸Œë¦¬ë“œ)")
    print("- ì¤‘ë³µ ë¬¸ì„œ ê°ì§€")
    print("- ì£¼ì œë³„ í´ëŸ¬ìŠ¤í„°ë§")
    print("- ë¬¸ì„œ ìˆ˜ì§‘ ë° í†µí•©")
    print("- SQLite ê¸°ë°˜ ì˜êµ¬ ìºì‹±")
    print()
    print("ğŸ“š ë¬¸ì„œ:")
    print("- ì‚¬ìš©ì ê°€ì´ë“œ: docs/USER_GUIDE.md")
    print("- ì‹¤ì „ ì˜ˆì œ: docs/EXAMPLES.md")
    print()
    print("âš¡ ë¹ ë¥¸ ì‹œì‘:")
    print("  python -m src search --query 'TDD'")
    print("  python -m src collect --topic 'ë¦¬íŒ©í† ë§'")
    print("  python -m src duplicates")


def run_search(vault_path: str, query: str, top_k: int, threshold: float, config: dict):
    """ê²€ìƒ‰ ì‹¤í–‰"""
    try:
        print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
        results = search_engine.hybrid_search(query, top_k=top_k, threshold=threshold)
        
        if not results:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"\nğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
        print("-" * 80)
        
        for result in results:
            print(f"{result.rank}. {result.document.title}")
            print(f"   ê²½ë¡œ: {result.document.path}")
            print(f"   ìœ ì‚¬ë„: {result.similarity_score:.4f}")
            print(f"   íƒ€ì…: {result.match_type}")
            if result.matched_keywords:
                print(f"   í‚¤ì›Œë“œ: {', '.join(result.matched_keywords)}")
            if result.snippet:
                print(f"   ë‚´ìš©: {result.snippet[:100]}...")
            print()
        
        return True
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return False


def run_duplicate_detection(vault_path: str, config: dict):
    """ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ ì‹¤í–‰"""
    try:
        print("ğŸ” ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ì¤‘ë³µ ê°ì§€ê¸° ì´ˆê¸°í™”
        detector = DuplicateDetector(search_engine, config)
        
        # ì¤‘ë³µ ë¶„ì„ ìˆ˜í–‰
        analysis = detector.find_duplicates()
        
        print(f"\nğŸ“Š ì¤‘ë³µ ë¶„ì„ ê²°ê³¼:")
        print("-" * 50)
        print(f"ì „ì²´ ë¬¸ì„œ: {analysis.total_documents}ê°œ")
        print(f"ì¤‘ë³µ ê·¸ë£¹: {analysis.get_group_count()}ê°œ")
        print(f"ì¤‘ë³µ ë¬¸ì„œ: {analysis.duplicate_count}ê°œ")
        print(f"ê³ ìœ  ë¬¸ì„œ: {analysis.unique_count}ê°œ")
        print(f"ì¤‘ë³µ ë¹„ìœ¨: {analysis.get_duplicate_ratio():.1%}")
        
        if analysis.duplicate_groups:
            print(f"\nğŸ“‹ ì¤‘ë³µ ê·¸ë£¹ ìƒì„¸:")
            for group in analysis.duplicate_groups[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                print(f"\nê·¸ë£¹ {group.id}:")
                print(f"  ë¬¸ì„œ ìˆ˜: {group.get_document_count()}ê°œ")
                print(f"  í‰ê·  ìœ ì‚¬ë„: {group.average_similarity:.4f}")
                for doc in group.documents:
                    print(f"    - {doc.path} ({doc.word_count}ë‹¨ì–´)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì¤‘ë³µ ê°ì§€ ì‹¤íŒ¨: {e}")
        return False


def run_topic_collection(vault_path: str, topic: str, top_k: int, threshold: float, output_file: str, config: dict):
    """ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ì§‘ ì‹¤í–‰"""
    try:
        print(f"ğŸ“š ì£¼ì œ '{topic}' ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ì£¼ì œ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        collector = TopicCollector(search_engine, config)
        
        # ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ì§‘
        collection = collector.collect_topic(
            topic=topic,
            top_k=top_k,
            threshold=threshold,
            output_file=output_file
        )
        
        print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print("-" * 50)
        print(f"ì£¼ì œ: {collection.metadata.topic}")
        print(f"ìˆ˜ì§‘ ë¬¸ì„œ: {collection.metadata.total_documents}ê°œ")
        print(f"ì´ ë‹¨ì–´ìˆ˜: {collection.metadata.total_word_count:,}ê°œ")
        print(f"ì´ í¬ê¸°: {collection.metadata.total_size_bytes / 1024:.1f}KB")
        
        if collection.metadata.tag_distribution:
            print(f"\nğŸ·ï¸ íƒœê·¸ ë¶„í¬:")
            for tag, count in sorted(collection.metadata.tag_distribution.items(), 
                                   key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {tag}: {count}ê°œ")
        
        if collection.metadata.directory_distribution:
            print(f"\nğŸ“ ë””ë ‰í† ë¦¬ ë¶„í¬:")
            for dir_path, count in sorted(collection.metadata.directory_distribution.items(), 
                                       key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {dir_path}: {count}ê°œ")
        
        if output_file:
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì£¼ì œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return False


def run_topic_analysis(vault_path: str, config: dict):
    """ì£¼ì œ ë¶„ì„ ì‹¤í–‰"""
    try:
        print("ğŸ” ì£¼ì œ ë¶„ì„ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ì£¼ì œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = TopicAnalyzer(search_engine, config)
        
        # ì£¼ì œ ë¶„ì„ ìˆ˜í–‰
        analysis = analyzer.analyze_topics()
        
        print(f"\nğŸ“Š ì£¼ì œ ë¶„ì„ ê²°ê³¼:")
        print("-" * 50)
        print(f"ë¶„ì„ ë¬¸ì„œ: {analysis.total_documents}ê°œ")
        print(f"ë°œê²¬ ì£¼ì œ: {analysis.topic_count}ê°œ")
        print(f"í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•: {analysis.clustering_method}")
        
        if analysis.topics:
            print(f"\nğŸ·ï¸ ì£¼ìš” ì£¼ì œë“¤:")
            for topic in analysis.topics[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                print(f"\nì£¼ì œ {topic.id}: {topic.name}")
                print(f"  ë¬¸ì„œ ìˆ˜: {topic.document_count}ê°œ")
                print(f"  ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(topic.keywords[:5])}")
                if topic.description:
                    print(f"  ì„¤ëª…: {topic.description[:100]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return False


def run_reindex(vault_path: str, force: bool, config: dict):
    """ì „ì²´ ì¬ì¸ë±ì‹± ì‹¤í–‰"""
    try:
        print("ğŸ”„ ì „ì²´ ì¬ì¸ë±ì‹± ì‹œì‘...")
        if force:
            print("âš ï¸ ê°•ì œ ëª¨ë“œ: ê¸°ì¡´ ìºì‹œë¥¼ ë¬´ì‹œí•˜ê³  ëª¨ë“  ë¬¸ì„œë¥¼ ì¬ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        # ì§„í–‰ë¥  í‘œì‹œ í•¨ìˆ˜
        def progress_callback(current, total):
            percentage = (current / total) * 100
            print(f"ğŸ“Š ì§„í–‰ë¥ : {current}/{total} ({percentage:.1f}%)")
        
        # ì¸ë±ìŠ¤ êµ¬ì¶• (force_rebuild ì˜µì…˜ ì‚¬ìš©)
        print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        success = search_engine.build_index(
            force_rebuild=force, 
            progress_callback=progress_callback
        )
        
        if success:
            stats = search_engine.get_search_statistics()
            print(f"\nâœ… ì¬ì¸ë±ì‹± ì™„ë£Œ!")
            print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
            print(f"  - ì¸ë±ì‹±ëœ ë¬¸ì„œ: {stats['indexed_documents']:,}ê°œ")
            print(f"  - ì„ë² ë”© ì°¨ì›: {stats['embedding_dimension']}ì°¨ì›")
            print(f"  - ìºì‹œëœ ì„ë² ë”©: {stats['cache_statistics']['total_embeddings']:,}ê°œ")
            print(f"  - Vault í¬ê¸°: {stats['vault_statistics']['total_size_mb']:.1f}MB")
            return True
        else:
            print("âŒ ì¬ì¸ë±ì‹± ì‹¤íŒ¨!")
            return False
        
    except Exception as e:
        print(f"âŒ ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Vault Intelligence System V2 - Sentence Transformers ê¸°ë°˜ ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œìŠ¤í…œ"
    )
    
    parser.add_argument(
        "command",
        choices=["init", "test", "info", "search", "duplicates", "collect", "analyze", "reindex"],
        help="ì‹¤í–‰í•  ëª…ë ¹ì–´"
    )
    
    parser.add_argument(
        "--vault-path",
        default="/Users/msbaek/DocumentsLocal/msbaek_vault",
        help="Vault ê²½ë¡œ (ê¸°ë³¸ê°’: /Users/msbaek/DocumentsLocal/msbaek_vault)"
    )
    
    parser.add_argument(
        "--config",
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥"
    )
    
    # Phase 2 ê¸°ëŠ¥ ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--query",
        help="ê²€ìƒ‰ ì¿¼ë¦¬"
    )
    
    parser.add_argument(
        "--top-k",
        type=int,
        default=10,
        help="ìƒìœ„ Kê°œ ê²°ê³¼ (ê¸°ë³¸ê°’: 10)"
    )
    
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.3,
        help="ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)"
    )
    
    parser.add_argument(
        "--topic",
        help="ìˆ˜ì§‘í•  ì£¼ì œ"
    )
    
    parser.add_argument(
        "--output",
        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--force",
        action="store_true",
        help="ê°•ì œ ì „ì²´ ì¬ì¸ë±ì‹± (ê¸°ì¡´ ìºì‹œ ë¬´ì‹œ)"
    )
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ì„¤ì • ë¡œë”©
    config = load_config(args.config)
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    if args.command == "info":
        show_system_info()
    
    elif args.command == "test":
        if not check_dependencies():
            sys.exit(1)
        
        if run_tests():
            print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        else:
            print("âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "init":
        if not check_dependencies():
            sys.exit(1)
        
        if initialize_system(args.vault_path, config):
            print("\nğŸ‰ Vault Intelligence System V2 ì´ˆê¸°í™” ì™„ë£Œ!")
            print("\në‹¤ìŒ ë‹¨ê³„:")
            print("1. python -m src search --query 'TDD'     # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
            print("2. python -m src duplicates               # ì¤‘ë³µ ê°ì§€")  
            print("3. python -m src collect --topic 'TDD'   # ì£¼ì œ ìˆ˜ì§‘")
            print("4. python -m src analyze                  # ì£¼ì œ ë¶„ì„")
        else:
            print("âŒ ì´ˆê¸°í™” ì‹¤íŒ¨!")
            sys.exit(1)
    
    # Phase 2 ê¸°ëŠ¥ë“¤
    elif args.command == "search":
        if not check_dependencies():
            sys.exit(1)
        
        if not args.query:
            print("âŒ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. --query ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
        
        if run_search(args.vault_path, args.query, args.top_k, args.threshold, config):
            print("âœ… ê²€ìƒ‰ ì™„ë£Œ!")
        else:
            print("âŒ ê²€ìƒ‰ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "duplicates":
        if not check_dependencies():
            sys.exit(1)
        
        if run_duplicate_detection(args.vault_path, config):
            print("âœ… ì¤‘ë³µ ê°ì§€ ì™„ë£Œ!")
        else:
            print("âŒ ì¤‘ë³µ ê°ì§€ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "collect":
        if not check_dependencies():
            sys.exit(1)
        
        if not args.topic:
            print("âŒ ìˆ˜ì§‘í•  ì£¼ì œê°€ í•„ìš”í•©ë‹ˆë‹¤. --topic ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
        
        if run_topic_collection(args.vault_path, args.topic, args.top_k, args.threshold, args.output, config):
            print("âœ… ì£¼ì œ ìˆ˜ì§‘ ì™„ë£Œ!")
        else:
            print("âŒ ì£¼ì œ ìˆ˜ì§‘ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "analyze":
        if not check_dependencies():
            sys.exit(1)
        
        if run_topic_analysis(args.vault_path, config):
            print("âœ… ì£¼ì œ ë¶„ì„ ì™„ë£Œ!")
        else:
            print("âŒ ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "reindex":
        if not check_dependencies():
            sys.exit(1)
        
        if run_reindex(args.vault_path, args.force, config):
            print("âœ… ì¬ì¸ë±ì‹± ì™„ë£Œ!")
        else:
            print("âŒ ì¬ì¸ë±ì‹± ì‹¤íŒ¨!")
            sys.exit(1)


if __name__ == "__main__":
    main()