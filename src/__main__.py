#!/usr/bin/env python3
"""
Vault Intelligence System V2 - Main Entry Point

CLI ì¸í„°í˜ì´ìŠ¤ì™€ ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import argparse
import logging
from pathlib import Path
from typing import Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.core.sentence_transformer_engine import SentenceTransformerEngine
    from src.core.embedding_cache import EmbeddingCache
    from src.core.vault_processor import VaultProcessor
    from src.features.advanced_search import AdvancedSearchEngine, SearchQuery
    from src.features.duplicate_detector import DuplicateDetector
    from src.features.topic_collector import TopicCollector
    from src.features.topic_analyzer import TopicAnalyzer
    from src.features.semantic_tagger import SemanticTagger, TaggingResult
    from src.features.moc_generator import MOCGenerator
    import yaml
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    print("í•„ìˆ˜ ì˜ì¡´ì„±ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    DEPENDENCIES_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_config(config_path: str = None) -> dict:
    """ì„¤ì • íŒŒì¼ ë¡œë”©"""
    if config_path is None:
        config_path = project_root / "config" / "settings.yaml"
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"ì„¤ì • íŒŒì¼ ë¡œë”© ì™„ë£Œ: {config_path}")
        return config
    except Exception as e:
        logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        return {}


def check_dependencies() -> bool:
    """í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸"""
    print("ğŸ” ì˜ì¡´ì„± í™•ì¸ ì¤‘...")
    
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ í•µì‹¬ ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    missing_deps = []
    
    # ê¸°íƒ€ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    try:
        import numpy
        print(f"âœ… NumPy: {numpy.__version__}")
    except ImportError:
        missing_deps.append("numpy")
    
    try:
        import sklearn
        print(f"âœ… Scikit-learn: {sklearn.__version__}")
    except ImportError:
        missing_deps.append("scikit-learn")
    
    try:
        import yaml
        print(f"âœ… PyYAML ì‚¬ìš© ê°€ëŠ¥")
    except ImportError:
        missing_deps.append("pyyaml")
    
    if missing_deps:
        print(f"âŒ ëˆ„ë½ëœ ì˜ì¡´ì„±: {', '.join(missing_deps)}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print(f"pip install {' '.join(missing_deps)}")
        return False
    
    print("âœ… ëª¨ë“  ì˜ì¡´ì„±ì´ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
    return True


def initialize_system(vault_path: str, config: dict) -> bool:
    """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    print("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    
    try:
        # Vault ê²½ë¡œ í™•ì¸
        vault_path = Path(vault_path)
        if not vault_path.exists():
            print(f"âŒ Vault ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {vault_path}")
            return False
        
        print(f"ğŸ“ Vault ê²½ë¡œ: {vault_path}")
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        cache_dir = project_root / "cache"
        cache_dir.mkdir(exist_ok=True)
        print(f"ğŸ’¾ ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
        models_dir = project_root / "models"
        models_dir.mkdir(exist_ok=True)
        print(f"ğŸ¤– ëª¨ë¸ ë””ë ‰í† ë¦¬: {models_dir}")
        
        # Sentence Transformer ì—”ì§„ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        print("ğŸ§  Sentence Transformer ì—”ì§„ í…ŒìŠ¤íŠ¸ ì¤‘...")
        engine = SentenceTransformerEngine(
            model_name=config.get('model', {}).get('name', 'paraphrase-multilingual-mpnet-base-v2'),
            cache_dir=str(models_dir),
            device=config.get('model', {}).get('device')
        )
        
        # TF-IDF ì—”ì§„ì˜ ê²½ìš° í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¡œ í›ˆë ¨ í•„ìš”
        print("ğŸ“š TF-IDF ì—”ì§„ í›ˆë ¨ ì¤‘...")
        test_docs = [
            "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 1: TDDëŠ” í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œì…ë‹ˆë‹¤.",
            "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 2: ë¦¬íŒ©í† ë§ì€ ì½”ë“œ ê°œì„  ê¸°ë²•ì…ë‹ˆë‹¤.",
            "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 3: í´ë¦°ì½”ë“œëŠ” ì½ê¸° ì‰¬ìš´ ì½”ë“œë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤."
        ]
        engine.fit_documents(test_docs)
        
        # í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        test_text = "í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"
        test_embedding = engine.encode_text(test_text)
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›: {len(test_embedding)}")
        
        # ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("ğŸ’¾ ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        cache = EmbeddingCache(str(cache_dir))
        cache_stats = cache.get_statistics()
        print(f"ğŸ“Š ìºì‹œ í†µê³„: {cache_stats['total_embeddings']}ê°œ ì„ë² ë”©")
        
        # Vault í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        print("ğŸ“š Vault í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì¤‘...")
        processor = VaultProcessor(
            str(vault_path),
            excluded_dirs=config.get('vault', {}).get('excluded_dirs'),
            file_extensions=config.get('vault', {}).get('file_extensions')
        )
        
        vault_stats = processor.get_vault_statistics()
        print(f"ğŸ“ˆ Vault í†µê³„: {vault_stats['total_files']}ê°œ íŒŒì¼, "
              f"{vault_stats['total_size_mb']}MB")
        
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.exception("ì´ˆê¸°í™” ì¤‘ ìƒì„¸ ì˜¤ë¥˜:")
        return False


def run_tests():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    success_count = 0
    total_tests = 3
    
    # ì—”ì§„ í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£ Sentence Transformer ì—”ì§„ í…ŒìŠ¤íŠ¸")
    try:
        from src.core.sentence_transformer_engine import test_engine
        if test_engine():
            success_count += 1
    except Exception as e:
        print(f"ì—”ì§„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ìºì‹œ í…ŒìŠ¤íŠ¸
    print("\n2ï¸âƒ£ ì„ë² ë”© ìºì‹œ í…ŒìŠ¤íŠ¸")
    try:
        from src.core.embedding_cache import test_cache
        if test_cache():
            success_count += 1
    except Exception as e:
        print(f"ìºì‹œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ Vault í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸")
    try:
        from src.core.vault_processor import test_processor
        if test_processor():
            success_count += 1
    except Exception as e:
        print(f"í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {success_count}/{total_tests} í†µê³¼")
    return success_count == total_tests


def show_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ"""
    print("â„¹ï¸ Vault Intelligence System V2")
    print("=" * 50)
    print(f"í”„ë¡œì íŠ¸ ê²½ë¡œ: {project_root}")
    print(f"Python ë²„ì „: {sys.version}")
    
    try:
        import torch
        device = "CUDA" if torch.cuda.is_available() else "CPU"
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = "MPS (Metal)"
        print(f"PyTorch ì¥ì¹˜: {device}")
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    except:
        print("PyTorch: ì‚¬ìš© ë¶ˆê°€")
    
    # ìºì‹œ ìƒíƒœ í™•ì¸
    try:
        from .core.embedding_cache import EmbeddingCache
        cache_dir = str(project_root / "cache")
        cache = EmbeddingCache(cache_dir)
        
        print("\nğŸ’¾ ìºì‹œ ìƒíƒœ:")
        stats = cache.get_statistics()
        print(f"- Dense ì„ë² ë”©: {stats.get('total_embeddings', 0):,}ê°œ")
        
        colbert_stats = cache.get_colbert_statistics()
        print(f"- ColBERT ì„ë² ë”©: {colbert_stats.get('total_colbert_embeddings', 0):,}ê°œ")
        print(f"- ìºì‹œ DB í¬ê¸°: {stats.get('db_size', 0) / (1024*1024):.1f}MB")
        
    except Exception as e:
        print(f"\nğŸ’¾ ìºì‹œ ìƒíƒœ: í™•ì¸ ë¶ˆê°€ ({e})")
    
    print("\nğŸ¯ ì™„ë£Œëœ ê¸°ëŠ¥ (Phase 1-7):")
    print("- BGE-M3 ê¸°ë°˜ Dense + Sparse + ColBERT ê²€ìƒ‰")
    print("- Cross-encoder ì¬ìˆœìœ„í™” (BGE Reranker V2-M3)")
    print("- ì¿¼ë¦¬ í™•ì¥ (ë™ì˜ì–´ + HyDE)")
    print("- ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ ë° ê·¸ë£¹í™”")
    print("- ì£¼ì œë³„ í´ëŸ¬ìŠ¤í„°ë§ ë° ë¶„ì„")
    print("- ì§€ì‹ ê·¸ë˜í”„ ë° ê´€ë ¨ ë¬¸ì„œ ì¶”ì²œ")
    print("- ìë™ íƒœê¹… ì‹œìŠ¤í…œ")
    print("- ColBERT ì¦ë¶„ ìºì‹± ì‹œìŠ¤í…œ (ì‹ ê·œ!)")
    print()
    print("âš¡ ColBERT ê²€ìƒ‰ ëª…ë ¹ì–´:")
    print("  python -m src reindex --with-colbert     # ColBERT í¬í•¨ ì¸ë±ì‹±")
    print("  python -m src reindex --colbert-only     # ColBERTë§Œ ì¸ë±ì‹±")
    print("  python -m src search --query 'TDD' --search-method colbert")
    print()
    print("âš¡ ê¸°ë³¸ ëª…ë ¹ì–´:")
    print("  python -m src search --query 'TDD'")
    print("  python -m src collect --topic 'ë¦¬íŒ©í† ë§'")
    print("  python -m src duplicates")


def run_search(vault_path: str, query: str, top_k: int, threshold: float, config: dict, sample_size: Optional[int] = None, use_reranker: bool = False, search_method: str = "hybrid", use_expansion: bool = False, include_synonyms: bool = True, include_hyde: bool = True, use_centrality: bool = False, centrality_weight: float = 0.2):
    """ê²€ìƒ‰ ì‹¤í–‰"""
    try:
        print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{query}'")
        if sample_size:
            print(f"ğŸ“Š ìƒ˜í”Œë§ ëª¨ë“œ: {sample_size}ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬")
        if use_reranker:
            print("ğŸ¯ ì¬ìˆœìœ„í™” ëª¨ë“œ í™œì„±í™”")
        if use_expansion:
            print("ğŸ“ ì¿¼ë¦¬ í™•ì¥ ëª¨ë“œ í™œì„±í™”")
            expand_features = []
            if include_synonyms:
                expand_features.append("ë™ì˜ì–´")
            if include_hyde:
                expand_features.append("HyDE")
            print(f"   í™•ì¥ ê¸°ëŠ¥: {', '.join(expand_features)}")
        if use_centrality:
            print(f"ğŸ¯ ì¤‘ì‹¬ì„± ë¶€ìŠ¤íŒ… í™œì„±í™” (ê°€ì¤‘ì¹˜: {centrality_weight})")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index(sample_size=sample_size):
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ê²€ìƒ‰ ìˆ˜í–‰ (í™•ì¥, ì¬ìˆœìœ„í™”, ì¤‘ì‹¬ì„± ë¶€ìŠ¤íŒ… ì˜µì…˜ í¬í•¨)
        if use_centrality:
            # ì¤‘ì‹¬ì„± ë¶€ìŠ¤íŒ… ê²€ìƒ‰
            results = search_engine.search_with_centrality_boost(
                query=query,
                search_method=search_method,
                top_k=top_k,
                centrality_weight=centrality_weight,
                threshold=threshold
            )
        elif use_expansion:
            # ì¿¼ë¦¬ í™•ì¥ì„ í¬í•¨í•œ ê²€ìƒ‰
            results = search_engine.expanded_search(
                query=query,
                search_method=search_method,
                top_k=top_k,
                threshold=threshold,
                include_synonyms=include_synonyms,
                include_hyde=include_hyde
            )
        elif use_reranker:
            # ì¬ìˆœìœ„í™”ë¥¼ í¬í•¨í•œ ê³ ê¸‰ ê²€ìƒ‰
            results = search_engine.search_with_reranking(
                query=query, 
                search_method=search_method,
                initial_k=top_k * 3,  # 3ë°°ìˆ˜ í›„ë³´ ê²€ìƒ‰
                final_k=top_k, 
                threshold=threshold,
                use_reranker=True
            )
        else:
            # ê¸°ì¡´ ê²€ìƒ‰ ë°©ë²•
            if search_method == "semantic":
                results = search_engine.semantic_search(query, top_k=top_k, threshold=threshold)
            elif search_method == "keyword":
                results = search_engine.keyword_search(query, top_k=top_k)
            elif search_method == "colbert":
                results = search_engine.colbert_search(query, top_k=top_k, threshold=threshold)
            else:  # hybrid
                results = search_engine.hybrid_search(query, top_k=top_k, threshold=threshold)
        
        if not results:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"\nğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
        print("-" * 80)
        
        for result in results:
            print(f"{result.rank}. {result.document.title}")
            print(f"   ê²½ë¡œ: {result.document.path}")
            print(f"   ìœ ì‚¬ë„: {result.similarity_score:.4f}")
            print(f"   íƒ€ì…: {result.match_type}")
            if result.matched_keywords:
                print(f"   í‚¤ì›Œë“œ: {', '.join(result.matched_keywords)}")
            if result.snippet:
                print(f"   ë‚´ìš©: {result.snippet[:100]}...")
            print()
        
        return True
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return False


def run_duplicate_detection(vault_path: str, config: dict):
    """ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ ì‹¤í–‰"""
    try:
        print("ğŸ” ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ì¤‘ë³µ ê°ì§€ê¸° ì´ˆê¸°í™”
        detector = DuplicateDetector(search_engine, config)
        
        # ì¤‘ë³µ ë¶„ì„ ìˆ˜í–‰
        analysis = detector.find_duplicates()
        
        print(f"\nğŸ“Š ì¤‘ë³µ ë¶„ì„ ê²°ê³¼:")
        print("-" * 50)
        print(f"ì „ì²´ ë¬¸ì„œ: {analysis.total_documents}ê°œ")
        print(f"ì¤‘ë³µ ê·¸ë£¹: {analysis.get_group_count()}ê°œ")
        print(f"ì¤‘ë³µ ë¬¸ì„œ: {analysis.duplicate_count}ê°œ")
        print(f"ê³ ìœ  ë¬¸ì„œ: {analysis.unique_count}ê°œ")
        print(f"ì¤‘ë³µ ë¹„ìœ¨: {analysis.get_duplicate_ratio():.1%}")
        
        if analysis.duplicate_groups:
            print(f"\nğŸ“‹ ì¤‘ë³µ ê·¸ë£¹ ìƒì„¸:")
            for group in analysis.duplicate_groups[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                print(f"\nê·¸ë£¹ {group.id}:")
                print(f"  ë¬¸ì„œ ìˆ˜: {group.get_document_count()}ê°œ")
                print(f"  í‰ê·  ìœ ì‚¬ë„: {group.average_similarity:.4f}")
                for doc in group.documents:
                    print(f"    - {doc.path} ({doc.word_count}ë‹¨ì–´)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì¤‘ë³µ ê°ì§€ ì‹¤íŒ¨: {e}")
        return False


def run_topic_collection(vault_path: str, topic: str, top_k: int, threshold: float, output_file: str, config: dict, use_expansion: bool = False, include_synonyms: bool = True, include_hyde: bool = True):
    """ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ì§‘ ì‹¤í–‰"""
    try:
        print(f"ğŸ“š ì£¼ì œ '{topic}' ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘...")
        if use_expansion:
            expand_features = []
            if include_synonyms:
                expand_features.append("ë™ì˜ì–´")
            if include_hyde:
                expand_features.append("HyDE")
            print(f"ğŸ“ ì¿¼ë¦¬ í™•ì¥ ëª¨ë“œ í™œì„±í™”: {', '.join(expand_features)}")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ì£¼ì œ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        collector = TopicCollector(search_engine, config)
        
        # ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ì§‘
        collection = collector.collect_topic(
            topic=topic,
            top_k=top_k,
            threshold=threshold,
            output_file=output_file,
            use_expansion=use_expansion,
            include_synonyms=include_synonyms,
            include_hyde=include_hyde
        )
        
        print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print("-" * 50)
        print(f"ì£¼ì œ: {collection.metadata.topic}")
        print(f"ìˆ˜ì§‘ ë¬¸ì„œ: {collection.metadata.total_documents}ê°œ")
        print(f"ì´ ë‹¨ì–´ìˆ˜: {collection.metadata.total_word_count:,}ê°œ")
        print(f"ì´ í¬ê¸°: {collection.metadata.total_size_bytes / 1024:.1f}KB")
        
        if collection.metadata.tag_distribution:
            print(f"\nğŸ·ï¸ íƒœê·¸ ë¶„í¬:")
            for tag, count in sorted(collection.metadata.tag_distribution.items(), 
                                   key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {tag}: {count}ê°œ")
        
        if collection.metadata.directory_distribution:
            print(f"\nğŸ“ ë””ë ‰í† ë¦¬ ë¶„í¬:")
            for dir_path, count in sorted(collection.metadata.directory_distribution.items(), 
                                       key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {dir_path}: {count}ê°œ")
        
        if output_file:
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì£¼ì œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return False


def run_topic_analysis(vault_path: str, output_file: str, config: dict):
    """ì£¼ì œ ë¶„ì„ ì‹¤í–‰"""
    try:
        print("ğŸ” ì£¼ì œ ë¶„ì„ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ì£¼ì œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = TopicAnalyzer(search_engine, config)
        
        # ì£¼ì œ ë¶„ì„ ìˆ˜í–‰ (ìƒˆë¡œìš´ ì£¼ì œ ê¸°ë°˜ ë°©ì‹ ì‚¬ìš©)
        print("ğŸ¯ ì£¼ì œ ê¸°ë°˜ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        analysis = analyzer.analyze_topics_by_predefined_subjects(min_docs_per_topic=5)
        
        print(f"\nğŸ“Š ì£¼ì œ ë¶„ì„ ê²°ê³¼:")
        print("-" * 50)
        print(f"ë¶„ì„ ë¬¸ì„œ: {analysis.total_documents}ê°œ")
        print(f"ë°œê²¬ í´ëŸ¬ìŠ¤í„°: {analysis.get_cluster_count()}ê°œ")
        print(f"í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•: {analysis.algorithm}")
        if analysis.silhouette_score is not None:
            print(f"ì‹¤ë£¨ì—£ ì ìˆ˜: {analysis.silhouette_score:.3f}")
        
        if analysis.clusters:
            print(f"\nğŸ·ï¸ ì£¼ìš” í´ëŸ¬ìŠ¤í„°ë“¤:")
            for i, cluster in enumerate(analysis.clusters[:10]):  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                print(f"\ní´ëŸ¬ìŠ¤í„° {i+1}: {cluster.label}")
                print(f"  ë¬¸ì„œ ìˆ˜: {cluster.get_document_count()}ê°œ")
                print(f"  ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(cluster.keywords[:5]) if cluster.keywords else 'ì—†ìŒ'}")
                if cluster.coherence_score is not None:
                    print(f"  ì¼ê´€ì„± ì ìˆ˜: {cluster.coherence_score:.3f}")
                if cluster.representative_doc:
                    print(f"  ëŒ€í‘œ ë¬¸ì„œ: {cluster.representative_doc.title[:50]}")
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)
        if output_file:
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥
            if output_file.lower().endswith('.md'):
                if analyzer.export_markdown_report(analysis, output_file):
                    print(f"\nğŸ’¾ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    print(f"\nâŒ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {output_file}")
            else:
                if analyzer.export_analysis(analysis, output_file):
                    print(f"\nğŸ’¾ JSON ë¶„ì„ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    print(f"\nâŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {output_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return False


def run_related_documents(vault_path: str, file_path: str, top_k: int, config: dict, 
                         include_centrality: bool = True, similarity_threshold: float = 0.3):
    """ê´€ë ¨ ë¬¸ì„œ ì¶”ì²œ ì‹¤í–‰"""
    try:
        print(f"ğŸ”— '{file_path}' ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        related_results = search_engine.get_related_documents(
            document_path=file_path,
            top_k=top_k,
            include_centrality_boost=include_centrality,
            similarity_threshold=similarity_threshold
        )
        
        if not related_results:
            print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"\nğŸ“„ ê´€ë ¨ ë¬¸ì„œ ({len(related_results)}ê°œ):")
        print("-" * 80)
        
        for result in related_results:
            print(f"{result.rank}. {result.document.title}")
            print(f"   ê²½ë¡œ: {result.document.path}")
            print(f"   ê´€ë ¨ë„: {result.similarity_score:.4f}")
            print(f"   íƒ€ì…: {result.match_type}")
            if result.document.tags:
                print(f"   íƒœê·¸: {', '.join(result.document.tags)}")
            if result.snippet:
                print(f"   ë‚´ìš©: {result.snippet[:100]}...")
            print()
        
        return True
        
    except Exception as e:
        print(f"âŒ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° ì‹¤íŒ¨: {e}")
        return False


def run_knowledge_gap_analysis(vault_path: str, config: dict, output_file: str = None,
                              similarity_threshold: float = 0.3, min_connections: int = 2):
    """ì§€ì‹ ê³µë°± ë¶„ì„ ì‹¤í–‰"""
    try:
        print("ğŸ” ì§€ì‹ ê³µë°± ë¶„ì„ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ì§€ì‹ ê³µë°± ë¶„ì„ ìˆ˜í–‰
        analysis = search_engine.analyze_knowledge_gaps(
            similarity_threshold=similarity_threshold,
            min_connections=min_connections
        )
        
        if not analysis:
            print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        summary = analysis.get('summary', {})
        
        print(f"\nğŸ“Š ì§€ì‹ ê³µë°± ë¶„ì„ ê²°ê³¼:")
        print("-" * 50)
        print(f"ì „ì²´ ë¬¸ì„œ: {summary.get('total_documents', 0)}ê°œ")
        print(f"ê³ ë¦½ ë¬¸ì„œ: {summary.get('isolated_count', 0)}ê°œ")
        print(f"ì•½í•œ ì—°ê²° ë¬¸ì„œ: {summary.get('weakly_connected_count', 0)}ê°œ")
        print(f"ê³ ë¦½ íƒœê·¸: {summary.get('isolated_tag_count', 0)}ê°œ")
        print(f"ê³ ë¦½ë¥ : {summary.get('isolation_rate', 0):.1%}")
        
        # ê³ ë¦½ëœ ë¬¸ì„œë“¤ ìƒì„¸ ì •ë³´
        isolated_docs = analysis.get('isolated_documents', [])
        if isolated_docs:
            print(f"\nğŸï¸ ê³ ë¦½ëœ ë¬¸ì„œë“¤:")
            for doc in isolated_docs[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                print(f"  - {doc['title']} ({doc['word_count']}ë‹¨ì–´)")
                if doc['tags']:
                    print(f"    íƒœê·¸: {', '.join(doc['tags'])}")
        
        # ì•½í•œ ì—°ê²° ë¬¸ì„œë“¤
        weak_docs = analysis.get('weakly_connected_documents', [])
        if weak_docs:
            print(f"\nğŸ”— ì•½í•œ ì—°ê²° ë¬¸ì„œë“¤:")
            for doc in weak_docs[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                print(f"  - {doc['title']} ({doc['connections']}ê°œ ì—°ê²°, {doc['word_count']}ë‹¨ì–´)")
                if doc['tags']:
                    print(f"    íƒœê·¸: {', '.join(doc['tags'])}")
        
        # ê³ ë¦½ëœ íƒœê·¸ë“¤
        isolated_tags = analysis.get('isolated_tags', {})
        if isolated_tags:
            print(f"\nğŸ·ï¸ ê³ ë¦½ëœ íƒœê·¸ë“¤ (ìƒìœ„ 10ê°œ):")
            for tag, docs in list(isolated_tags.items())[:10]:
                print(f"  - {tag}: {', '.join(docs)}")
        
        # íƒœê·¸ ë¶„í¬ (ìƒìœ„ íƒœê·¸ë“¤)
        tag_dist = analysis.get('tag_distribution', {})
        if tag_dist:
            print(f"\nğŸ“ˆ ì£¼ìš” íƒœê·¸ ë¶„í¬:")
            sorted_tags = sorted(tag_dist.items(), key=lambda x: len(x[1]), reverse=True)
            for tag, docs in sorted_tags[:10]:
                print(f"  - {tag}: {len(docs)}ê°œ ë¬¸ì„œ")
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)
        if output_file:
            try:
                import json
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(analysis, f, indent=2, ensure_ascii=False, default=str)
                print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"\nâŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì§€ì‹ ê³µë°± ë¶„ì„ ì‹¤íŒ¨: {e}")
        return False


def run_reindex(vault_path: str, force: bool, config: dict, sample_size: Optional[int] = None, 
                include_folders: Optional[list] = None, exclude_folders: Optional[list] = None,
                with_colbert: bool = False, colbert_only: bool = False):
    """ì „ì²´ ì¬ì¸ë±ì‹± ì‹¤í–‰ (ColBERT ì§€ì›)"""
    try:
        print("ğŸ”„ ì „ì²´ ì¬ì¸ë±ì‹± ì‹œì‘...")
        if force:
            print("âš ï¸ ê°•ì œ ëª¨ë“œ: ê¸°ì¡´ ìºì‹œë¥¼ ë¬´ì‹œí•˜ê³  ëª¨ë“  ë¬¸ì„œë¥¼ ì¬ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        if sample_size:
            print(f"ğŸ“Š ìƒ˜í”Œë§ ëª¨ë“œ: {sample_size}ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        if include_folders:
            print(f"ğŸ“ í´ë” í•„í„°: {', '.join(include_folders)} í¬í•¨")
        if exclude_folders:
            print(f"ğŸš« í´ë” ì œì™¸: {', '.join(exclude_folders)}")
        if with_colbert:
            print("ğŸ¯ ColBERT ì¸ë±ì‹± í¬í•¨")
        if colbert_only:
            print("ğŸ¯ ColBERTë§Œ ì¬ì¸ë±ì‹± (Dense ì„ë² ë”© ì œì™¸)")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” (í´ë” í•„í„°ë§ ì„¤ì •)
        cache_dir = str(project_root / "cache")
        
        # ì„ì‹œë¡œ vault ì„¤ì •ì— í´ë” í•„í„° ì¶”ê°€
        temp_config = config.copy()
        if include_folders or exclude_folders:
            temp_config['vault'] = temp_config.get('vault', {}).copy()
            if include_folders:
                temp_config['vault']['include_folders'] = include_folders
            if exclude_folders:
                temp_config['vault']['exclude_folders'] = exclude_folders
        
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, temp_config)
        
        # ì§„í–‰ë¥  í‘œì‹œ í•¨ìˆ˜
        def progress_callback(current, total):
            percentage = (current / total) * 100
            print(f"ğŸ“Š ì§„í–‰ë¥ : {current}/{total} ({percentage:.1f}%)")
        
        # Dense ì„ë² ë”© ì¸ë±ìŠ¤ êµ¬ì¶• (colbert_onlyê°€ ì•„ë‹Œ ê²½ìš°)
        if not colbert_only:
            print("ğŸ“š Dense ì„ë² ë”© ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            success = search_engine.build_index(
                force_rebuild=force, 
                progress_callback=progress_callback,
                sample_size=sample_size
            )
            
            if not success:
                print("âŒ Dense ì„ë² ë”© ì¸ë±ì‹± ì‹¤íŒ¨!")
                return False
        
        # ColBERT ì¸ë±ì‹±
        if with_colbert or colbert_only:
            print("ğŸ¯ ColBERT ì¸ë±ì‹± ì‹œì‘...")
            try:
                from .features.colbert_search import ColBERTSearchEngine
                
                colbert_config = temp_config.get('colbert', {})
                colbert_engine = ColBERTSearchEngine(
                    model_name=colbert_config.get('model_name', 'BAAI/bge-m3'),
                    device=colbert_config.get('device', temp_config.get('model', {}).get('device')),
                    use_fp16=colbert_config.get('use_fp16', True),
                    cache_folder=colbert_config.get('cache_folder', temp_config.get('model', {}).get('cache_folder')),
                    max_length=colbert_config.get('max_length', temp_config.get('model', {}).get('max_length', 4096)),
                    cache_dir=cache_dir,
                    enable_cache=colbert_config.get('enable_cache', True)
                )
                
                if colbert_engine.is_available():
                    # ë¬¸ì„œ ë¡œë“œ (search_engineì—ì„œ ê°€ì ¸ì˜¤ê¸°)
                    if hasattr(search_engine, 'documents') and search_engine.documents:
                        documents = search_engine.documents
                    else:
                        # ë¬¸ì„œë¥¼ ì§ì ‘ ë¡œë“œ
                        from .core.vault_processor import VaultProcessor
                        vault_config = temp_config.get('vault', {})
                        vault_processor = VaultProcessor(
                            vault_path=vault_path,
                            excluded_dirs=vault_config.get('excluded_dirs', None),
                            excluded_files=vault_config.get('excluded_files', None), 
                            file_extensions=vault_config.get('file_extensions', None)
                        )
                        documents = vault_processor.process_files()
                        if sample_size:
                            documents = documents[:sample_size]
                    
                    colbert_success = colbert_engine.build_index(
                        documents=documents,
                        batch_size=colbert_config.get('batch_size', 8),
                        max_documents=colbert_config.get('max_documents', None),
                        force_rebuild=force
                    )
                    
                    if colbert_success:
                        print(f"âœ… ColBERT ì¸ë±ì‹± ì™„ë£Œ!")
                    else:
                        print("âš ï¸ ColBERT ì¸ë±ì‹± ì‹¤íŒ¨, ê³„ì† ì§„í–‰...")
                else:
                    print("âš ï¸ ColBERT ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                print(f"âš ï¸ ColBERT ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ê²°ê³¼ í†µê³„ ì¶œë ¥
        if not colbert_only:
            stats = search_engine.get_search_statistics()
            print(f"\nâœ… ì¬ì¸ë±ì‹± ì™„ë£Œ!")
            print(f"ğŸ“Š Dense ì„ë² ë”© ê²°ê³¼:")
            print(f"  - ì¸ë±ì‹±ëœ ë¬¸ì„œ: {stats['indexed_documents']:,}ê°œ")
            print(f"  - ì„ë² ë”© ì°¨ì›: {stats['embedding_dimension']}ì°¨ì›")
            print(f"  - ìºì‹œëœ ì„ë² ë”©: {stats['cache_statistics']['total_embeddings']:,}ê°œ")
            print(f"  - Vault í¬ê¸°: {stats['vault_statistics']['total_size_mb']:.1f}MB")
        
        # ColBERT ìºì‹œ í†µê³„
        if with_colbert or colbert_only:
            try:
                from .core.embedding_cache import EmbeddingCache
                cache = EmbeddingCache(cache_dir)
                colbert_stats = cache.get_colbert_statistics()
                if colbert_stats.get('total_colbert_embeddings', 0) > 0:
                    print(f"ğŸ¯ ColBERT ìºì‹œ ê²°ê³¼:")
                    print(f"  - ColBERT ì„ë² ë”©: {colbert_stats['total_colbert_embeddings']:,}ê°œ")
                    print(f"  - í‰ê·  í† í° ìˆ˜: {colbert_stats['avg_tokens']}ê°œ")
                    print(f"  - ìºì‹œ íŒŒì¼ í¬ê¸°: {colbert_stats['total_file_size']:,} bytes")
            except Exception as e:
                logger.debug(f"ColBERT í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
        return False


def run_tagging(vault_path: str, target: str, recursive: bool, dry_run: bool, 
               force: bool, batch_size: int, config: dict):
    """ìë™ íƒœê¹… ì‹¤í–‰"""
    try:
        print(f"ğŸ·ï¸ ìë™ íƒœê¹… ì‹œì‘: {target}")
        if dry_run:
            print("ğŸ” ë“œë¼ì´ëŸ° ëª¨ë“œ: ì‹¤ì œ ë³€ê²½ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        if force:
            print("âš ï¸ ê°•ì œ ëª¨ë“œ: ê¸°ì¡´ íƒœê·¸ë¥¼ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        if recursive:
            print("ğŸ“ ì¬ê·€ ëª¨ë“œ: í•˜ìœ„ í´ë”ì˜ ëª¨ë“  íŒŒì¼ì„ í¬í•¨í•©ë‹ˆë‹¤.")
        
        # ì˜ë¯¸ì  íƒœê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        tagger = SemanticTagger(vault_path, config)
        
        # target ê²½ë¡œ ì²˜ë¦¬ ë° vault ë‚´ ê²€ìƒ‰
        vault_base = Path(vault_path)
        target_path = None
        
        # 1. ì ˆëŒ€ ê²½ë¡œì¸ ê²½ìš°
        if Path(target).is_absolute():
            target_path = Path(target)
            if target_path.exists():
                print(f"ğŸ“„ ì ˆëŒ€ ê²½ë¡œë¡œ íŒŒì¼ í™•ì¸: {target_path}")
            else:
                print(f"âŒ ì ˆëŒ€ ê²½ë¡œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {target}")
                return False
        
        # 2. ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° (vault ê¸°ì¤€) - ë¨¼ì € í™•ì¸
        elif "/" in target:
            candidate_path = vault_base / target
            if candidate_path.exists():
                target_path = candidate_path
                print(f"ğŸ“ Vault ìƒëŒ€ ê²½ë¡œë¡œ í™•ì¸: {target}")
            else:
                print(f"âŒ Vault ë‚´ ìƒëŒ€ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {target}")
                return False
        
        # 3. íŒŒì¼ëª…ë§Œ ì œê³µëœ ê²½ìš° - vault ì „ì²´ì—ì„œ ê²€ìƒ‰
        else:
            print(f"ğŸ” '{target}' íŒŒì¼ì„ vaultì—ì„œ ê²€ìƒ‰ ì¤‘...")
            found_files = []
            
            # .md í™•ì¥ìê°€ ì—†ìœ¼ë©´ ì¶”ê°€í•´ì„œë„ ê²€ìƒ‰
            search_patterns = [target]
            if not target.endswith('.md'):
                search_patterns.append(f"{target}.md")
            
            for pattern in search_patterns:
                # vault ì „ì²´ì—ì„œ íŒŒì¼ëª… ê²€ìƒ‰ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
                for md_file in vault_base.rglob("*.md"):
                    if md_file.name.lower() == pattern.lower():
                        found_files.append(md_file)
                    elif pattern.lower() in md_file.name.lower():
                        found_files.append(md_file)
            
            if not found_files:
                print(f"âŒ '{target}' íŒŒì¼ì„ vaultì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"   ê²€ìƒ‰ ê²½ë¡œ: {vault_base}")
                return False
            elif len(found_files) == 1:
                target_path = found_files[0]
                rel_path = target_path.relative_to(vault_base)
                print(f"âœ… íŒŒì¼ ë°œê²¬: {rel_path}")
            else:
                print(f"ğŸ“‹ '{target}' ê´€ë ¨ íŒŒì¼ì´ {len(found_files)}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
                for i, file_path in enumerate(found_files[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                    rel_path = file_path.relative_to(vault_base)
                    print(f"  {i}. {rel_path}")
                
                if len(found_files) <= 10:
                    # ì²« ë²ˆì§¸ íŒŒì¼ì„ ê¸°ë³¸ ì„ íƒ
                    target_path = found_files[0]
                    rel_path = target_path.relative_to(vault_base)
                    print(f"ğŸ¯ ì²« ë²ˆì§¸ íŒŒì¼ì„ ì„ íƒ: {rel_path}")
                else:
                    print(f"   ... ë° {len(found_files) - 10}ê°œ ë”")
                    print("âŒ ë„ˆë¬´ ë§ì€ íŒŒì¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ íŒŒì¼ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
                    return False
        
        # vault ë‚´ë¶€ ê²½ë¡œì¸ì§€ í™•ì¸
        try:
            if target_path:
                relative_path = target_path.resolve().relative_to(vault_base.resolve())
                print(f"ğŸ“ Vault ë‚´ë¶€ ê²½ë¡œ í™•ì¸: {relative_path}")
        except ValueError:
            print(f"âš ï¸ ê²½ê³ : Vault ì™¸ë¶€ ê²½ë¡œì…ë‹ˆë‹¤: {target_path}")
        
        if not target_path or not target_path.exists():
            print(f"âŒ ìµœì¢… ëŒ€ìƒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {target_path}")
            return False
        
        results = []
        
        # ë‹¨ì¼ íŒŒì¼ íƒœê¹…
        if target_path.is_file():
            if target.lower().endswith('.md'):
                print(f"ğŸ“„ ë‹¨ì¼ íŒŒì¼ íƒœê¹…: {target_path.name}")
                result = tagger.tag_document(str(target_path), dry_run=dry_run)
                results.append(result)
            else:
                print("âŒ ë§ˆí¬ë‹¤ìš´ íŒŒì¼(.md)ë§Œ íƒœê¹… ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                return False
        
        # í´ë” ë°°ì¹˜ íƒœê¹…
        elif target_path.is_dir():
            print(f"ğŸ“ í´ë” ë°°ì¹˜ íƒœê¹…: {target_path}")
            results = tagger.tag_folder(
                str(target_path), 
                recursive=recursive, 
                dry_run=dry_run
            )
        
        else:
            print("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ëŒ€ìƒ ê²½ë¡œì…ë‹ˆë‹¤.")
            return False
        
        # ê²°ê³¼ ì¶œë ¥
        if results:
            successful = sum(1 for r in results if r.success)
            total = len(results)
            
            print(f"\nğŸ“Š íƒœê¹… ê²°ê³¼:")
            print("-" * 50)
            print(f"ì²˜ë¦¬ íŒŒì¼: {total}ê°œ")
            print(f"ì„±ê³µ: {successful}ê°œ")
            print(f"ì‹¤íŒ¨: {total - successful}ê°œ")
            print(f"ì„±ê³µë¥ : {successful/total*100:.1f}%")
            
            # ìƒì„¸ ê²°ê³¼ í‘œì‹œ (ìµœëŒ€ 10ê°œ)
            print(f"\nğŸ“‹ ìƒì„¸ ê²°ê³¼ (ìƒìœ„ {min(10, len(results))}ê°œ):")
            for i, result in enumerate(results[:10]):
                print(f"\n{i+1}. {Path(result.file_path).name}")
                if result.success:
                    print(f"   âœ… ì„±ê³µ (ì²˜ë¦¬ì‹œê°„: {result.processing_time:.2f}ì´ˆ)")
                    print(f"   ê¸°ì¡´ íƒœê·¸: {len(result.original_tags)}ê°œ")
                    print(f"   ìƒì„± íƒœê·¸: {len(result.generated_tags)}ê°œ")
                    
                    if result.generated_tags:
                        print(f"   ìƒˆ íƒœê·¸:")
                        for category, tags in result.categorized_tags.items():
                            if tags:
                                print(f"     {category}: {', '.join(tags)}")
                    
                    # ì‹ ë¢°ë„ ë†’ì€ íƒœê·¸ë“¤
                    high_confidence_tags = [
                        tag for tag, score in result.confidence_scores.items()
                        if score > 0.7
                    ]
                    if high_confidence_tags:
                        print(f"   ê³ ì‹ ë¢°ë„ íƒœê·¸: {', '.join(high_confidence_tags)}")
                else:
                    print(f"   âŒ ì‹¤íŒ¨: {result.error_message}")
            
            return successful == total
        
        else:
            print("âŒ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
    except Exception as e:
        print(f"âŒ ìë™ íƒœê¹… ì‹¤íŒ¨: {e}")
        logger.exception("íƒœê¹… ì¤‘ ìƒì„¸ ì˜¤ë¥˜:")
        return False


def display_tagging_result(result: TaggingResult):
    """ë‹¨ì¼ íƒœê¹… ê²°ê³¼ í‘œì‹œ"""
    print(f"\nğŸ“„ íƒœê¹… ê²°ê³¼: {Path(result.file_path).name}")
    print("-" * 50)
    
    if result.success:
        print(f"âœ… ì„±ê³µ (ì²˜ë¦¬ì‹œê°„: {result.processing_time:.2f}ì´ˆ)")
        print(f"ê¸°ì¡´ íƒœê·¸: {result.original_tags if result.original_tags else 'ì—†ìŒ'}")
        print(f"ìƒì„± íƒœê·¸: {len(result.generated_tags)}ê°œ")
        
        if result.categorized_tags:
            print("\nğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ íƒœê·¸:")
            for category, tags in result.categorized_tags.items():
                if tags:
                    print(f"  {category}: {', '.join(tags)}")
        
        if result.confidence_scores:
            print(f"\nğŸ“Š ì‹ ë¢°ë„ ì ìˆ˜:")
            sorted_scores = sorted(
                result.confidence_scores.items(), 
                key=lambda x: x[1], 
                reverse=True
            )
            for tag, score in sorted_scores[:5]:  # ìƒìœ„ 5ê°œ
                print(f"  {tag}: {score:.3f}")
    else:
        print(f"âŒ ì‹¤íŒ¨: {result.error_message}")


def run_moc_generation(
    vault_path: str,
    topic: str,
    top_k: int,
    threshold: float,
    output_file: Optional[str],
    config: dict,
    include_orphans: bool = False,
    use_expansion: bool = True
):
    """MOC ìƒì„± ì‹¤í–‰"""
    try:
        print(f"ğŸ“š '{topic}' MOC ìƒì„± ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(project_root / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # MOC ìƒì„±ê¸° ì´ˆê¸°í™”
        moc_generator = MOCGenerator(search_engine, config)
        
        # ì¶œë ¥ íŒŒì¼ëª… ìë™ ìƒì„± (ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
        if not output_file:
            safe_topic = topic.replace(' ', '-').replace('/', '-')
            output_file = f"MOC-{safe_topic}.md"
        
        # MOC ìƒì„±
        moc_data = moc_generator.generate_moc(
            topic=topic,
            top_k=top_k,
            threshold=threshold,
            include_orphans=include_orphans,
            use_expansion=use_expansion,
            output_file=output_file
        )
        
        print(f"\nğŸ“Š MOC ìƒì„± ê²°ê³¼:")
        print("-" * 50)
        print(f"ì£¼ì œ: {moc_data.topic}")
        print(f"ì´ ë¬¸ì„œ: {moc_data.total_documents}ê°œ")
        print(f"í•µì‹¬ ë¬¸ì„œ: {len(moc_data.core_documents)}ê°œ")
        print(f"ì¹´í…Œê³ ë¦¬: {len(moc_data.categories)}ê°œ")
        print(f"í•™ìŠµ ê²½ë¡œ: {len(moc_data.learning_path)}ë‹¨ê³„")
        print(f"ê´€ë ¨ ì£¼ì œ: {len(moc_data.related_topics)}ê°œ")
        print(f"ìµœê·¼ ì—…ë°ì´íŠ¸: {len(moc_data.recent_updates)}ê°œ")
        print(f"ë¬¸ì„œ ê´€ê³„: {len(moc_data.relationships)}ê°œ")
        
        if moc_data.categories:
            print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ë¶„í¬:")
            for category in moc_data.categories:
                print(f"  {category.name}: {len(category.documents)}ê°œ ë¬¸ì„œ")
        
        if moc_data.learning_path:
            print(f"\nğŸ›¤ï¸ í•™ìŠµ ê²½ë¡œ:")
            for step in moc_data.learning_path:
                print(f"  {step.step}. {step.title} ({step.difficulty_level}) - {len(step.documents)}ê°œ ë¬¸ì„œ")
        
        if output_file:
            print(f"\nğŸ’¾ MOC íŒŒì¼ì´ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        return True
        
    except Exception as e:
        print(f"âŒ MOC ìƒì„± ì‹¤íŒ¨: {e}")
        logger.exception("MOC ìƒì„± ì¤‘ ìƒì„¸ ì˜¤ë¥˜:")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Vault Intelligence System V2 - Sentence Transformers ê¸°ë°˜ ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œìŠ¤í…œ"
    )
    
    parser.add_argument(
        "command",
        choices=["init", "test", "info", "search", "duplicates", "collect", "analyze", "reindex", "related", "analyze-gaps", "tag", "generate-moc"],
        help="ì‹¤í–‰í•  ëª…ë ¹ì–´"
    )
    
    parser.add_argument(
        "--vault-path",
        default="/Users/msbaek/DocumentsLocal/msbaek_vault",
        help="Vault ê²½ë¡œ (ê¸°ë³¸ê°’: /Users/msbaek/DocumentsLocal/msbaek_vault)"
    )
    
    parser.add_argument(
        "--config",
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥"
    )
    
    # Phase 2 ê¸°ëŠ¥ ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--query",
        help="ê²€ìƒ‰ ì¿¼ë¦¬"
    )
    
    parser.add_argument(
        "--top-k",
        type=int,
        default=10,
        help="ìƒìœ„ Kê°œ ê²°ê³¼ (ê¸°ë³¸ê°’: 10)"
    )
    
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.3,
        help="ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)"
    )
    
    parser.add_argument(
        "--rerank",
        action="store_true",
        help="ì¬ìˆœìœ„í™” í™œì„±í™” (BGE Reranker V2-M3 ì‚¬ìš©)"
    )
    
    parser.add_argument(
        "--search-method",
        choices=["semantic", "keyword", "hybrid", "colbert"],
        default="hybrid",
        help="ê²€ìƒ‰ ë°©ë²• (ê¸°ë³¸ê°’: hybrid)"
    )
    
    parser.add_argument(
        "--expand",
        action="store_true",
        help="ì¿¼ë¦¬ í™•ì¥ í™œì„±í™” (ë™ì˜ì–´ + HyDE)"
    )
    
    parser.add_argument(
        "--no-synonyms",
        action="store_true",
        help="ë™ì˜ì–´ í™•ì¥ ë¹„í™œì„±í™”"
    )
    
    parser.add_argument(
        "--no-hyde",
        action="store_true",
        help="HyDE í™•ì¥ ë¹„í™œì„±í™”"
    )
    
    parser.add_argument(
        "--topic",
        help="ìˆ˜ì§‘í•  ì£¼ì œ"
    )
    
    parser.add_argument(
        "--output",
        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--force",
        action="store_true",
        help="ê°•ì œ ì „ì²´ ì¬ì¸ë±ì‹± (ê¸°ì¡´ ìºì‹œ ë¬´ì‹œ)"
    )
    
    parser.add_argument(
        "--sample-size",
        type=int,
        help="ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜ (ëŒ€ê·œëª¨ vault ì„±ëŠ¥ ìµœì í™”ìš©)"
    )
    
    parser.add_argument(
        "--include-folders",
        nargs="+",
        help="í¬í•¨í•  í´ë” ëª©ë¡ (í´ë”ë³„ ì ì§„ì  ìƒ‰ì¸)"
    )
    
    parser.add_argument(
        "--exclude-folders", 
        nargs="+",
        help="ì œì™¸í•  í´ë” ëª©ë¡"
    )
    
    parser.add_argument(
        "--file",
        help="ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ê¸°ì¤€ íŒŒì¼ (related ëª…ë ¹ì–´ìš©)"
    )
    
    parser.add_argument(
        "--with-centrality",
        action="store_true",
        help="ì¤‘ì‹¬ì„± ì ìˆ˜ë¥¼ ê²€ìƒ‰ ë­í‚¹ì— ë°˜ì˜"
    )
    
    parser.add_argument(
        "--centrality-weight",
        type=float,
        default=0.2,
        help="ì¤‘ì‹¬ì„± ì ìˆ˜ ê°€ì¤‘ì¹˜ (0.0-1.0, ê¸°ë³¸ê°’: 0.2)"
    )
    
    parser.add_argument(
        "--similarity-threshold",
        type=float,
        default=0.3,
        help="ê´€ë ¨ì„± íŒì • ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)"
    )
    
    parser.add_argument(
        "--min-connections",
        type=int,
        default=2,
        help="ìµœì†Œ ì—°ê²° ìˆ˜ (ì´ë³´ë‹¤ ì ìœ¼ë©´ ì•½í•œ ì—°ê²°ë¡œ íŒì •, ê¸°ë³¸ê°’: 2)"
    )
    
    # ColBERT ì¸ë±ì‹± ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--with-colbert",
        action="store_true",
        help="ColBERT ì¸ë±ì‹± í¬í•¨ (reindex ëª…ë ¹ì–´ìš©)"
    )
    
    parser.add_argument(
        "--colbert-only",
        action="store_true",
        help="ColBERTë§Œ ì¬ì¸ë±ì‹± (Dense ì„ë² ë”© ì œì™¸)"
    )
    
    # íƒœê¹… ê´€ë ¨ ì¸ìë“¤ (Phase 7)
    parser.add_argument(
        "--target",
        help="íƒœê¹…í•  ëŒ€ìƒ íŒŒì¼ ë˜ëŠ” í´ë” ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--recursive",
        action="store_true", 
        help="í•˜ìœ„ í´ë” í¬í•¨ (í´ë” íƒœê¹… ì‹œ)"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ ë³€ê²½ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°"
    )
    
    parser.add_argument(
        "--tag-force",
        action="store_true",
        help="ê¸°ì¡´ íƒœê·¸ ë¬´ì‹œí•˜ê³  ì¬ìƒì„±"
    )
    
    parser.add_argument(
        "--batch-size", 
        type=int,
        default=10,
        help="ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 10)"
    )
    
    # MOC ìƒì„± ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--include-orphans",
        action="store_true",
        help="ì—°ê²°ë˜ì§€ ì•Šì€ ë¬¸ì„œë„ MOCì— í¬í•¨"
    )
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ì„¤ì • ë¡œë”©
    config = load_config(args.config)
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    if args.command == "info":
        show_system_info()
    
    elif args.command == "test":
        if not check_dependencies():
            sys.exit(1)
        
        if run_tests():
            print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        else:
            print("âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "init":
        if not check_dependencies():
            sys.exit(1)
        
        if initialize_system(args.vault_path, config):
            print("\nğŸ‰ Vault Intelligence System V2 ì´ˆê¸°í™” ì™„ë£Œ!")
            print("\në‹¤ìŒ ë‹¨ê³„:")
            print("1. python -m src search --query 'TDD'     # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
            print("2. python -m src duplicates               # ì¤‘ë³µ ê°ì§€")  
            print("3. python -m src collect --topic 'TDD'   # ì£¼ì œ ìˆ˜ì§‘")
            print("4. python -m src analyze                  # ì£¼ì œ ë¶„ì„")
        else:
            print("âŒ ì´ˆê¸°í™” ì‹¤íŒ¨!")
            sys.exit(1)
    
    # Phase 2 ê¸°ëŠ¥ë“¤
    elif args.command == "search":
        if not check_dependencies():
            sys.exit(1)
        
        if not args.query:
            print("âŒ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. --query ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
        
        if run_search(
            args.vault_path, 
            args.query, 
            args.top_k, 
            args.threshold, 
            config, 
            getattr(args, 'sample_size', None),
            use_reranker=args.rerank,
            search_method=args.search_method,
            use_expansion=args.expand,
            include_synonyms=not args.no_synonyms,
            include_hyde=not args.no_hyde,
            use_centrality=args.with_centrality,
            centrality_weight=args.centrality_weight
        ):
            print("âœ… ê²€ìƒ‰ ì™„ë£Œ!")
        else:
            print("âŒ ê²€ìƒ‰ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "duplicates":
        if not check_dependencies():
            sys.exit(1)
        
        if run_duplicate_detection(args.vault_path, config):
            print("âœ… ì¤‘ë³µ ê°ì§€ ì™„ë£Œ!")
        else:
            print("âŒ ì¤‘ë³µ ê°ì§€ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "collect":
        if not check_dependencies():
            sys.exit(1)
        
        if not args.topic:
            print("âŒ ìˆ˜ì§‘í•  ì£¼ì œê°€ í•„ìš”í•©ë‹ˆë‹¤. --topic ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
        
        if run_topic_collection(
            args.vault_path, 
            args.topic, 
            args.top_k, 
            args.threshold, 
            args.output, 
            config,
            use_expansion=args.expand,
            include_synonyms=not args.no_synonyms,
            include_hyde=not args.no_hyde
        ):
            print("âœ… ì£¼ì œ ìˆ˜ì§‘ ì™„ë£Œ!")
        else:
            print("âŒ ì£¼ì œ ìˆ˜ì§‘ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "analyze":
        if not check_dependencies():
            sys.exit(1)
        
        if run_topic_analysis(args.vault_path, args.output, config):
            print("âœ… ì£¼ì œ ë¶„ì„ ì™„ë£Œ!")
        else:
            print("âŒ ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "reindex":
        if not check_dependencies():
            sys.exit(1)
        
        if run_reindex(args.vault_path, args.force, config, 
                      getattr(args, 'sample_size', None),
                      getattr(args, 'include_folders', None),
                      getattr(args, 'exclude_folders', None),
                      getattr(args, 'with_colbert', False),
                      getattr(args, 'colbert_only', False)):
            print("âœ… ì¬ì¸ë±ì‹± ì™„ë£Œ!")
        else:
            print("âŒ ì¬ì¸ë±ì‹± ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "related":
        if not check_dependencies():
            sys.exit(1)
        
        if not args.file:
            print("âŒ ê¸°ì¤€ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤. --file ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
        
        if run_related_documents(
            args.vault_path,
            args.file,
            args.top_k,
            config,
            include_centrality=True,  # í•­ìƒ ì¤‘ì‹¬ì„± ì ìˆ˜ í¬í•¨
            similarity_threshold=args.similarity_threshold
        ):
            print("âœ… ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° ì™„ë£Œ!")
        else:
            print("âŒ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "analyze-gaps":
        if not check_dependencies():
            sys.exit(1)
        
        if run_knowledge_gap_analysis(
            args.vault_path,
            config,
            output_file=args.output,
            similarity_threshold=args.similarity_threshold,
            min_connections=args.min_connections
        ):
            print("âœ… ì§€ì‹ ê³µë°± ë¶„ì„ ì™„ë£Œ!")
        else:
            print("âŒ ì§€ì‹ ê³µë°± ë¶„ì„ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "tag":
        if not check_dependencies():
            sys.exit(1)
        
        # íƒœê¹… ëŒ€ìƒ í™•ì¸
        if args.target:
            target_path = args.target
        elif args.query:  # ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ í•´ë‹¹ íŒŒì¼ ì°¾ê¸°
            target_path = f"./*{args.query}*.md"
        else:
            print("âŒ íƒœê¹… ëŒ€ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("ì‚¬ìš©ë²•:")
            print("  # íŒŒì¼ëª…ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ íƒœê¹…")
            print("  python -m src tag --target spring-tdd")
            print("  python -m src tag --target my-file.md")
            print("")
            print("  # vault ìƒëŒ€ ê²½ë¡œë¡œ íƒœê¹…")
            print("  python -m src tag --target 003-RESOURCES/books/clean-code.md")
            print("  python -m src tag --target 003-RESOURCES/ --recursive")
            print("")
            print("  # ì „ì²´ vault íƒœê¹… (ì£¼ì˜!)")
            print("  python -m src tag --target . --recursive --dry-run")
            print("")
            print("  # ê°•ì œ ì¬íƒœê¹…")
            print("  python -m src tag --target my-file --tag-force")
            sys.exit(1)
        
        if run_tagging(
            vault_path=args.vault_path,
            target=target_path,
            recursive=args.recursive,
            dry_run=args.dry_run,
            force=args.tag_force,
            batch_size=args.batch_size,
            config=config
        ):
            print("âœ… ìë™ íƒœê¹… ì™„ë£Œ!")
        else:
            print("âŒ ìë™ íƒœê¹… ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "generate-moc":
        if not check_dependencies():
            sys.exit(1)
        
        if not args.topic:
            print("âŒ MOC ìƒì„±í•  ì£¼ì œê°€ í•„ìš”í•©ë‹ˆë‹¤. --topic ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            print("ì‚¬ìš©ë²•:")
            print("  python -m src generate-moc --topic 'TDD'")
            print("  python -m src generate-moc --topic 'TDD' --output 'TDD-MOC.md'")
            print("  python -m src generate-moc --topic 'TDD' --top-k 50 --include-orphans")
            sys.exit(1)
        
        if run_moc_generation(
            vault_path=args.vault_path,
            topic=args.topic,
            top_k=args.top_k,
            threshold=args.threshold,
            output_file=args.output,
            config=config,
            include_orphans=args.include_orphans,
            use_expansion=args.expand
        ):
            print("âœ… MOC ìƒì„± ì™„ë£Œ!")
        else:
            print("âŒ MOC ìƒì„± ì‹¤íŒ¨!")
            sys.exit(1)


if __name__ == "__main__":
    main()