#!/usr/bin/env python3
"""
Vault Intelligence System V2 - Main Entry Point

CLI ì¸í„°í˜ì´ìŠ¤ì™€ ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import argparse
import logging
from pathlib import Path
from typing import Optional

# ë°ì´í„° ë””ë ‰í† ë¦¬ ê²°ì • (ìºì‹œ, ì„¤ì •, ëª¨ë¸ ì €ì¥ ìœ„ì¹˜)
# ìš°ì„ ìˆœìœ„: í™˜ê²½ë³€ìˆ˜ VAULT_INTELLIGENCE_HOME > ê¸°ë³¸ê°’ ~/git/vault-intelligence
_DEFAULT_DATA_DIR = Path.home() / "git" / "vault-intelligence"


def get_data_dir(cli_data_dir: str = None) -> Path:
    """ë°ì´í„° ë””ë ‰í† ë¦¬ ê²°ì • (ìºì‹œ, ì„¤ì •, ëª¨ë¸ ì €ì¥ ìœ„ì¹˜)"""
    if cli_data_dir:
        return Path(cli_data_dir)
    env_home = os.environ.get("VAULT_INTELLIGENCE_HOME")
    if env_home:
        return Path(env_home)
    return _DEFAULT_DATA_DIR


data_dir = get_data_dir()

try:
    from src.core.sentence_transformer_engine import SentenceTransformerEngine
    from src.core.embedding_cache import EmbeddingCache
    from src.core.vault_processor import VaultProcessor
    from src.features.advanced_search import AdvancedSearchEngine, SearchQuery
    from src.features.duplicate_detector import DuplicateDetector
    from src.features.topic_collector import TopicCollector
    from src.features.topic_analyzer import TopicAnalyzer
    from src.features.semantic_tagger import SemanticTagger, TaggingResult
    from src.features.moc_generator import MOCGenerator
    from src.features.content_clusterer import ContentClusterer
    from src.features.learning_reviewer import LearningReviewer
    from src.utils.output_manager import resolve_output_path
    import yaml
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    print("í•„ìˆ˜ ì˜ì¡´ì„±ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    DEPENDENCIES_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_config(config_path: str = None) -> dict:
    """ì„¤ì • íŒŒì¼ ë¡œë”©"""
    if config_path is None:
        config_path = data_dir / "config" / "settings.yaml"
        if not config_path.exists():
            # íŒ¨í‚¤ì§€ ë‚´ ê¸°ë³¸ ì„¤ì • íŒŒì¼ ì‚¬ìš©
            config_path = Path(__file__).parent.parent / "config" / "settings.yaml"

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"ì„¤ì • íŒŒì¼ ë¡œë”© ì™„ë£Œ: {config_path}")
        return config
    except Exception as e:
        logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        return {}


def check_dependencies() -> bool:
    """í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸"""
    print("ğŸ” ì˜ì¡´ì„± í™•ì¸ ì¤‘...")
    
    if not DEPENDENCIES_AVAILABLE:
        print("âŒ í•µì‹¬ ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    missing_deps = []
    
    # ê¸°íƒ€ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    try:
        import numpy
        print(f"âœ… NumPy: {numpy.__version__}")
    except ImportError:
        missing_deps.append("numpy")
    
    try:
        import sklearn
        print(f"âœ… Scikit-learn: {sklearn.__version__}")
    except ImportError:
        missing_deps.append("scikit-learn")
    
    try:
        import yaml
        print(f"âœ… PyYAML ì‚¬ìš© ê°€ëŠ¥")
    except ImportError:
        missing_deps.append("pyyaml")
    
    if missing_deps:
        print(f"âŒ ëˆ„ë½ëœ ì˜ì¡´ì„±: {', '.join(missing_deps)}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print(f"pip install {' '.join(missing_deps)}")
        return False
    
    print("âœ… ëª¨ë“  ì˜ì¡´ì„±ì´ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
    return True


def initialize_system(vault_path: str, config: dict) -> bool:
    """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    print("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    
    try:
        # Vault ê²½ë¡œ í™•ì¸
        vault_path = Path(vault_path)
        if not vault_path.exists():
            print(f"âŒ Vault ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {vault_path}")
            return False
        
        print(f"ğŸ“ Vault ê²½ë¡œ: {vault_path}")
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        cache_dir = data_dir / "cache"
        cache_dir.mkdir(exist_ok=True)
        print(f"ğŸ’¾ ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
        models_dir = data_dir / "models"
        models_dir.mkdir(exist_ok=True)
        print(f"ğŸ¤– ëª¨ë¸ ë””ë ‰í† ë¦¬: {models_dir}")
        
        # Sentence Transformer ì—”ì§„ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        print("ğŸ§  Sentence Transformer ì—”ì§„ í…ŒìŠ¤íŠ¸ ì¤‘...")
        engine = SentenceTransformerEngine(
            model_name=config.get('model', {}).get('name', 'paraphrase-multilingual-mpnet-base-v2'),
            cache_dir=str(models_dir),
            device=config.get('model', {}).get('device')
        )
        
        # TF-IDF ì—”ì§„ì˜ ê²½ìš° í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¡œ í›ˆë ¨ í•„ìš”
        print("ğŸ“š TF-IDF ì—”ì§„ í›ˆë ¨ ì¤‘...")
        test_docs = [
            "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 1: TDDëŠ” í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œì…ë‹ˆë‹¤.",
            "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 2: ë¦¬íŒ©í† ë§ì€ ì½”ë“œ ê°œì„  ê¸°ë²•ì…ë‹ˆë‹¤.",
            "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 3: í´ë¦°ì½”ë“œëŠ” ì½ê¸° ì‰¬ìš´ ì½”ë“œë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤."
        ]
        engine.fit_documents(test_docs)
        
        # í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        test_text = "í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"
        test_embedding = engine.encode_text(test_text)
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›: {len(test_embedding)}")
        
        # ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("ğŸ’¾ ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        cache = EmbeddingCache(str(cache_dir))
        cache_stats = cache.get_statistics()
        print(f"ğŸ“Š ìºì‹œ í†µê³„: {cache_stats['total_embeddings']}ê°œ ì„ë² ë”©")
        
        # Vault í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        print("ğŸ“š Vault í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì¤‘...")
        processor = VaultProcessor(
            str(vault_path),
            excluded_dirs=config.get('vault', {}).get('excluded_dirs'),
            file_extensions=config.get('vault', {}).get('file_extensions')
        )
        
        vault_stats = processor.get_vault_statistics()
        print(f"ğŸ“ˆ Vault í†µê³„: {vault_stats['total_files']}ê°œ íŒŒì¼, "
              f"{vault_stats['total_size_mb']}MB")
        
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.exception("ì´ˆê¸°í™” ì¤‘ ìƒì„¸ ì˜¤ë¥˜:")
        return False


def run_tests():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    success_count = 0
    total_tests = 3
    
    # ì—”ì§„ í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£ Sentence Transformer ì—”ì§„ í…ŒìŠ¤íŠ¸")
    try:
        from src.core.sentence_transformer_engine import test_engine
        if test_engine():
            success_count += 1
    except Exception as e:
        print(f"ì—”ì§„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ìºì‹œ í…ŒìŠ¤íŠ¸
    print("\n2ï¸âƒ£ ì„ë² ë”© ìºì‹œ í…ŒìŠ¤íŠ¸")
    try:
        from src.core.embedding_cache import test_cache
        if test_cache():
            success_count += 1
    except Exception as e:
        print(f"ìºì‹œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ Vault í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸")
    try:
        from src.core.vault_processor import test_processor
        if test_processor():
            success_count += 1
    except Exception as e:
        print(f"í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {success_count}/{total_tests} í†µê³¼")
    return success_count == total_tests


def show_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ"""
    print("â„¹ï¸ Vault Intelligence System V2")
    print("=" * 50)
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    print(f"ìºì‹œ ë””ë ‰í† ë¦¬: {data_dir / 'cache'}")
    print(f"ì„¤ì • íŒŒì¼: {data_dir / 'config' / 'settings.yaml'}")
    print(f"Python ë²„ì „: {sys.version}")
    
    try:
        import torch
        device = "CUDA" if torch.cuda.is_available() else "CPU"
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = "MPS (Metal)"
        print(f"PyTorch ì¥ì¹˜: {device}")
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    except:
        print("PyTorch: ì‚¬ìš© ë¶ˆê°€")
    
    # ìºì‹œ ìƒíƒœ í™•ì¸
    try:
        from .core.embedding_cache import EmbeddingCache
        cache_dir = str(data_dir / "cache")
        cache = EmbeddingCache(cache_dir)
        
        print("\nğŸ’¾ ìºì‹œ ìƒíƒœ:")
        stats = cache.get_statistics()
        print(f"- Dense ì„ë² ë”©: {stats.get('total_embeddings', 0):,}ê°œ")
        
        colbert_stats = cache.get_colbert_statistics()
        print(f"- ColBERT ì„ë² ë”©: {colbert_stats.get('total_colbert_embeddings', 0):,}ê°œ")
        print(f"- ìºì‹œ DB í¬ê¸°: {stats.get('db_size', 0) / (1024*1024):.1f}MB")
        
    except Exception as e:
        print(f"\nğŸ’¾ ìºì‹œ ìƒíƒœ: í™•ì¸ ë¶ˆê°€ ({e})")
    
    print("\nğŸ¯ ì™„ë£Œëœ ê¸°ëŠ¥ (Phase 1-7):")
    print("- BGE-M3 ê¸°ë°˜ Dense + Sparse + ColBERT ê²€ìƒ‰")
    print("- Cross-encoder ì¬ìˆœìœ„í™” (BGE Reranker V2-M3)")
    print("- ì¿¼ë¦¬ í™•ì¥ (ë™ì˜ì–´ + HyDE)")
    print("- ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ ë° ê·¸ë£¹í™”")
    print("- ì£¼ì œë³„ í´ëŸ¬ìŠ¤í„°ë§ ë° ë¶„ì„")
    print("- ì§€ì‹ ê·¸ë˜í”„ ë° ê´€ë ¨ ë¬¸ì„œ ì¶”ì²œ")
    print("- ìë™ íƒœê¹… ì‹œìŠ¤í…œ")
    print("- ColBERT ì¦ë¶„ ìºì‹± ì‹œìŠ¤í…œ (ì‹ ê·œ!)")
    print()
    print("âš¡ ColBERT ê²€ìƒ‰ ëª…ë ¹ì–´:")
    print("  vault-intel reindex --with-colbert     # ColBERT í¬í•¨ ì¸ë±ì‹±")
    print("  vault-intel reindex --colbert-only     # ColBERTë§Œ ì¸ë±ì‹±")
    print("  vault-intel search --query 'TDD' --search-method colbert")
    print()
    print("âš¡ ê¸°ë³¸ ëª…ë ¹ì–´:")
    print("  vault-intel search --query 'TDD'")
    print("  vault-intel collect --topic 'ë¦¬íŒ©í† ë§'")
    print("  vault-intel duplicates")


def run_search(vault_path: str, query: str, top_k: int, threshold: float, config: dict, sample_size: Optional[int] = None, use_reranker: bool = False, search_method: str = "hybrid", use_expansion: bool = False, include_synonyms: bool = True, include_hyde: bool = True, use_centrality: bool = False, centrality_weight: float = 0.2):
    """ê²€ìƒ‰ ì‹¤í–‰"""
    try:
        print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{query}'")
        if sample_size:
            print(f"ğŸ“Š ìƒ˜í”Œë§ ëª¨ë“œ: {sample_size}ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬")
        if use_reranker:
            print("ğŸ¯ ì¬ìˆœìœ„í™” ëª¨ë“œ í™œì„±í™”")
        if use_expansion:
            print("ğŸ“ ì¿¼ë¦¬ í™•ì¥ ëª¨ë“œ í™œì„±í™”")
            expand_features = []
            if include_synonyms:
                expand_features.append("ë™ì˜ì–´")
            if include_hyde:
                expand_features.append("HyDE")
            print(f"   í™•ì¥ ê¸°ëŠ¥: {', '.join(expand_features)}")
        if use_centrality:
            print(f"ğŸ¯ ì¤‘ì‹¬ì„± ë¶€ìŠ¤íŒ… í™œì„±í™” (ê°€ì¤‘ì¹˜: {centrality_weight})")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(data_dir / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index(sample_size=sample_size):
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ê²€ìƒ‰ ìˆ˜í–‰ (í™•ì¥, ì¬ìˆœìœ„í™”, ì¤‘ì‹¬ì„± ë¶€ìŠ¤íŒ… ì˜µì…˜ í¬í•¨)
        if use_centrality:
            # ì¤‘ì‹¬ì„± ë¶€ìŠ¤íŒ… ê²€ìƒ‰
            results = search_engine.search_with_centrality_boost(
                query=query,
                search_method=search_method,
                top_k=top_k,
                centrality_weight=centrality_weight,
                threshold=threshold
            )
        elif use_expansion:
            # ì¿¼ë¦¬ í™•ì¥ì„ í¬í•¨í•œ ê²€ìƒ‰
            results = search_engine.expanded_search(
                query=query,
                search_method=search_method,
                top_k=top_k,
                threshold=threshold,
                include_synonyms=include_synonyms,
                include_hyde=include_hyde
            )
        elif use_reranker:
            # ì¬ìˆœìœ„í™”ë¥¼ í¬í•¨í•œ ê³ ê¸‰ ê²€ìƒ‰
            results = search_engine.search_with_reranking(
                query=query, 
                search_method=search_method,
                initial_k=top_k * 3,  # 3ë°°ìˆ˜ í›„ë³´ ê²€ìƒ‰
                final_k=top_k, 
                threshold=threshold,
                use_reranker=True
            )
        else:
            # ê¸°ì¡´ ê²€ìƒ‰ ë°©ë²•
            if search_method == "semantic":
                results = search_engine.semantic_search(query, top_k=top_k, threshold=threshold)
            elif search_method == "keyword":
                results = search_engine.keyword_search(query, top_k=top_k)
            elif search_method == "colbert":
                results = search_engine.colbert_search(query, top_k=top_k, threshold=threshold)
            else:  # hybrid
                results = search_engine.hybrid_search(query, top_k=top_k, threshold=threshold)
        
        if not results:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"\nğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
        print("-" * 80)
        
        for result in results:
            print(f"{result.rank}. {result.document.title}")
            print(f"   ê²½ë¡œ: {result.document.path}")
            print(f"   ìœ ì‚¬ë„: {result.similarity_score:.4f}")
            print(f"   íƒ€ì…: {result.match_type}")
            if result.matched_keywords:
                print(f"   í‚¤ì›Œë“œ: {', '.join(result.matched_keywords)}")
            if result.snippet:
                print(f"   ë‚´ìš©: {result.snippet[:100]}...")
            print()
        
        return True
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return False


def run_duplicate_detection(vault_path: str, config: dict):
    """ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ ì‹¤í–‰"""
    try:
        print("ğŸ” ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(data_dir / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ì¤‘ë³µ ê°ì§€ê¸° ì´ˆê¸°í™”
        detector = DuplicateDetector(search_engine, config)
        
        # ì¤‘ë³µ ë¶„ì„ ìˆ˜í–‰
        analysis = detector.find_duplicates()
        
        print(f"\nğŸ“Š ì¤‘ë³µ ë¶„ì„ ê²°ê³¼:")
        print("-" * 50)
        print(f"ì „ì²´ ë¬¸ì„œ: {analysis.total_documents}ê°œ")
        print(f"ì¤‘ë³µ ê·¸ë£¹: {analysis.get_group_count()}ê°œ")
        print(f"ì¤‘ë³µ ë¬¸ì„œ: {analysis.duplicate_count}ê°œ")
        print(f"ê³ ìœ  ë¬¸ì„œ: {analysis.unique_count}ê°œ")
        print(f"ì¤‘ë³µ ë¹„ìœ¨: {analysis.get_duplicate_ratio():.1%}")
        
        if analysis.duplicate_groups:
            print(f"\nğŸ“‹ ì¤‘ë³µ ê·¸ë£¹ ìƒì„¸:")
            for group in analysis.duplicate_groups[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                print(f"\nê·¸ë£¹ {group.id}:")
                print(f"  ë¬¸ì„œ ìˆ˜: {group.get_document_count()}ê°œ")
                print(f"  í‰ê·  ìœ ì‚¬ë„: {group.average_similarity:.4f}")
                for doc in group.documents:
                    print(f"    - {doc.path} ({doc.word_count}ë‹¨ì–´)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì¤‘ë³µ ê°ì§€ ì‹¤íŒ¨: {e}")
        return False


def run_topic_collection(vault_path: str, topic: str, top_k: int, threshold: float, output_file: str, config: dict, use_expansion: bool = False, include_synonyms: bool = True, include_hyde: bool = True):
    """ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ì§‘ ì‹¤í–‰"""
    try:
        print(f"ğŸ“š ì£¼ì œ '{topic}' ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘...")
        if use_expansion:
            expand_features = []
            if include_synonyms:
                expand_features.append("ë™ì˜ì–´")
            if include_hyde:
                expand_features.append("HyDE")
            print(f"ğŸ“ ì¿¼ë¦¬ í™•ì¥ ëª¨ë“œ í™œì„±í™”: {', '.join(expand_features)}")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(data_dir / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ì£¼ì œ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        collector = TopicCollector(search_engine, config)
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ê²°ì • (--output í”Œë˜ê·¸ê°€ ìˆì„ ë•Œë§Œ)
        resolved_output = resolve_output_path(vault_path, output_file, "collect", topic)

        # ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ì§‘
        collection = collector.collect_topic(
            topic=topic,
            top_k=top_k,
            threshold=threshold,
            output_file=resolved_output,
            use_expansion=use_expansion,
            include_synonyms=include_synonyms,
            include_hyde=include_hyde
        )
        
        print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print("-" * 50)
        print(f"ì£¼ì œ: {collection.metadata.topic}")
        print(f"ìˆ˜ì§‘ ë¬¸ì„œ: {collection.metadata.total_documents}ê°œ")
        print(f"ì´ ë‹¨ì–´ìˆ˜: {collection.metadata.total_word_count:,}ê°œ")
        print(f"ì´ í¬ê¸°: {collection.metadata.total_size_bytes / 1024:.1f}KB")
        
        if collection.metadata.tag_distribution:
            print(f"\nğŸ·ï¸ íƒœê·¸ ë¶„í¬:")
            for tag, count in sorted(collection.metadata.tag_distribution.items(), 
                                   key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {tag}: {count}ê°œ")
        
        if collection.metadata.directory_distribution:
            print(f"\nğŸ“ ë””ë ‰í† ë¦¬ ë¶„í¬:")
            for dir_path, count in sorted(collection.metadata.directory_distribution.items(), 
                                       key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {dir_path}: {count}ê°œ")
        
        if resolved_output:
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ {resolved_output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì£¼ì œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return False


def run_topic_analysis(vault_path: str, output_file: str, config: dict):
    """ì£¼ì œ ë¶„ì„ ì‹¤í–‰"""
    try:
        print("ğŸ” ì£¼ì œ ë¶„ì„ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(data_dir / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ì£¼ì œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = TopicAnalyzer(search_engine, config)
        
        # ì£¼ì œ ë¶„ì„ ìˆ˜í–‰ (ìƒˆë¡œìš´ ì£¼ì œ ê¸°ë°˜ ë°©ì‹ ì‚¬ìš©)
        print("ğŸ¯ ì£¼ì œ ê¸°ë°˜ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        analysis = analyzer.analyze_topics_by_predefined_subjects(min_docs_per_topic=5)
        
        print(f"\nğŸ“Š ì£¼ì œ ë¶„ì„ ê²°ê³¼:")
        print("-" * 50)
        print(f"ë¶„ì„ ë¬¸ì„œ: {analysis.total_documents}ê°œ")
        print(f"ë°œê²¬ í´ëŸ¬ìŠ¤í„°: {analysis.get_cluster_count()}ê°œ")
        print(f"í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•: {analysis.algorithm}")
        if analysis.silhouette_score is not None:
            print(f"ì‹¤ë£¨ì—£ ì ìˆ˜: {analysis.silhouette_score:.3f}")
        
        if analysis.clusters:
            print(f"\nğŸ·ï¸ ì£¼ìš” í´ëŸ¬ìŠ¤í„°ë“¤:")
            for i, cluster in enumerate(analysis.clusters[:10]):  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                print(f"\ní´ëŸ¬ìŠ¤í„° {i+1}: {cluster.label}")
                print(f"  ë¬¸ì„œ ìˆ˜: {cluster.get_document_count()}ê°œ")
                print(f"  ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(cluster.keywords[:5]) if cluster.keywords else 'ì—†ìŒ'}")
                if cluster.coherence_score is not None:
                    print(f"  ì¼ê´€ì„± ì ìˆ˜: {cluster.coherence_score:.3f}")
                if cluster.representative_doc:
                    print(f"  ëŒ€í‘œ ë¬¸ì„œ: {cluster.representative_doc.title[:50]}")
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ê²°ì • (--output í”Œë˜ê·¸ê°€ ìˆì„ ë•Œë§Œ)
        resolved_output = resolve_output_path(vault_path, output_file, "analyze")
        
        # íŒŒì¼ì´ ì €ì¥ë˜ì–´ì•¼ í•˜ëŠ” ê²½ìš°ì—ë§Œ ì €ì¥
        if resolved_output:
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥
            if resolved_output.lower().endswith('.md'):
                if analyzer.export_markdown_report(analysis, resolved_output):
                    print(f"\nğŸ’¾ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œê°€ {resolved_output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    print(f"\nâŒ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {resolved_output}")
            else:
                if analyzer.export_analysis(analysis, resolved_output):
                    print(f"\nğŸ’¾ JSON ë¶„ì„ ê²°ê³¼ê°€ {resolved_output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    print(f"\nâŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {resolved_output}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return False


def run_related_documents(vault_path: str, file_path: str, top_k: int, config: dict, 
                         include_centrality: bool = True, similarity_threshold: float = 0.3):
    """ê´€ë ¨ ë¬¸ì„œ ì¶”ì²œ ì‹¤í–‰"""
    try:
        print(f"ğŸ”— '{file_path}' ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(data_dir / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        related_results = search_engine.get_related_documents(
            document_path=file_path,
            top_k=top_k,
            include_centrality_boost=include_centrality,
            similarity_threshold=similarity_threshold
        )
        
        if not related_results:
            print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"\nğŸ“„ ê´€ë ¨ ë¬¸ì„œ ({len(related_results)}ê°œ):")
        print("-" * 80)
        
        for result in related_results:
            print(f"{result.rank}. {result.document.title}")
            print(f"   ê²½ë¡œ: {result.document.path}")
            print(f"   ê´€ë ¨ë„: {result.similarity_score:.4f}")
            print(f"   íƒ€ì…: {result.match_type}")
            if result.document.tags:
                print(f"   íƒœê·¸: {', '.join(result.document.tags)}")
            if result.snippet:
                print(f"   ë‚´ìš©: {result.snippet[:100]}...")
            print()
        
        return True
        
    except Exception as e:
        print(f"âŒ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° ì‹¤íŒ¨: {e}")
        return False


def run_knowledge_gap_analysis(vault_path: str, config: dict, output_file: str = None,
                              similarity_threshold: float = 0.3, min_connections: int = 2):
    """ì§€ì‹ ê³µë°± ë¶„ì„ ì‹¤í–‰"""
    try:
        print("ğŸ” ì§€ì‹ ê³µë°± ë¶„ì„ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(data_dir / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ì§€ì‹ ê³µë°± ë¶„ì„ ìˆ˜í–‰
        analysis = search_engine.analyze_knowledge_gaps(
            similarity_threshold=similarity_threshold,
            min_connections=min_connections
        )
        
        if not analysis:
            print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        summary = analysis.get('summary', {})
        
        print(f"\nğŸ“Š ì§€ì‹ ê³µë°± ë¶„ì„ ê²°ê³¼:")
        print("-" * 50)
        print(f"ì „ì²´ ë¬¸ì„œ: {summary.get('total_documents', 0)}ê°œ")
        print(f"ê³ ë¦½ ë¬¸ì„œ: {summary.get('isolated_count', 0)}ê°œ")
        print(f"ì•½í•œ ì—°ê²° ë¬¸ì„œ: {summary.get('weakly_connected_count', 0)}ê°œ")
        print(f"ê³ ë¦½ íƒœê·¸: {summary.get('isolated_tag_count', 0)}ê°œ")
        print(f"ê³ ë¦½ë¥ : {summary.get('isolation_rate', 0):.1%}")
        
        # ê³ ë¦½ëœ ë¬¸ì„œë“¤ ìƒì„¸ ì •ë³´
        isolated_docs = analysis.get('isolated_documents', [])
        if isolated_docs:
            print(f"\nğŸï¸ ê³ ë¦½ëœ ë¬¸ì„œë“¤:")
            for doc in isolated_docs[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                print(f"  - {doc['title']} ({doc['word_count']}ë‹¨ì–´)")
                if doc['tags']:
                    print(f"    íƒœê·¸: {', '.join(doc['tags'])}")
        
        # ì•½í•œ ì—°ê²° ë¬¸ì„œë“¤
        weak_docs = analysis.get('weakly_connected_documents', [])
        if weak_docs:
            print(f"\nğŸ”— ì•½í•œ ì—°ê²° ë¬¸ì„œë“¤:")
            for doc in weak_docs[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                print(f"  - {doc['title']} ({doc['connections']}ê°œ ì—°ê²°, {doc['word_count']}ë‹¨ì–´)")
                if doc['tags']:
                    print(f"    íƒœê·¸: {', '.join(doc['tags'])}")
        
        # ê³ ë¦½ëœ íƒœê·¸ë“¤
        isolated_tags = analysis.get('isolated_tags', {})
        if isolated_tags:
            print(f"\nğŸ·ï¸ ê³ ë¦½ëœ íƒœê·¸ë“¤ (ìƒìœ„ 10ê°œ):")
            for tag, docs in list(isolated_tags.items())[:10]:
                print(f"  - {tag}: {', '.join(docs)}")
        
        # íƒœê·¸ ë¶„í¬ (ìƒìœ„ íƒœê·¸ë“¤)
        tag_dist = analysis.get('tag_distribution', {})
        if tag_dist:
            print(f"\nğŸ“ˆ ì£¼ìš” íƒœê·¸ ë¶„í¬:")
            sorted_tags = sorted(tag_dist.items(), key=lambda x: len(x[1]), reverse=True)
            for tag, docs in sorted_tags[:10]:
                print(f"  - {tag}: {len(docs)}ê°œ ë¬¸ì„œ")
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (--output í”Œë˜ê·¸ê°€ ìˆì„ ë•Œë§Œ)
        resolved_output = resolve_output_path(vault_path, output_file, "gaps")
        if resolved_output:
            try:
                import json
                # í™•ì¥ìê°€ ì—†ìœ¼ë©´ .json ì¶”ê°€
                if not resolved_output.lower().endswith('.json'):
                    resolved_output = resolved_output.rsplit('.', 1)[0] + '.json'
                with open(resolved_output, 'w', encoding='utf-8') as f:
                    json.dump(analysis, f, indent=2, ensure_ascii=False, default=str)
                print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ê°€ {resolved_output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"\nâŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì§€ì‹ ê³µë°± ë¶„ì„ ì‹¤íŒ¨: {e}")
        return False


def run_clean_tags(vault_path: str, config: dict, dry_run: bool = True, top_k: int = 50):
    """ê³ ë¦½ íƒœê·¸ ì •ë¦¬ ì‹¤í–‰"""
    try:
        mode = "ë¯¸ë¦¬ë³´ê¸°" if dry_run else "ì‹¤í–‰"
        print(f"ğŸ·ï¸ ê³ ë¦½ íƒœê·¸ ì •ë¦¬ ({mode}) ì‹œì‘...")

        cache_dir = str(data_dir / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)

        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False

        result = search_engine.clean_isolated_tags(dry_run=dry_run, top_k=top_k)

        if not result:
            print("âŒ ê³ ë¦½ íƒœê·¸ ì •ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        total = result.get('total_isolated', 0)
        files = result.get('files_affected', 0)

        print(f"\nğŸ“Š ê³ ë¦½ íƒœê·¸ ì •ë¦¬ ê²°ê³¼:")
        print(f"  ê³ ë¦½ íƒœê·¸ ìˆ˜: {total}ê°œ")
        print(f"  ì˜í–¥ë°›ëŠ” íŒŒì¼: {files}ê°œ")

        if dry_run:
            print(f"\nğŸ’¡ ì‹¤ì œ ì œê±°í•˜ë ¤ë©´ --dry-run ì—†ì´ ì‹¤í–‰í•˜ì„¸ìš”:")
            print(f"  vault-intel clean-tags")
        else:
            removed = result.get('tags_removed', 0)
            print(f"  ì œê±°ëœ íƒœê·¸: {removed}ê°œ")

        return True

    except Exception as e:
        print(f"âŒ ê³ ë¦½ íƒœê·¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return False


def run_reindex(vault_path: str, force: bool, config: dict, sample_size: Optional[int] = None,
                include_folders: Optional[list] = None, exclude_folders: Optional[list] = None,
                with_colbert: bool = False, colbert_only: bool = False):
    """ì „ì²´ ì¬ì¸ë±ì‹± ì‹¤í–‰ (ColBERT ì§€ì›)"""
    try:
        print("ğŸ”„ ì „ì²´ ì¬ì¸ë±ì‹± ì‹œì‘...")
        if force:
            print("âš ï¸ ê°•ì œ ëª¨ë“œ: ê¸°ì¡´ ìºì‹œë¥¼ ë¬´ì‹œí•˜ê³  ëª¨ë“  ë¬¸ì„œë¥¼ ì¬ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        if sample_size:
            print(f"ğŸ“Š ìƒ˜í”Œë§ ëª¨ë“œ: {sample_size}ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        if include_folders:
            print(f"ğŸ“ í´ë” í•„í„°: {', '.join(include_folders)} í¬í•¨")
        if exclude_folders:
            print(f"ğŸš« í´ë” ì œì™¸: {', '.join(exclude_folders)}")
        if with_colbert:
            print("ğŸ¯ ColBERT ì¸ë±ì‹± í¬í•¨")
        if colbert_only:
            print("ğŸ¯ ColBERTë§Œ ì¬ì¸ë±ì‹± (Dense ì„ë² ë”© ì œì™¸)")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” (í´ë” í•„í„°ë§ ì„¤ì •)
        cache_dir = str(data_dir / "cache")
        
        # ì„ì‹œë¡œ vault ì„¤ì •ì— í´ë” í•„í„° ì¶”ê°€
        temp_config = config.copy()
        if include_folders or exclude_folders:
            temp_config['vault'] = temp_config.get('vault', {}).copy()
            if include_folders:
                temp_config['vault']['include_folders'] = include_folders
            if exclude_folders:
                temp_config['vault']['exclude_folders'] = exclude_folders
        
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, temp_config)
        
        # ì§„í–‰ë¥  í‘œì‹œ í•¨ìˆ˜
        def progress_callback(current, total):
            percentage = (current / total) * 100
            print(f"ğŸ“Š ì§„í–‰ë¥ : {current}/{total} ({percentage:.1f}%)")
        
        # Dense ì„ë² ë”© ì¸ë±ìŠ¤ êµ¬ì¶• (colbert_onlyê°€ ì•„ë‹Œ ê²½ìš°)
        if not colbert_only:
            print("ğŸ“š Dense ì„ë² ë”© ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            success = search_engine.build_index(
                force_rebuild=force, 
                progress_callback=progress_callback,
                sample_size=sample_size
            )
            
            if not success:
                print("âŒ Dense ì„ë² ë”© ì¸ë±ì‹± ì‹¤íŒ¨!")
                return False
        
        # ColBERT ì¸ë±ì‹±
        if with_colbert or colbert_only:
            print("ğŸ¯ ColBERT ì¸ë±ì‹± ì‹œì‘...")
            try:
                from .features.colbert_search import ColBERTSearchEngine
                
                colbert_config = temp_config.get('colbert', {})
                colbert_engine = ColBERTSearchEngine(
                    model_name=colbert_config.get('model_name', 'BAAI/bge-m3'),
                    device=colbert_config.get('device', temp_config.get('model', {}).get('device')),
                    use_fp16=colbert_config.get('use_fp16', True),
                    cache_folder=colbert_config.get('cache_folder', temp_config.get('model', {}).get('cache_folder')),
                    max_length=colbert_config.get('max_length', temp_config.get('model', {}).get('max_length', 4096)),
                    cache_dir=cache_dir,
                    enable_cache=colbert_config.get('enable_cache', True)
                )
                
                if colbert_engine.is_available():
                    # ë¬¸ì„œ ë¡œë“œ (search_engineì—ì„œ ê°€ì ¸ì˜¤ê¸°)
                    if hasattr(search_engine, 'documents') and search_engine.documents:
                        documents = search_engine.documents
                    else:
                        # ë¬¸ì„œë¥¼ ì§ì ‘ ë¡œë“œ
                        from .core.vault_processor import VaultProcessor
                        vault_config = temp_config.get('vault', {})
                        vault_processor = VaultProcessor(
                            vault_path=vault_path,
                            excluded_dirs=vault_config.get('excluded_dirs', None),
                            excluded_files=vault_config.get('excluded_files', None), 
                            file_extensions=vault_config.get('file_extensions', None)
                        )
                        documents = vault_processor.process_files()
                        if sample_size:
                            documents = documents[:sample_size]
                    
                    colbert_success = colbert_engine.build_index(
                        documents=documents,
                        batch_size=colbert_config.get('batch_size', 8),
                        max_documents=colbert_config.get('max_documents', None),
                        force_rebuild=force
                    )
                    
                    if colbert_success:
                        print(f"âœ… ColBERT ì¸ë±ì‹± ì™„ë£Œ!")
                    else:
                        print("âš ï¸ ColBERT ì¸ë±ì‹± ì‹¤íŒ¨, ê³„ì† ì§„í–‰...")
                else:
                    print("âš ï¸ ColBERT ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                print(f"âš ï¸ ColBERT ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ê²°ê³¼ í†µê³„ ì¶œë ¥
        if not colbert_only:
            stats = search_engine.get_search_statistics()
            print(f"\nâœ… ì¬ì¸ë±ì‹± ì™„ë£Œ!")
            print(f"ğŸ“Š Dense ì„ë² ë”© ê²°ê³¼:")
            print(f"  - ì¸ë±ì‹±ëœ ë¬¸ì„œ: {stats['indexed_documents']:,}ê°œ")
            print(f"  - ì„ë² ë”© ì°¨ì›: {stats['embedding_dimension']}ì°¨ì›")
            print(f"  - ìºì‹œëœ ì„ë² ë”©: {stats['cache_statistics']['total_embeddings']:,}ê°œ")
            print(f"  - Vault í¬ê¸°: {stats['vault_statistics']['total_size_mb']:.1f}MB")
        
        # ColBERT ìºì‹œ í†µê³„
        if with_colbert or colbert_only:
            try:
                from .core.embedding_cache import EmbeddingCache
                cache = EmbeddingCache(cache_dir)
                colbert_stats = cache.get_colbert_statistics()
                if colbert_stats.get('total_colbert_embeddings', 0) > 0:
                    print(f"ğŸ¯ ColBERT ìºì‹œ ê²°ê³¼:")
                    print(f"  - ColBERT ì„ë² ë”©: {colbert_stats['total_colbert_embeddings']:,}ê°œ")
                    print(f"  - í‰ê·  í† í° ìˆ˜: {colbert_stats['avg_tokens']}ê°œ")
                    print(f"  - ìºì‹œ íŒŒì¼ í¬ê¸°: {colbert_stats['total_file_size']:,} bytes")
            except Exception as e:
                logger.debug(f"ColBERT í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
        return False


def run_tagging(vault_path: str, target: str, recursive: bool, dry_run: bool, 
               force: bool, batch_size: int, config: dict):
    """ìë™ íƒœê¹… ì‹¤í–‰"""
    try:
        print(f"ğŸ·ï¸ ìë™ íƒœê¹… ì‹œì‘: {target}")
        if dry_run:
            print("ğŸ” ë“œë¼ì´ëŸ° ëª¨ë“œ: ì‹¤ì œ ë³€ê²½ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        if force:
            print("âš ï¸ ê°•ì œ ëª¨ë“œ: ê¸°ì¡´ íƒœê·¸ë¥¼ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        if recursive:
            print("ğŸ“ ì¬ê·€ ëª¨ë“œ: í•˜ìœ„ í´ë”ì˜ ëª¨ë“  íŒŒì¼ì„ í¬í•¨í•©ë‹ˆë‹¤.")
        
        # ì˜ë¯¸ì  íƒœê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        tagger = SemanticTagger(vault_path, config)
        
        # target ê²½ë¡œ ì²˜ë¦¬ ë° vault ë‚´ ê²€ìƒ‰
        vault_base = Path(vault_path)
        target_path = None
        
        # 1. ì ˆëŒ€ ê²½ë¡œì¸ ê²½ìš°
        if Path(target).is_absolute():
            target_path = Path(target)
            if target_path.exists():
                print(f"ğŸ“„ ì ˆëŒ€ ê²½ë¡œë¡œ íŒŒì¼ í™•ì¸: {target_path}")
            else:
                print(f"âŒ ì ˆëŒ€ ê²½ë¡œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {target}")
                return False
        
        # 2. ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° (vault ê¸°ì¤€) - ë¨¼ì € í™•ì¸
        elif "/" in target:
            candidate_path = vault_base / target
            if candidate_path.exists():
                target_path = candidate_path
                print(f"ğŸ“ Vault ìƒëŒ€ ê²½ë¡œë¡œ í™•ì¸: {target}")
            else:
                print(f"âŒ Vault ë‚´ ìƒëŒ€ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {target}")
                return False
        
        # 3. íŒŒì¼ëª…ë§Œ ì œê³µëœ ê²½ìš° - vault ì „ì²´ì—ì„œ ê²€ìƒ‰
        else:
            print(f"ğŸ” '{target}' íŒŒì¼ì„ vaultì—ì„œ ê²€ìƒ‰ ì¤‘...")
            found_files = []
            
            # .md í™•ì¥ìê°€ ì—†ìœ¼ë©´ ì¶”ê°€í•´ì„œë„ ê²€ìƒ‰
            search_patterns = [target]
            if not target.endswith('.md'):
                search_patterns.append(f"{target}.md")
            
            for pattern in search_patterns:
                # vault ì „ì²´ì—ì„œ íŒŒì¼ëª… ê²€ìƒ‰ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
                for md_file in vault_base.rglob("*.md"):
                    if md_file.name.lower() == pattern.lower():
                        found_files.append(md_file)
                    elif pattern.lower() in md_file.name.lower():
                        found_files.append(md_file)
            
            if not found_files:
                print(f"âŒ '{target}' íŒŒì¼ì„ vaultì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"   ê²€ìƒ‰ ê²½ë¡œ: {vault_base}")
                return False
            elif len(found_files) == 1:
                target_path = found_files[0]
                rel_path = target_path.relative_to(vault_base)
                print(f"âœ… íŒŒì¼ ë°œê²¬: {rel_path}")
            else:
                print(f"ğŸ“‹ '{target}' ê´€ë ¨ íŒŒì¼ì´ {len(found_files)}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
                for i, file_path in enumerate(found_files[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                    rel_path = file_path.relative_to(vault_base)
                    print(f"  {i}. {rel_path}")
                
                if len(found_files) <= 10:
                    # ì²« ë²ˆì§¸ íŒŒì¼ì„ ê¸°ë³¸ ì„ íƒ
                    target_path = found_files[0]
                    rel_path = target_path.relative_to(vault_base)
                    print(f"ğŸ¯ ì²« ë²ˆì§¸ íŒŒì¼ì„ ì„ íƒ: {rel_path}")
                else:
                    print(f"   ... ë° {len(found_files) - 10}ê°œ ë”")
                    print("âŒ ë„ˆë¬´ ë§ì€ íŒŒì¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ íŒŒì¼ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
                    return False
        
        # vault ë‚´ë¶€ ê²½ë¡œì¸ì§€ í™•ì¸
        try:
            if target_path:
                relative_path = target_path.resolve().relative_to(vault_base.resolve())
                print(f"ğŸ“ Vault ë‚´ë¶€ ê²½ë¡œ í™•ì¸: {relative_path}")
        except ValueError:
            print(f"âš ï¸ ê²½ê³ : Vault ì™¸ë¶€ ê²½ë¡œì…ë‹ˆë‹¤: {target_path}")
        
        if not target_path or not target_path.exists():
            print(f"âŒ ìµœì¢… ëŒ€ìƒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {target_path}")
            return False
        
        results = []
        
        # ë‹¨ì¼ íŒŒì¼ íƒœê¹…
        if target_path.is_file():
            if target.lower().endswith('.md'):
                print(f"ğŸ“„ ë‹¨ì¼ íŒŒì¼ íƒœê¹…: {target_path.name}")
                result = tagger.tag_document(str(target_path), dry_run=dry_run)
                results.append(result)
            else:
                print("âŒ ë§ˆí¬ë‹¤ìš´ íŒŒì¼(.md)ë§Œ íƒœê¹… ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                return False
        
        # í´ë” ë°°ì¹˜ íƒœê¹…
        elif target_path.is_dir():
            print(f"ğŸ“ í´ë” ë°°ì¹˜ íƒœê¹…: {target_path}")
            results = tagger.tag_folder(
                str(target_path), 
                recursive=recursive, 
                dry_run=dry_run
            )
        
        else:
            print("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ëŒ€ìƒ ê²½ë¡œì…ë‹ˆë‹¤.")
            return False
        
        # ê²°ê³¼ ì¶œë ¥
        if results:
            successful = sum(1 for r in results if r.success)
            total = len(results)
            
            print(f"\nğŸ“Š íƒœê¹… ê²°ê³¼:")
            print("-" * 50)
            print(f"ì²˜ë¦¬ íŒŒì¼: {total}ê°œ")
            print(f"ì„±ê³µ: {successful}ê°œ")
            print(f"ì‹¤íŒ¨: {total - successful}ê°œ")
            print(f"ì„±ê³µë¥ : {successful/total*100:.1f}%")
            
            # ìƒì„¸ ê²°ê³¼ í‘œì‹œ (ìµœëŒ€ 10ê°œ)
            print(f"\nğŸ“‹ ìƒì„¸ ê²°ê³¼ (ìƒìœ„ {min(10, len(results))}ê°œ):")
            for i, result in enumerate(results[:10]):
                print(f"\n{i+1}. {Path(result.file_path).name}")
                if result.success:
                    print(f"   âœ… ì„±ê³µ (ì²˜ë¦¬ì‹œê°„: {result.processing_time:.2f}ì´ˆ)")
                    print(f"   ê¸°ì¡´ íƒœê·¸: {len(result.original_tags)}ê°œ")
                    print(f"   ìƒì„± íƒœê·¸: {len(result.generated_tags)}ê°œ")
                    
                    if result.generated_tags:
                        print(f"   ìƒˆ íƒœê·¸:")
                        for category, tags in result.categorized_tags.items():
                            if tags:
                                print(f"     {category}: {', '.join(tags)}")
                    
                    # ì‹ ë¢°ë„ ë†’ì€ íƒœê·¸ë“¤
                    high_confidence_tags = [
                        tag for tag, score in result.confidence_scores.items()
                        if score > 0.7
                    ]
                    if high_confidence_tags:
                        print(f"   ê³ ì‹ ë¢°ë„ íƒœê·¸: {', '.join(high_confidence_tags)}")
                else:
                    print(f"   âŒ ì‹¤íŒ¨: {result.error_message}")
            
            return successful == total
        
        else:
            print("âŒ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
    except Exception as e:
        print(f"âŒ ìë™ íƒœê¹… ì‹¤íŒ¨: {e}")
        logger.exception("íƒœê¹… ì¤‘ ìƒì„¸ ì˜¤ë¥˜:")
        return False


def display_tagging_result(result: TaggingResult):
    """ë‹¨ì¼ íƒœê¹… ê²°ê³¼ í‘œì‹œ"""
    print(f"\nğŸ“„ íƒœê¹… ê²°ê³¼: {Path(result.file_path).name}")
    print("-" * 50)
    
    if result.success:
        print(f"âœ… ì„±ê³µ (ì²˜ë¦¬ì‹œê°„: {result.processing_time:.2f}ì´ˆ)")
        print(f"ê¸°ì¡´ íƒœê·¸: {result.original_tags if result.original_tags else 'ì—†ìŒ'}")
        print(f"ìƒì„± íƒœê·¸: {len(result.generated_tags)}ê°œ")
        
        if result.categorized_tags:
            print("\nğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ íƒœê·¸:")
            for category, tags in result.categorized_tags.items():
                if tags:
                    print(f"  {category}: {', '.join(tags)}")
        
        if result.confidence_scores:
            print(f"\nğŸ“Š ì‹ ë¢°ë„ ì ìˆ˜:")
            sorted_scores = sorted(
                result.confidence_scores.items(), 
                key=lambda x: x[1], 
                reverse=True
            )
            for tag, score in sorted_scores[:5]:  # ìƒìœ„ 5ê°œ
                print(f"  {tag}: {score:.3f}")
    else:
        print(f"âŒ ì‹¤íŒ¨: {result.error_message}")


def run_document_clustering(
    vault_path: str,
    config: dict,
    n_clusters: Optional[int] = None,
    algorithm: Optional[str] = None,
    topic: Optional[str] = None,
    since: Optional[str] = None,
    max_docs: Optional[int] = None,
    output_file: Optional[str] = None,
    sample_size: Optional[int] = None
):
    """ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰"""
    try:
        print("ğŸ” ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(data_dir / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # ë¬¸ì„œ ë¡œë“œ
        documents = search_engine.processor.get_all_documents()
        
        # ë‚ ì§œ í•„í„°ë§ (since ì˜µì…˜)
        if since:
            try:
                from datetime import datetime
                since_date = datetime.strptime(since, "%Y-%m-%d")
                original_count = len(documents)
                documents = [doc for doc in documents if doc.modified_at >= since_date]
                print(f"ğŸ“… ë‚ ì§œ í•„í„°ë§: {original_count}ê°œ â†’ {len(documents)}ê°œ ë¬¸ì„œ")
            except ValueError:
                print(f"âŒ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {since} (YYYY-MM-DD í˜•ì‹ì´ì–´ì•¼ í•¨)")
                return False
        
        # ì£¼ì œ í•„í„°ë§ (topic ì˜µì…˜)
        if topic:
            print(f"ğŸ¯ ì£¼ì œ '{topic}' ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§...")
            topic_results = search_engine.semantic_search(topic, top_k=1000, threshold=0.2)
            topic_paths = {result.document.path for result in topic_results}
            original_count = len(documents)
            documents = [doc for doc in documents if doc.path in topic_paths]
            print(f"ğŸ” ì£¼ì œ í•„í„°ë§: {original_count}ê°œ â†’ {len(documents)}ê°œ ë¬¸ì„œ")
        
        # ìƒ˜í”Œë§ (ì„±ëŠ¥ ìµœì í™”)
        if sample_size and len(documents) > sample_size:
            import random
            random.shuffle(documents)
            documents = documents[:sample_size]
            print(f"ğŸ“Š ìƒ˜í”Œë§: {sample_size}ê°œ ë¬¸ì„œ ì„ íƒ")
        
        if len(documents) < 3:
            print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§í•˜ê¸°ì—ëŠ” ë¬¸ì„œê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {len(documents)}ê°œ")
            return False
        
        print(f"ğŸ“š ì´ {len(documents)}ê°œ ë¬¸ì„œë¥¼ í´ëŸ¬ìŠ¤í„°ë§í•©ë‹ˆë‹¤.")
        
        # ContentClusterer ì´ˆê¸°í™”
        embedding_cache = search_engine.cache
        clustering_engine = ContentClusterer(
            search_engine.engine,
            embedding_cache,
            config
        )
        
        # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        clustering_result = clustering_engine.cluster_documents(
            documents=documents,
            algorithm=algorithm,
            n_clusters=n_clusters
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print_clustering_results(clustering_result)
        
        # ê²°ê³¼ ì €ì¥ (ìš”ì²­ ì‹œ)
        if output_file:
            save_clustering_results(clustering_result, output_file, topic)
            print(f"ğŸ’¾ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
        logger.exception("í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ìƒì„¸ ì˜¤ë¥˜:")
        return False


def print_clustering_results(clustering_result):
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì¶œë ¥"""
    print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
    print("=" * 60)
    
    summary = clustering_result.get_cluster_summary()
    print(f"ì•Œê³ ë¦¬ì¦˜: {summary['algorithm']}")
    print(f"ì´ ë¬¸ì„œ: {summary['total_documents']}ê°œ")
    print(f"í´ëŸ¬ìŠ¤í„° ìˆ˜: {summary['n_clusters']}ê°œ")
    print(f"ì‹¤ë£¨ì—£ ì ìˆ˜: {summary['silhouette_score']:.3f}")
    print(f"í‰ê·  í´ëŸ¬ìŠ¤í„° í¬ê¸°: {summary['avg_cluster_size']:.1f}ê°œ")
    
    print(f"\nğŸ“‹ í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸:")
    print("-" * 60)
    
    for i, cluster in enumerate(clustering_result.clusters, 1):
        print(f"\nğŸ¯ í´ëŸ¬ìŠ¤í„° {i}: {cluster.label}")
        print(f"   ë¬¸ì„œ ìˆ˜: {cluster.size}ê°œ")
        print(f"   ìœ ì‚¬ë„: {cluster.similarity_score:.3f}")
        
        if cluster.keywords:
            keywords = ", ".join(cluster.keywords[:5])
            print(f"   í‚¤ì›Œë“œ: {keywords}")
        
        if cluster.representative_doc:
            print(f"   ëŒ€í‘œ ë¬¸ì„œ: {cluster.representative_doc.title}")
        
        # ìƒìœ„ 3ê°œ ë¬¸ì„œ í‘œì‹œ
        print(f"   ì£¼ìš” ë¬¸ì„œ:")
        for j, doc in enumerate(cluster.documents[:3], 1):
            print(f"     {j}. {doc.title}")
        
        if cluster.size > 3:
            print(f"     ... ë° {cluster.size - 3}ê°œ ë”")


def save_clustering_results(clustering_result, output_file: str, topic: Optional[str] = None):
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥"""
    from datetime import datetime
    
    content = []
    
    # í—¤ë”
    content.append(f"# ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")
    if topic:
        content.append(f"\n**ì£¼ì œ**: {topic}")
    content.append(f"**ìƒì„±ì¼**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ìš”ì•½ ì •ë³´
    summary = clustering_result.get_cluster_summary()
    content.append(f"\n## ğŸ“Š ìš”ì•½")
    content.append(f"- **ì•Œê³ ë¦¬ì¦˜**: {summary['algorithm']}")
    content.append(f"- **ì´ ë¬¸ì„œ**: {summary['total_documents']}ê°œ")
    content.append(f"- **í´ëŸ¬ìŠ¤í„° ìˆ˜**: {summary['n_clusters']}ê°œ") 
    content.append(f"- **ì‹¤ë£¨ì—£ ì ìˆ˜**: {summary['silhouette_score']:.3f}")
    content.append(f"- **í‰ê·  í´ëŸ¬ìŠ¤í„° í¬ê¸°**: {summary['avg_cluster_size']:.1f}ê°œ")
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸
    content.append(f"\n## ğŸ“‹ í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸")
    
    for i, cluster in enumerate(clustering_result.clusters, 1):
        content.append(f"\n### ğŸ¯ í´ëŸ¬ìŠ¤í„° {i}: {cluster.label}")
        content.append(f"- **ë¬¸ì„œ ìˆ˜**: {cluster.size}ê°œ")
        content.append(f"- **ë‚´ë¶€ ìœ ì‚¬ë„**: {cluster.similarity_score:.3f}")
        
        if cluster.keywords:
            keywords = ", ".join(cluster.keywords)
            content.append(f"- **í‚¤ì›Œë“œ**: {keywords}")
        
        if cluster.representative_doc:
            content.append(f"- **ëŒ€í‘œ ë¬¸ì„œ**: {cluster.representative_doc.title}")
        
        content.append(f"\n#### ğŸ“š í¬í•¨ ë¬¸ì„œ:")
        for j, doc in enumerate(cluster.documents, 1):
            # Obsidian ë§í¬ í˜•ì‹ìœ¼ë¡œ ì €ì¥
            title = doc.title.replace('[', '').replace(']', '')  # ëŒ€ê´„í˜¸ ì œê±°
            content.append(f"{j}. [[{title}]]")
    
    # íŒŒì¼ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(content))


def run_learning_review(
    vault_path: str,
    config: dict,
    period: str = "weekly",
    start_date_str: str = None,
    end_date_str: str = None,
    topic_filter: str = None,
    output_file: str = None
):
    """í•™ìŠµ ë¦¬ë·° ì‹¤í–‰"""
    try:
        print(f"ğŸ“š {period} í•™ìŠµ ë¦¬ë·° ìƒì„± ì‹œì‘...")
        
        # ë‚ ì§œ íŒŒì‹±
        start_date = None
        end_date = None
        
        if start_date_str:
            try:
                start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
                print(f"ğŸ“… ì‹œì‘ ë‚ ì§œ: {start_date_str}")
            except ValueError:
                print(f"âŒ ì‹œì‘ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {start_date_str} (YYYY-MM-DD í˜•ì‹ì´ì–´ì•¼ í•¨)")
                return False
        
        if end_date_str:
            try:
                end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
                print(f"ğŸ“… ì¢…ë£Œ ë‚ ì§œ: {end_date_str}")
            except ValueError:
                print(f"âŒ ì¢…ë£Œ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {end_date_str} (YYYY-MM-DD í˜•ì‹ì´ì–´ì•¼ í•¨)")
                return False
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(data_dir / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # LearningReviewer ì´ˆê¸°í™”
        reviewer = LearningReviewer(search_engine, config)
        
        # í•™ìŠµ ë¦¬ë·° ìƒì„±
        review = reviewer.generate_learning_review(
            period=period,
            start_date=start_date,
            end_date=end_date,
            topic_filter=topic_filter
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print_learning_review(review)
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ê²°ì • (--output í”Œë˜ê·¸ê°€ ìˆì„ ë•Œë§Œ)
        resolved_output = resolve_output_path(vault_path, output_file, f"review-{period}")
        
        if resolved_output:
            save_learning_review(review, resolved_output)
            print(f"ğŸ’¾ í•™ìŠµ ë¦¬ë·°ê°€ {resolved_output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"âŒ í•™ìŠµ ë¦¬ë·° ìƒì„± ì‹¤íŒ¨: {e}")
        logger.exception("í•™ìŠµ ë¦¬ë·° ì¤‘ ìƒì„¸ ì˜¤ë¥˜:")
        return False


def print_learning_review(review):
    """í•™ìŠµ ë¦¬ë·° ê²°ê³¼ ì¶œë ¥"""
    print(f"\nğŸ“Š {review.period.upper()} í•™ìŠµ ë¦¬ë·°")
    print("=" * 60)
    
    print(f"ğŸ“… ê¸°ê°„: {review.start_date.strftime('%Y-%m-%d')} ~ {review.end_date.strftime('%Y-%m-%d')}")
    print(f"ğŸ“ˆ ì´ í™œë™: {review.total_activities}ê°œ")
    print(f"ğŸ“ ìƒì„± ë¬¸ì„œ: {review.documents_created}ê°œ")
    print(f"âœï¸ ìˆ˜ì • ë¬¸ì„œ: {review.documents_modified}ê°œ")
    print(f"ğŸ“† í™œë™ ì¼ìˆ˜: {review.active_days}ì¼")
    print(f"â­ í’ˆì§ˆ ì ìˆ˜: {review.quality_score:.3f}")
    
    if review.topic_progress:
        print(f"\nğŸ¯ ì£¼ì œë³„ ì§„ì „ë„ (ìƒìœ„ {min(5, len(review.topic_progress))}ê°œ):")
        print("-" * 60)
        for i, topic in enumerate(review.topic_progress[:5], 1):
            print(f"{i}. {topic.topic}")
            print(f"   ë¬¸ì„œ: {topic.documents_count}ê°œ, í™œë™: {topic.activity_count}ê°œ")
            print(f"   ì§„ì „ë„: {topic.progress_score:.3f}, ì„±ì¥ë¥ : {topic.growth_rate:.1%}")
            print(f"   í‰ê·  ë‹¨ì–´ ìˆ˜: {topic.average_word_count:.0f}ê°œ")
    
    if review.learning_insights:
        print(f"\nğŸ’¡ í•™ìŠµ ì¸ì‚¬ì´íŠ¸:")
        print("-" * 60)
        for insight in review.learning_insights:
            icon = {"strength": "ğŸ’ª", "weakness": "âš ï¸", "trend": "ğŸ“ˆ", "recommendation": "ğŸ’¡"}.get(insight.insight_type, "ğŸ“")
            print(f"{icon} {insight.title}")
            print(f"   {insight.description}")
            if insight.evidence:
                print(f"   ê·¼ê±°: {', '.join(insight.evidence)}")
    
    if review.strengths:
        print(f"\nğŸ’ª ê°•ì :")
        for strength in review.strengths:
            print(f"   â€¢ {strength}")
    
    if review.weaknesses:
        print(f"\nâš ï¸ ê°œì„ ì :")
        for weakness in review.weaknesses:
            print(f"   â€¢ {weakness}")
    
    if review.recommendations:
        print(f"\nğŸ¯ ê¶Œì¥ì‚¬í•­:")
        for recommendation in review.recommendations:
            print(f"   â€¢ {recommendation}")
    
    if review.trending_topics:
        print(f"\nğŸ”¥ íŠ¸ë Œë”© ì£¼ì œ:")
        for topic in review.trending_topics:
            print(f"   â€¢ {topic}")


def save_learning_review(review, output_file: str):
    """í•™ìŠµ ë¦¬ë·°ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥"""
    content = []
    
    # í—¤ë”
    content.append(f"# {review.period.title()} í•™ìŠµ ë¦¬ë·°")
    content.append(f"**ìƒì„±ì¼**: {review.generated_at.strftime('%Y-%m-%d %H:%M:%S')}")
    content.append(f"**ê¸°ê°„**: {review.start_date.strftime('%Y-%m-%d')} ~ {review.end_date.strftime('%Y-%m-%d')}")
    
    # ìš”ì•½ ì •ë³´
    content.append(f"\n## ğŸ“Š í•™ìŠµ í™œë™ ìš”ì•½")
    content.append(f"- **ì´ í™œë™**: {review.total_activities}ê°œ")
    content.append(f"- **ë¬¸ì„œ ìƒì„±**: {review.documents_created}ê°œ")
    content.append(f"- **ë¬¸ì„œ ìˆ˜ì •**: {review.documents_modified}ê°œ")
    content.append(f"- **í™œë™ ì¼ìˆ˜**: {review.active_days}ì¼")
    content.append(f"- **í’ˆì§ˆ ì ìˆ˜**: {review.quality_score:.3f}")
    
    # ì£¼ì œë³„ ì§„ì „ë„
    if review.topic_progress:
        content.append(f"\n## ğŸ¯ ì£¼ì œë³„ í•™ìŠµ ì§„ì „ë„")
        for i, topic in enumerate(review.topic_progress, 1):
            content.append(f"\n### {i}. {topic.topic}")
            content.append(f"- **ë¬¸ì„œ ìˆ˜**: {topic.documents_count}ê°œ")
            content.append(f"- **í™œë™ ìˆ˜**: {topic.activity_count}ê°œ")
            content.append(f"- **ì§„ì „ë„ ì ìˆ˜**: {topic.progress_score:.3f}")
            content.append(f"- **ì„±ì¥ë¥ **: {topic.growth_rate:.1%}")
            content.append(f"- **í‰ê·  ë‹¨ì–´ ìˆ˜**: {topic.average_word_count:.0f}ê°œ")
            content.append(f"- **í™œë™ ê¸°ê°„**: {topic.first_activity.strftime('%Y-%m-%d')} ~ {topic.last_activity.strftime('%Y-%m-%d')}")
    
    # í•™ìŠµ ì¸ì‚¬ì´íŠ¸
    if review.learning_insights:
        content.append(f"\n## ğŸ’¡ í•™ìŠµ ì¸ì‚¬ì´íŠ¸")
        for insight in review.learning_insights:
            content.append(f"\n### {insight.title} ({insight.insight_type.title()})")
            content.append(f"{insight.description}")
            if insight.evidence:
                content.append(f"\n**ê·¼ê±°**:")
                for evidence in insight.evidence:
                    content.append(f"- {evidence}")
            content.append(f"\n**ì‹ ë¢°ë„**: {insight.confidence_score:.3f}")
    
    # ê°•ì ê³¼ ê°œì„ ì 
    if review.strengths:
        content.append(f"\n## ğŸ’ª ê°•ì ")
        for strength in review.strengths:
            content.append(f"- {strength}")
    
    if review.weaknesses:
        content.append(f"\n## âš ï¸ ê°œì„ ì ")
        for weakness in review.weaknesses:
            content.append(f"- {weakness}")
    
    # ê¶Œì¥ì‚¬í•­
    if review.recommendations:
        content.append(f"\n## ğŸ¯ ê¶Œì¥ì‚¬í•­")
        for recommendation in review.recommendations:
            content.append(f"- {recommendation}")
    
    # íŠ¸ë Œë”© ì£¼ì œ
    if review.trending_topics:
        content.append(f"\n## ğŸ”¥ íŠ¸ë Œë”© ì£¼ì œ")
        for topic in review.trending_topics:
            content.append(f"- {topic}")
    
    # íŒŒì¼ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(content))


def _resolve_file_path(file_path: str, vault_path: str) -> Optional[str]:
    """
    íŒŒì¼ ê²½ë¡œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ë‹¤ìŒ ìˆœì„œë¡œ ì‹œë„:
    1. ì ˆëŒ€ ê²½ë¡œì¸ ê²½ìš° ì§ì ‘ í™•ì¸
    2. vault ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¡œ í™•ì¸
    3. íŒŒì¼ëª…ë§Œ ìˆëŠ” ê²½ìš° vault ì „ì²´ì—ì„œ ì¬ê·€ ê²€ìƒ‰
    
    Args:
        file_path: ì…ë ¥ëœ íŒŒì¼ ê²½ë¡œ
        vault_path: vault ë£¨íŠ¸ ê²½ë¡œ
        
    Returns:
        í•´ê²°ëœ ì ˆëŒ€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
    """
    vault_base = Path(vault_path)
    
    # 1. ì ˆëŒ€ ê²½ë¡œì¸ ê²½ìš°
    if Path(file_path).is_absolute():
        target_path = Path(file_path)
        if target_path.exists() and target_path.suffix.lower() in ['.md', '.markdown']:
            return str(target_path)
        return None
    
    # 2. vault ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš°
    if "/" in file_path or "\\" in file_path:
        candidate_path = vault_base / file_path
        if candidate_path.exists() and candidate_path.suffix.lower() in ['.md', '.markdown']:
            return str(candidate_path)
        return None
    
    # 3. íŒŒì¼ëª…ë§Œ ì œê³µëœ ê²½ìš° - vault ì „ì²´ì—ì„œ ì¬ê·€ ê²€ìƒ‰
    print(f"ğŸ” '{file_path}' íŒŒì¼ì„ vaultì—ì„œ ê²€ìƒ‰ ì¤‘...")
    
    # ê²€ìƒ‰ íŒ¨í„´ ì¤€ë¹„
    search_patterns = [file_path]
    if not file_path.endswith('.md') and not file_path.endswith('.markdown'):
        search_patterns.append(f"{file_path}.md")
        search_patterns.append(f"{file_path}.markdown")
    
    found_files = []
    
    for pattern in search_patterns:
        # vault ì „ì²´ì—ì„œ íŒŒì¼ëª… ê²€ìƒ‰ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
        for md_file in vault_base.rglob("*.md"):
            if md_file.name.lower() == pattern.lower():
                found_files.append(md_file)
        
        # ë¶€ë¶„ ë§¤ì¹˜ë„ ì‹œë„ (ì •í™•í•œ ë§¤ì¹˜ê°€ ì—†ì„ ê²½ìš°)
        if not found_files:
            for md_file in vault_base.rglob("*.md"):
                if pattern.lower() in md_file.name.lower():
                    found_files.append(md_file)
    
    # ì¤‘ë³µ ì œê±°
    found_files = list(set(found_files))
    
    if not found_files:
        return None
    elif len(found_files) == 1:
        rel_path = found_files[0].relative_to(vault_base)
        print(f"âœ… íŒŒì¼ ë°œê²¬: {rel_path}")
        return str(found_files[0])
    else:
        print(f"ğŸ“‹ '{file_path}' ê´€ë ¨ íŒŒì¼ì´ {len(found_files)}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
        for i, file_path_found in enumerate(found_files[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            rel_path = file_path_found.relative_to(vault_base)
            print(f"  {i}. {rel_path}")
        
        if len(found_files) > 10:
            print(f"   ... ë° {len(found_files) - 10}ê°œ ë”")
        
        # ê°€ì¥ ì •í™•í•œ ë§¤ì¹˜ ì„ íƒ (íŒŒì¼ëª…ì´ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²ƒ ìš°ì„ )
        exact_matches = [f for f in found_files if f.stem.lower() == Path(file_path).stem.lower()]
        if exact_matches:
            selected_file = exact_matches[0]
        else:
            selected_file = found_files[0]
        
        rel_path = selected_file.relative_to(vault_base)
        print(f"ğŸ¯ ê°€ì¥ ì í•©í•œ íŒŒì¼ì„ ì„ íƒ: {rel_path}")
        return str(selected_file)


def run_relate_docs_update(
    vault_path: str,
    config: dict,
    file_path: str = None,
    batch: bool = False,
    pattern: str = None,
    top_k: int = 5,
    threshold: float = 0.3,
    update_existing: bool = True,
    backup: bool = False,
    dry_run: bool = False,
    format_style: str = "detailed"
):
    """ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ì¶”ê°€/ì—…ë°ì´íŠ¸ ì‹¤í–‰"""
    try:
        print("ğŸ”— ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(data_dir / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # RelatedDocsFinder ì´ˆê¸°í™”
        finder_config = {
            'related_docs': {
                'default_top_k': top_k,
                'default_threshold': threshold,
                'section_title': '## ê´€ë ¨ ë¬¸ì„œ',
                'show_similarity': format_style == "detailed",
                'show_snippet': False
            }
        }
        finder_config.update(config)
        
        from src.features.related_docs_finder import RelatedDocsFinder
        finder = RelatedDocsFinder(search_engine, finder_config)
        
        if batch:
            # ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
            if not pattern:
                pattern = "*.md"  # ê¸°ë³¸ íŒ¨í„´
            
            print(f"ğŸ“ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ: íŒ¨í„´ '{pattern}'")
            if dry_run:
                print("ğŸ” ë“œë¼ì´ëŸ° ëª¨ë“œ: ì‹¤ì œ ë³€ê²½ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            
            results = finder.batch_process(
                file_patterns=[pattern],
                top_k=top_k,
                threshold=threshold,
                update_existing=update_existing,
                backup=backup,
                dry_run=dry_run
            )
            
            if not results:
                print("âŒ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥
            successful = sum(1 for r in results if r.success)
            total = len(results)
            
            print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼:")
            print("-" * 60)
            print(f"ì²˜ë¦¬ëœ íŒŒì¼: {total}ê°œ")
            print(f"ì„±ê³µ: {successful}ê°œ")
            print(f"ì‹¤íŒ¨: {total - successful}ê°œ")
            print(f"ì„±ê³µë¥ : {successful/total*100:.1f}%")
            
            # ìƒì„¸ ê²°ê³¼ (ìµœëŒ€ 10ê°œ)
            if results:
                print(f"\nğŸ“‹ ìƒì„¸ ê²°ê³¼ (ìƒìœ„ {min(10, len(results))}ê°œ):")
                for i, result in enumerate(results[:10]):
                    status = "âœ…" if result.success else "âŒ"
                    file_name = Path(result.target_file_path).name
                    print(f"{i+1}. {status} {file_name}")
                    
                    if result.success:
                        related_count = len(result.related_docs)
                        if result.section_added:
                            print(f"   ğŸ“„ ìƒˆ ì„¹ì…˜ ì¶”ê°€ ({related_count}ê°œ ê´€ë ¨ ë¬¸ì„œ)")
                        elif result.existing_section_updated:
                            print(f"   ğŸ”„ ê¸°ì¡´ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ({related_count}ê°œ ê´€ë ¨ ë¬¸ì„œ)")
                        print(f"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
                    else:
                        print(f"   âŒ ì˜¤ë¥˜: {result.error_message}")
                        
            return successful == total
        
        else:
            # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ
            if not file_path:
                print("âŒ íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤. --file ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
                return False
            
            # íŒŒì¼ ê²½ë¡œ í•´ê²° (vault ë‚´ì—ì„œ ê²€ìƒ‰ í¬í•¨)
            resolved_file_path = _resolve_file_path(file_path, vault_path)
            if not resolved_file_path:
                print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return False
            
            print(f"ğŸ“„ ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬: {Path(resolved_file_path).name}")
            vault_relative_path = Path(resolved_file_path).relative_to(Path(vault_path))
            print(f"ğŸ“ ê²½ë¡œ: {vault_relative_path}")
            
            if dry_run:
                print("ğŸ” ë“œë¼ì´ëŸ° ëª¨ë“œ: ì‹¤ì œ ë³€ê²½ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            
            result = finder.update_document(
                file_path=resolved_file_path,
                top_k=top_k,
                threshold=threshold,
                update_existing=update_existing,
                backup=backup,
                dry_run=dry_run
            )
            
            # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“„ ì²˜ë¦¬ ê²°ê³¼: {Path(result.target_file_path).name}")
            print("-" * 50)
            
            if result.success:
                print(f"âœ… ì„±ê³µ (ì²˜ë¦¬ì‹œê°„: {result.processing_time:.2f}ì´ˆ)")
                print(f"ê´€ë ¨ ë¬¸ì„œ: {len(result.related_docs)}ê°œ")
                
                if result.section_added:
                    print("ğŸ“„ ìƒˆë¡œìš´ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
                elif result.existing_section_updated:
                    print("ğŸ”„ ê¸°ì¡´ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ì„ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
                else:
                    print("â„¹ï¸ ë³€ê²½ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
                
                # ê´€ë ¨ ë¬¸ì„œ ëª©ë¡ í‘œì‹œ (ìƒìœ„ 3ê°œë§Œ)
                if result.related_docs:
                    print(f"\nğŸ”— ë°œê²¬ëœ ê´€ë ¨ ë¬¸ì„œ (ìƒìœ„ {min(3, len(result.related_docs))}ê°œ):")
                    for i, related in enumerate(result.related_docs[:3], 1):
                        print(f"  {i}. {related.document.title} (ìœ ì‚¬ë„: {related.similarity_score:.3f})")
                
            else:
                print(f"âŒ ì‹¤íŒ¨: {result.error_message}")
                
            return result.success
        
    except Exception as e:
        print(f"âŒ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        logger.exception("ê´€ë ¨ ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì¤‘ ìƒì„¸ ì˜¤ë¥˜:")
        return False


def run_moc_generation(
    vault_path: str,
    topic: str,
    top_k: int,
    threshold: float,
    output_file: Optional[str],
    config: dict,
    include_orphans: bool = False,
    use_expansion: bool = True
):
    """MOC ìƒì„± ì‹¤í–‰"""
    try:
        print(f"ğŸ“š '{topic}' MOC ìƒì„± ì‹œì‘...")
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        cache_dir = str(data_dir / "cache")
        search_engine = AdvancedSearchEngine(vault_path, cache_dir, config)
        
        if not search_engine.indexed:
            print("ğŸ“š ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            if not search_engine.build_index():
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                return False
        
        # MOC ìƒì„±ê¸° ì´ˆê¸°í™”
        moc_generator = MOCGenerator(search_engine, config)
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ê²°ì • (--output í”Œë˜ê·¸ê°€ ìˆì„ ë•Œë§Œ)
        resolved_output = resolve_output_path(vault_path, output_file, "moc", topic)
        
        # MOC ìƒì„±
        moc_data = moc_generator.generate_moc(
            topic=topic,
            top_k=top_k,
            threshold=threshold,
            include_orphans=include_orphans,
            use_expansion=use_expansion,
            output_file=resolved_output
        )
        
        print(f"\nğŸ“Š MOC ìƒì„± ê²°ê³¼:")
        print("-" * 50)
        print(f"ì£¼ì œ: {moc_data.topic}")
        print(f"ì´ ë¬¸ì„œ: {moc_data.total_documents}ê°œ")
        print(f"í•µì‹¬ ë¬¸ì„œ: {len(moc_data.core_documents)}ê°œ")
        print(f"ì¹´í…Œê³ ë¦¬: {len(moc_data.categories)}ê°œ")
        print(f"í•™ìŠµ ê²½ë¡œ: {len(moc_data.learning_path)}ë‹¨ê³„")
        print(f"ê´€ë ¨ ì£¼ì œ: {len(moc_data.related_topics)}ê°œ")
        print(f"ìµœê·¼ ì—…ë°ì´íŠ¸: {len(moc_data.recent_updates)}ê°œ")
        print(f"ë¬¸ì„œ ê´€ê³„: {len(moc_data.relationships)}ê°œ")
        
        if moc_data.categories:
            print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ë¶„í¬:")
            for category in moc_data.categories:
                print(f"  {category.name}: {len(category.documents)}ê°œ ë¬¸ì„œ")
        
        if moc_data.learning_path:
            print(f"\nğŸ›¤ï¸ í•™ìŠµ ê²½ë¡œ:")
            for step in moc_data.learning_path:
                print(f"  {step.step}. {step.title} ({step.difficulty_level}) - {len(step.documents)}ê°œ ë¬¸ì„œ")
        
        if resolved_output:
            print(f"\nğŸ’¾ MOC íŒŒì¼ì´ {resolved_output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        return True
        
    except Exception as e:
        print(f"âŒ MOC ìƒì„± ì‹¤íŒ¨: {e}")
        logger.exception("MOC ìƒì„± ì¤‘ ìƒì„¸ ì˜¤ë¥˜:")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Vault Intelligence System V2 - Sentence Transformers ê¸°ë°˜ ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œìŠ¤í…œ"
    )
    
    parser.add_argument(
        "command",
        choices=["init", "test", "info", "search", "duplicates", "collect", "analyze", "reindex", "related", "analyze-gaps", "clean-tags", "tag", "generate-moc", "summarize", "review", "add-related-docs"],
        help="ì‹¤í–‰í•  ëª…ë ¹ì–´"
    )
    
    parser.add_argument(
        "--data-dir",
        help="ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìºì‹œ, ì„¤ì •, ëª¨ë¸ ì €ì¥ ìœ„ì¹˜. ê¸°ë³¸ê°’: ~/git/vault-intelligence)"
    )

    parser.add_argument(
        "--vault-path",
        help="Vault ê²½ë¡œ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì„¤ì • íŒŒì¼ì—ì„œ ì½ìŒ)"
    )

    parser.add_argument(
        "--config",
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ"
    )

    parser.add_argument(
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥"
    )
    
    # Phase 2 ê¸°ëŠ¥ ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--query",
        help="ê²€ìƒ‰ ì¿¼ë¦¬"
    )
    
    parser.add_argument(
        "--top-k",
        type=int,
        default=10,
        help="ìƒìœ„ Kê°œ ê²°ê³¼ (ê¸°ë³¸ê°’: 10)"
    )
    
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.3,
        help="ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)"
    )
    
    parser.add_argument(
        "--rerank",
        action="store_true",
        help="ì¬ìˆœìœ„í™” í™œì„±í™” (BGE Reranker V2-M3 ì‚¬ìš©)"
    )
    
    parser.add_argument(
        "--search-method",
        choices=["semantic", "keyword", "hybrid", "colbert"],
        default="hybrid",
        help="ê²€ìƒ‰ ë°©ë²• (ê¸°ë³¸ê°’: hybrid)"
    )
    
    parser.add_argument(
        "--expand",
        action="store_true",
        help="ì¿¼ë¦¬ í™•ì¥ í™œì„±í™” (ë™ì˜ì–´ + HyDE)"
    )
    
    parser.add_argument(
        "--no-synonyms",
        action="store_true",
        help="ë™ì˜ì–´ í™•ì¥ ë¹„í™œì„±í™”"
    )
    
    parser.add_argument(
        "--no-hyde",
        action="store_true",
        help="HyDE í™•ì¥ ë¹„í™œì„±í™”"
    )
    
    parser.add_argument(
        "--topic",
        help="ìˆ˜ì§‘í•  ì£¼ì œ"
    )
    
    parser.add_argument(
        "--output",
        nargs='?',  # ì˜µì…˜ ì¸ì (í”Œë˜ê·¸ë§Œ ìˆì–´ë„ ë˜ê³ , ê°’ë„ ë°›ì„ ìˆ˜ ìˆìŒ)
        const="",   # í”Œë˜ê·¸ë§Œ ì œê³µë˜ì—ˆì„ ë•Œì˜ ê¸°ë³¸ê°’
        help="ì¶œë ¥ íŒŒì¼ ì €ì¥ (--outputë§Œ ì‚¬ìš©í•˜ë©´ ê¸°ë³¸ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥, --output filename.mdë¡œ íŒŒì¼ëª… ì§€ì • ê°€ëŠ¥)"
    )
    
    parser.add_argument(
        "--force",
        action="store_true",
        help="ê°•ì œ ì „ì²´ ì¬ì¸ë±ì‹± (ê¸°ì¡´ ìºì‹œ ë¬´ì‹œ)"
    )
    
    parser.add_argument(
        "--sample-size",
        type=int,
        help="ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜ (ëŒ€ê·œëª¨ vault ì„±ëŠ¥ ìµœì í™”ìš©)"
    )
    
    parser.add_argument(
        "--include-folders",
        nargs="+",
        help="í¬í•¨í•  í´ë” ëª©ë¡ (í´ë”ë³„ ì ì§„ì  ìƒ‰ì¸)"
    )
    
    parser.add_argument(
        "--exclude-folders", 
        nargs="+",
        help="ì œì™¸í•  í´ë” ëª©ë¡"
    )
    
    parser.add_argument(
        "--file",
        help="ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ê¸°ì¤€ íŒŒì¼ (related ëª…ë ¹ì–´ìš©)"
    )
    
    parser.add_argument(
        "--with-centrality",
        action="store_true",
        help="ì¤‘ì‹¬ì„± ì ìˆ˜ë¥¼ ê²€ìƒ‰ ë­í‚¹ì— ë°˜ì˜"
    )
    
    parser.add_argument(
        "--centrality-weight",
        type=float,
        default=0.2,
        help="ì¤‘ì‹¬ì„± ì ìˆ˜ ê°€ì¤‘ì¹˜ (0.0-1.0, ê¸°ë³¸ê°’: 0.2)"
    )
    
    parser.add_argument(
        "--similarity-threshold",
        type=float,
        default=0.3,
        help="ê´€ë ¨ì„± íŒì • ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)"
    )
    
    parser.add_argument(
        "--min-connections",
        type=int,
        default=2,
        help="ìµœì†Œ ì—°ê²° ìˆ˜ (ì´ë³´ë‹¤ ì ìœ¼ë©´ ì•½í•œ ì—°ê²°ë¡œ íŒì •, ê¸°ë³¸ê°’: 2)"
    )
    
    # ColBERT ì¸ë±ì‹± ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--with-colbert",
        action="store_true",
        help="ColBERT ì¸ë±ì‹± í¬í•¨ (reindex ëª…ë ¹ì–´ìš©)"
    )
    
    parser.add_argument(
        "--colbert-only",
        action="store_true",
        help="ColBERTë§Œ ì¬ì¸ë±ì‹± (Dense ì„ë² ë”© ì œì™¸)"
    )
    
    # íƒœê¹… ê´€ë ¨ ì¸ìë“¤ (Phase 7)
    parser.add_argument(
        "--target",
        help="íƒœê¹…í•  ëŒ€ìƒ íŒŒì¼ ë˜ëŠ” í´ë” ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--recursive",
        action="store_true", 
        help="í•˜ìœ„ í´ë” í¬í•¨ (í´ë” íƒœê¹… ì‹œ)"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ ë³€ê²½ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°"
    )
    
    parser.add_argument(
        "--tag-force",
        action="store_true",
        help="ê¸°ì¡´ íƒœê·¸ ë¬´ì‹œí•˜ê³  ì¬ìƒì„±"
    )
    
    parser.add_argument(
        "--batch-size", 
        type=int,
        default=10,
        help="ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 10)"
    )
    
    # MOC ìƒì„± ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--include-orphans",
        action="store_true",
        help="ì—°ê²°ë˜ì§€ ì•Šì€ ë¬¸ì„œë„ MOCì— í¬í•¨"
    )
    
    # Phase 9: ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ë° ìš”ì•½ ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--clusters",
        type=int,
        help="í´ëŸ¬ìŠ¤í„° ìˆ˜ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ìë™ ê²°ì •)"
    )
    
    parser.add_argument(
        "--algorithm",
        choices=["kmeans", "dbscan", "agglomerative"],
        help="í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ (ê¸°ë³¸ê°’: ì„¤ì •íŒŒì¼ ê°’)"
    )
    
    parser.add_argument(
        "--style",
        choices=["brief", "detailed", "technical", "conceptual"],
        default="detailed",
        help="ìš”ì•½ ìŠ¤íƒ€ì¼ (ê¸°ë³¸ê°’: detailed)"
    )
    
    parser.add_argument(
        "--since",
        help="íŠ¹ì • ë‚ ì§œ ì´í›„ ë¬¸ì„œë§Œ ëŒ€ìƒ (YYYY-MM-DD í˜•ì‹)"
    )
    
    parser.add_argument(
        "--max-docs",
        type=int,
        help="í´ëŸ¬ìŠ¤í„°ë³„ ìµœëŒ€ ë¬¸ì„œ ìˆ˜"
    )
    
    # Phase 9: í•™ìŠµ ë¦¬ë·° ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--period",
        choices=["weekly", "monthly", "quarterly"],
        default="weekly",
        help="ë¦¬ë·° ê¸°ê°„ (ê¸°ë³¸ê°’: weekly)"
    )
    
    parser.add_argument(
        "--from",
        dest="start_date",
        help="ë¦¬ë·° ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)"
    )
    
    parser.add_argument(
        "--to", 
        dest="end_date",
        help="ë¦¬ë·° ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)"
    )
    
    # relate-docs-update ëª…ë ¹ì–´ ê´€ë ¨ ì¸ìë“¤
    parser.add_argument(
        "--batch",
        action="store_true",
        help="ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ (ì—¬ëŸ¬ íŒŒì¼ ì¼ê´„ ì²˜ë¦¬)"
    )
    
    parser.add_argument(
        "--pattern",
        help="ë°°ì¹˜ ì²˜ë¦¬ìš© íŒŒì¼ íŒ¨í„´ (ì˜ˆ: '*.md', '000-SLIPBOX/*.md')"
    )
    
    parser.add_argument(
        "--backup",
        action="store_true",
        help="ì›ë³¸ íŒŒì¼ ë°±ì—… ìƒì„±"
    )
    
    parser.add_argument(
        "--update-existing",
        action="store_true",
        default=True,
        help="ê¸°ì¡´ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ í—ˆìš©"
    )
    
    parser.add_argument(
        "--no-update-existing",
        dest="update_existing",
        action="store_false",
        help="ê¸°ì¡´ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ"
    )
    
    parser.add_argument(
        "--format-style",
        choices=["simple", "detailed"],
        default="detailed",
        help="ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ í¬ë§· ìŠ¤íƒ€ì¼"
    )
    
    args = parser.parse_args()

    # --data-dirì´ ì§€ì •ë˜ë©´ ì „ì—­ data_dir ì—…ë°ì´íŠ¸
    global data_dir
    data_dir = get_data_dir(args.data_dir)

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # ì„¤ì • ë¡œë”©
    config = load_config(args.config)
    
    # Vault ê²½ë¡œ ê²°ì • (CLI ì¸ìê°€ ìš°ì„ , ì—†ìœ¼ë©´ ì„¤ì • íŒŒì¼ì—ì„œ ì½ê¸°)
    vault_path = args.vault_path
    if not vault_path:
        vault_path = config.get('vault', {}).get('path')
        if not vault_path:
            print("âŒ Vault ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”:")
            print("1. --vault-path ì¸ì ì‚¬ìš©: vault-intel <command> --vault-path /path/to/vault")
            print("2. config/settings.yamlì˜ vault.path ì„¤ì •")
            sys.exit(1)
    
    print(f"ğŸ“ ì‚¬ìš© ì¤‘ì¸ Vault ê²½ë¡œ: {vault_path}")
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    if args.command == "info":
        show_system_info()
    
    elif args.command == "test":
        if not check_dependencies():
            sys.exit(1)
        
        if run_tests():
            print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        else:
            print("âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "init":
        if not check_dependencies():
            sys.exit(1)
        
        if initialize_system(vault_path, config):
            print("\nğŸ‰ Vault Intelligence System V2 ì´ˆê¸°í™” ì™„ë£Œ!")
            print("\në‹¤ìŒ ë‹¨ê³„:")
            print("1. vault-intel search --query 'TDD'     # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
            print("2. vault-intel duplicates               # ì¤‘ë³µ ê°ì§€")  
            print("3. vault-intel collect --topic 'TDD'   # ì£¼ì œ ìˆ˜ì§‘")
            print("4. vault-intel analyze                  # ì£¼ì œ ë¶„ì„")
        else:
            print("âŒ ì´ˆê¸°í™” ì‹¤íŒ¨!")
            sys.exit(1)
    
    # Phase 2 ê¸°ëŠ¥ë“¤
    elif args.command == "search":
        if not check_dependencies():
            sys.exit(1)
        
        if not args.query:
            print("âŒ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. --query ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
        
        if run_search(
            vault_path, 
            args.query, 
            args.top_k, 
            args.threshold, 
            config, 
            getattr(args, 'sample_size', None),
            use_reranker=args.rerank,
            search_method=args.search_method,
            use_expansion=args.expand,
            include_synonyms=not args.no_synonyms,
            include_hyde=not args.no_hyde,
            use_centrality=args.with_centrality,
            centrality_weight=args.centrality_weight
        ):
            print("âœ… ê²€ìƒ‰ ì™„ë£Œ!")
        else:
            print("âŒ ê²€ìƒ‰ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "duplicates":
        if not check_dependencies():
            sys.exit(1)
        
        if run_duplicate_detection(vault_path, config):
            print("âœ… ì¤‘ë³µ ê°ì§€ ì™„ë£Œ!")
        else:
            print("âŒ ì¤‘ë³µ ê°ì§€ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "collect":
        if not check_dependencies():
            sys.exit(1)
        
        if not args.topic:
            print("âŒ ìˆ˜ì§‘í•  ì£¼ì œê°€ í•„ìš”í•©ë‹ˆë‹¤. --topic ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
        
        if run_topic_collection(
            vault_path, 
            args.topic, 
            args.top_k, 
            args.threshold, 
            args.output, 
            config,
            use_expansion=args.expand,
            include_synonyms=not args.no_synonyms,
            include_hyde=not args.no_hyde
        ):
            print("âœ… ì£¼ì œ ìˆ˜ì§‘ ì™„ë£Œ!")
        else:
            print("âŒ ì£¼ì œ ìˆ˜ì§‘ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "analyze":
        if not check_dependencies():
            sys.exit(1)
        
        if run_topic_analysis(vault_path, args.output, config):
            print("âœ… ì£¼ì œ ë¶„ì„ ì™„ë£Œ!")
        else:
            print("âŒ ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "reindex":
        if not check_dependencies():
            sys.exit(1)
        
        if run_reindex(vault_path, args.force, config, 
                      getattr(args, 'sample_size', None),
                      getattr(args, 'include_folders', None),
                      getattr(args, 'exclude_folders', None),
                      getattr(args, 'with_colbert', False),
                      getattr(args, 'colbert_only', False)):
            print("âœ… ì¬ì¸ë±ì‹± ì™„ë£Œ!")
        else:
            print("âŒ ì¬ì¸ë±ì‹± ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "related":
        if not check_dependencies():
            sys.exit(1)
        
        if not args.file:
            print("âŒ ê¸°ì¤€ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤. --file ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
        
        if run_related_documents(
            vault_path,
            args.file,
            args.top_k,
            config,
            include_centrality=True,  # í•­ìƒ ì¤‘ì‹¬ì„± ì ìˆ˜ í¬í•¨
            similarity_threshold=args.similarity_threshold
        ):
            print("âœ… ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° ì™„ë£Œ!")
        else:
            print("âŒ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "analyze-gaps":
        if not check_dependencies():
            sys.exit(1)
        
        if run_knowledge_gap_analysis(
            vault_path,
            config,
            output_file=args.output,
            similarity_threshold=args.similarity_threshold,
            min_connections=args.min_connections
        ):
            print("âœ… ì§€ì‹ ê³µë°± ë¶„ì„ ì™„ë£Œ!")
        else:
            print("âŒ ì§€ì‹ ê³µë°± ë¶„ì„ ì‹¤íŒ¨!")
            sys.exit(1)

    elif args.command == "clean-tags":
        if not check_dependencies():
            sys.exit(1)

        if run_clean_tags(
            vault_path,
            config,
            dry_run=args.dry_run,
            top_k=args.top_k
        ):
            print("âœ… ê³ ë¦½ íƒœê·¸ ì •ë¦¬ ì™„ë£Œ!")
        else:
            print("âŒ ê³ ë¦½ íƒœê·¸ ì •ë¦¬ ì‹¤íŒ¨!")
            sys.exit(1)

    elif args.command == "tag":
        if not check_dependencies():
            sys.exit(1)
        
        # íƒœê¹… ëŒ€ìƒ í™•ì¸
        if args.target:
            target_path = args.target
        elif args.query:  # ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ í•´ë‹¹ íŒŒì¼ ì°¾ê¸°
            target_path = f"./*{args.query}*.md"
        else:
            print("âŒ íƒœê¹… ëŒ€ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("ì‚¬ìš©ë²•:")
            print("  # íŒŒì¼ëª…ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ íƒœê¹…")
            print("  vault-intel tag --target spring-tdd")
            print("  vault-intel tag --target my-file.md")
            print("")
            print("  # vault ìƒëŒ€ ê²½ë¡œë¡œ íƒœê¹…")
            print("  vault-intel tag --target 003-RESOURCES/books/clean-code.md")
            print("  vault-intel tag --target 003-RESOURCES/ --recursive")
            print("")
            print("  # ì „ì²´ vault íƒœê¹… (ì£¼ì˜!)")
            print("  vault-intel tag --target . --recursive --dry-run")
            print("")
            print("  # ê°•ì œ ì¬íƒœê¹…")
            print("  vault-intel tag --target my-file --tag-force")
            sys.exit(1)
        
        if run_tagging(
            vault_path=vault_path,
            target=target_path,
            recursive=args.recursive,
            dry_run=args.dry_run,
            force=args.tag_force,
            batch_size=args.batch_size,
            config=config
        ):
            print("âœ… ìë™ íƒœê¹… ì™„ë£Œ!")
        else:
            print("âŒ ìë™ íƒœê¹… ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "generate-moc":
        if not check_dependencies():
            sys.exit(1)
        
        if not args.topic:
            print("âŒ MOC ìƒì„±í•  ì£¼ì œê°€ í•„ìš”í•©ë‹ˆë‹¤. --topic ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            print("ì‚¬ìš©ë²•:")
            print("  vault-intel generate-moc --topic 'TDD'")
            print("  vault-intel generate-moc --topic 'TDD' --output 'TDD-MOC.md'")
            print("  vault-intel generate-moc --topic 'TDD' --top-k 50 --include-orphans")
            sys.exit(1)
        
        if run_moc_generation(
            vault_path=vault_path,
            topic=args.topic,
            top_k=args.top_k,
            threshold=args.threshold,
            output_file=args.output,
            config=config,
            include_orphans=args.include_orphans,
            use_expansion=args.expand
        ):
            print("âœ… MOC ìƒì„± ì™„ë£Œ!")
        else:
            print("âŒ MOC ìƒì„± ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "summarize":
        if not check_dependencies():
            sys.exit(1)
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ê²°ì • (--output í”Œë˜ê·¸ê°€ ìˆì„ ë•Œë§Œ)
        output_file = resolve_output_path(vault_path, args.output, "summarize", args.topic)
        
        if run_document_clustering(
            vault_path=vault_path,
            config=config,
            n_clusters=args.clusters,
            algorithm=args.algorithm,
            topic=args.topic,
            since=args.since,
            max_docs=args.max_docs,
            output_file=output_file,
            sample_size=args.sample_size
        ):
            print("âœ… ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
            print(f"\nğŸ“ ì‚¬ìš©ë²• ì˜ˆì‹œ:")
            print("  # ê¸°ë³¸ í´ëŸ¬ìŠ¤í„°ë§")
            print("  vault-intel summarize --clusters 5")
            print("  # ì£¼ì œë³„ í´ëŸ¬ìŠ¤í„°ë§")
            print("  vault-intel summarize --topic 'TDD' --clusters 3")
            print("  # ìµœê·¼ ë¬¸ì„œë§Œ ëŒ€ìƒ")
            print("  vault-intel summarize --since '2024-01-01' --output recent-clusters.md")
        else:
            print("âŒ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "review":
        if not check_dependencies():
            sys.exit(1)
        
        if run_learning_review(
            vault_path=vault_path,
            config=config,
            period=args.period,
            start_date_str=args.start_date,
            end_date_str=args.end_date,
            topic_filter=args.topic,
            output_file=args.output
        ):
            print("âœ… í•™ìŠµ ë¦¬ë·° ì™„ë£Œ!")
            print(f"\nğŸ“ ì‚¬ìš©ë²• ì˜ˆì‹œ:")
            print("  # ì£¼ê°„ í•™ìŠµ ë¦¬ë·°")
            print("  vault-intel review --period weekly")
            print("  # ì›”ê°„ í•™ìŠµ ë¦¬ë·°")
            print("  vault-intel review --period monthly --output monthly-review.md")
            print("  # íŠ¹ì • ê¸°ê°„ ë¦¬ë·°")
            print("  vault-intel review --from 2024-08-01 --to 2024-08-31")
            print("  # ì£¼ì œë³„ í•™ìŠµ ë¦¬ë·°")
            print("  vault-intel review --topic TDD --period quarterly")
        else:
            print("âŒ í•™ìŠµ ë¦¬ë·° ì‹¤íŒ¨!")
            sys.exit(1)
    
    elif args.command == "add-related-docs":
        if not check_dependencies():
            sys.exit(1)
        
        # ë‹¨ì¼ íŒŒì¼ vs ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ ê²°ì •
        if args.batch:
            if not args.pattern:
                print("âŒ ë°°ì¹˜ ëª¨ë“œì—ì„œëŠ” --pattern ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                print("ğŸ“ ì‚¬ìš©ë²• ì˜ˆì‹œ:")
                print("  # ëª¨ë“  ë§ˆí¬ë‹¤ìš´ íŒŒì¼")
                print("  vault-intel add-related-docs --batch --pattern '*.md'")
                print("  # íŠ¹ì • í´ë”ì˜ íŒŒì¼ë“¤")
                print("  vault-intel add-related-docs --batch --pattern '000-SLIPBOX/*.md'")
                sys.exit(1)
        else:
            if not args.file:
                print("âŒ ë‹¨ì¼ íŒŒì¼ ëª¨ë“œì—ì„œëŠ” --file ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                print("ğŸ“ ì‚¬ìš©ë²• ì˜ˆì‹œ:")
                print("  # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ (íŒŒì¼ëª…ë§Œìœ¼ë¡œ ê²€ìƒ‰)")
                print("  vault-intel add-related-docs --file 'tdd-basics.md'")
                print("  # ë“œë¼ì´ëŸ°ìœ¼ë¡œ ë¯¸ë¦¬ë³´ê¸°")
                print("  vault-intel add-related-docs --file 'tdd-basics.md' --dry-run")
                sys.exit(1)
        
        if run_relate_docs_update(
            vault_path=vault_path,
            config=config,
            file_path=args.file,
            batch=args.batch,
            pattern=args.pattern,
            top_k=args.top_k,
            threshold=args.threshold,
            update_existing=args.update_existing,
            backup=args.backup,
            dry_run=args.dry_run,
            format_style=args.format_style
        ):
            print("âœ… ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
        else:
            print("âŒ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨!")
            sys.exit(1)


if __name__ == "__main__":
    main()