#!/usr/bin/env python3
"""
Advanced Embedding Engine for Vault Intelligence System V2

BGE-M3 ê¸°ë°˜ ê³ í’ˆì§ˆ ì„ë² ë”© ì‹œìŠ¤í…œ
- Dense Embeddings (ì˜ë¯¸ì  ê²€ìƒ‰)
- Sparse Embeddings (í‚¤ì›Œë“œ ê²€ìƒ‰)  
- ColBERT Embeddings (í† í° ìˆ˜ì¤€)
- Hybrid Search ì§€ì›
"""

import os
import logging
from typing import List, Optional, Union, Tuple, Dict, Any
import numpy as np
from pathlib import Path
import torch
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeElapsedColumn, TimeRemainingColumn

# BGE-M3 ëª¨ë¸
from FlagEmbedding import BGEM3FlagModel

# BM25 for sparse retrieval
from rank_bm25 import BM25Okapi

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import hashlib
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AdvancedEmbeddingEngine:
    """BGE-M3 ê¸°ë°˜ ê³ í’ˆì§ˆ ì„ë² ë”© ì—”ì§„"""
    
    def __init__(
        self, 
        model_name: str = "BAAI/bge-m3",
        cache_dir: Optional[str] = None,
        device: Optional[str] = None,
        use_fp16: bool = False,
        batch_size: int = 4,
        max_length: int = 4096,
        num_workers: int = 6
    ):
        """
        Args:
            model_name: BGE ëª¨ë¸ëª… (ê¸°ë³¸: BAAI/bge-m3)
            cache_dir: ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
            device: ê³„ì‚° ì¥ì¹˜ (auto, cpu, cuda)
            use_fp16: FP16 ì •ë°€ë„ ì‚¬ìš© ì—¬ë¶€
            batch_size: ë°°ì¹˜ í¬ê¸°
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
            num_workers: ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        """
        self.model_name = model_name
        self.cache_dir = cache_dir or "cache"
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.use_fp16 = use_fp16 and torch.cuda.is_available()
        self.batch_size = batch_size
        self.max_length = max_length
        self.num_workers = num_workers
        
        logger.info(f"BGE-M3 ì„ë² ë”© ì—”ì§„ ì´ˆê¸°í™”: {model_name}")
        logger.info(f"ì¥ì¹˜: {self.device}, FP16: {self.use_fp16}, ë°°ì¹˜í¬ê¸°: {self.batch_size}")
        logger.info(f"ìµœì í™” ì„¤ì • - í† í°ê¸¸ì´: {self.max_length}, ì›Œì»¤ìˆ˜: {self.num_workers}")
        
        # BGE-M3 ëª¨ë¸ ë¡œë”©
        try:
            self.model = BGEM3FlagModel(
                model_name, 
                use_fp16=self.use_fp16,
                device=self.device
            )
            logger.info("âœ… BGE-M3 ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ BGE-M3 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
        
        # BM25 for sparse retrieval
        self.bm25_model = None
        self.tokenized_docs = []
        
        # ë¬¸ì„œ ë°ì´í„°
        self.document_paths = []
        self.document_contents = []
        self.dense_embeddings = None
        self.sparse_embeddings = None
        self.is_fitted = False
        
        # ëª¨ë¸ ì •ë³´
        self.embedding_dimension = 1024  # BGE-M3 dense embedding dimension
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def fit_documents(self, documents: List[str], document_paths: List[str] = None, sample_size: Optional[int] = None) -> None:
        """ë¬¸ì„œë“¤ì„ ì‚¬ìš©í•´ ì„ë² ë”© ìƒì„± ë° BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
        
        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            document_paths: ë¬¸ì„œ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            sample_size: ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ì „ì²´ ì²˜ë¦¬)
        """
        logger.info(f"ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘... ({len(documents)}ê°œ ë¬¸ì„œ)")
        
        # ìƒ˜í”Œë§ ì²˜ë¦¬
        if sample_size and sample_size < len(documents):
            logger.warning(f"âš ï¸  ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ {len(documents)}ê°œ ë¬¸ì„œ ì¤‘ {sample_size}ê°œë§Œ ìƒ˜í”Œë§")
            
            # ì¸ë±ìŠ¤ ê¸°ë°˜ ê· ë“± ìƒ˜í”Œë§ (ì „ì²´ ë¬¸ì„œì—ì„œ ê· ë“±í•˜ê²Œ ì„ íƒ)
            step = len(documents) // sample_size
            sample_indices = list(range(0, len(documents), step))[:sample_size]
            
            sampled_docs = [documents[i] for i in sample_indices]
            sampled_paths = [document_paths[i] if document_paths else f"doc_{i}.md" for i in sample_indices]
            
            logger.info(f"ğŸ“Š ìƒ˜í”Œë§ ì™„ë£Œ: {len(sampled_docs)}ê°œ ë¬¸ì„œ ì„ íƒ")
            documents = sampled_docs
            document_paths = sampled_paths
        
        # ë¹ˆ ë¬¸ì„œ ì²˜ë¦¬
        processed_docs = []
        for i, doc in enumerate(documents):
            if doc and doc.strip():
                processed_docs.append(doc.strip())
            else:
                processed_docs.append(f"ë¹ˆ ë¬¸ì„œ {i}")
        
        self.document_contents = processed_docs
        self.document_paths = document_paths or [f"doc_{i}.md" for i in range(len(documents))]
        
        # Dense embeddings ìƒì„±
        logger.info("Dense embeddings ìƒì„± ì¤‘...")
        self._generate_dense_embeddings(processed_docs)
        
        # Sparse embeddings (BM25) êµ¬ì¶•
        logger.info("Sparse embeddings (BM25) êµ¬ì¶• ì¤‘...")
        self._build_bm25_index(processed_docs)
        
        self.is_fitted = True
        logger.info(f"âœ… ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
    
    def _generate_dense_embeddings(self, documents: List[str]) -> None:
        """Dense embeddings ìƒì„± (ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬ + Rich ì§„í–‰ë¥ )"""
        try:
            total_docs = len(documents)
            # ìµœì í™”ëœ ì²­í¬ í¬ê¸°: MPS ê°€ì†ì„ ìœ„í•´ ë” í° ë°°ì¹˜ í™œìš©
            chunk_size = max(200, self.batch_size * 20)  # ì²­í¬ í¬ê¸° ì¦ê°€
            
            console = Console()
            console.print(f"ğŸš€ [bold green]BGE-M3 Dense ì„ë² ë”© ìƒì„± ì‹œì‘[/bold green]")
            console.print(f"ğŸ“Š ì´ ë¬¸ì„œ: {total_docs:,}ê°œ | ë°°ì¹˜í¬ê¸°: {self.batch_size} | ì²­í¬í¬ê¸°: {chunk_size}")
            console.print(f"âš¡ ê°€ì†: MPS | í† í°ê¸¸ì´: {self.max_length} | ì›Œì»¤: {self.num_workers}")
            
            all_embeddings = []
            
            # Rich ì§„í–‰ë¥  í‘œì‹œë¡œ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TaskProgressColumn(),
                TimeElapsedColumn(),
                TimeRemainingColumn(),
                console=console
            ) as progress:
                
                task = progress.add_task("ì„ë² ë”© ìƒì„± ì¤‘...", total=total_docs)
                
                for i in range(0, total_docs, chunk_size):
                    chunk_docs = documents[i:i + chunk_size]
                    chunk_num = i // chunk_size + 1
                    total_chunks = (total_docs + chunk_size - 1) // chunk_size
                    
                    progress.update(task, description=f"ì²­í¬ {chunk_num}/{total_chunks} ì²˜ë¦¬ ì¤‘...")
                    
                    # ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
                    embeddings_result = self.model.encode(
                        chunk_docs,
                        batch_size=self.batch_size,
                        max_length=self.max_length,
                        return_dense=True,
                        return_sparse=False,
                        return_colbert_vecs=False
                    )
                    
                    all_embeddings.append(embeddings_result['dense_vecs'])
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    processed_docs = min(i + len(chunk_docs), total_docs)
                    progress.update(task, completed=processed_docs)
            
            # ëª¨ë“  ì²­í¬ì˜ ì„ë² ë”©ì„ í•©ì¹¨
            self.dense_embeddings = np.vstack(all_embeddings) if all_embeddings else np.zeros((0, self.embedding_dimension))
            
            console.print(f"âœ… [bold green]Dense embeddings ìƒì„± ì™„ë£Œ[/bold green]: {self.dense_embeddings.shape}")
            console.print(f"ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ: MPS ê°€ì† + ë°°ì¹˜í¬ê¸° {self.batch_size} + ì²­í¬ ìµœì í™”")
            
        except Exception as e:
            logger.error(f"Dense embeddings ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ì œë¡œ ë²¡í„° ìƒì„±
            self.dense_embeddings = np.zeros((len(documents), self.embedding_dimension))
    
    def _build_bm25_index(self, documents: List[str]) -> None:
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            # ë¬¸ì„œë¥¼ í† í°í™” (ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜ ë¶„í• )
            self.tokenized_docs = []
            for doc in documents:
                # í•œêµ­ì–´ì™€ ì˜ì–´ ëª¨ë‘ ì§€ì›í•˜ëŠ” ê°„ë‹¨í•œ í† í°í™”
                tokens = doc.lower().split()
                self.tokenized_docs.append(tokens)
            
            # BM25 ëª¨ë¸ êµ¬ì¶•
            self.bm25_model = BM25Okapi(self.tokenized_docs)
            logger.info(f"BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.tokenized_docs)}ê°œ ë¬¸ì„œ")
            
        except Exception as e:
            logger.error(f"BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self.bm25_model = None
    
    def encode_text(self, text: str) -> np.ndarray:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ì˜ dense embedding ìƒì„±"""
        try:
            if not text or not text.strip():
                text = "ë¹ˆ í…ìŠ¤íŠ¸"
            
            # BGE-M3ë¡œ dense embedding ìƒì„±
            result = self.model.encode(
                [text.strip()],
                batch_size=1,
                max_length=self.max_length,
                return_dense=True,
                return_sparse=False,
                return_colbert_vecs=False
            )
            
            return result['dense_vecs'][0]
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros(self.embedding_dimension)
    
    def encode_texts(
        self, 
        texts: List[str], 
        batch_size: int = None,
        show_progress: bool = True
    ) -> np.ndarray:
        """ë‹¤ì¤‘ í…ìŠ¤íŠ¸ì˜ ë°°ì¹˜ dense embedding ìƒì„±"""
        try:
            if batch_size is None:
                batch_size = self.batch_size
            
            # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            processed_texts = []
            for i, text in enumerate(texts):
                if text and text.strip():
                    processed_texts.append(text.strip())
                else:
                    processed_texts.append(f"ë¹ˆ í…ìŠ¤íŠ¸ {i}")
            
            # BGE-M3ë¡œ ë°°ì¹˜ embedding ìƒì„±
            result = self.model.encode(
                processed_texts,
                batch_size=batch_size,
                max_length=self.max_length,
                return_dense=True,
                return_sparse=False,
                return_colbert_vecs=False
            )
            
            if show_progress:
                logger.info(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
            
            return result['dense_vecs']
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return np.zeros((len(texts), self.embedding_dimension))
    
    def semantic_search(
        self, 
        query: str, 
        top_k: int = 10, 
        threshold: float = 0.0
    ) -> List[Tuple[str, float]]:
        """Dense embedding ê¸°ë°˜ ì˜ë¯¸ì  ê²€ìƒ‰"""
        if not self.is_fitted:
            raise ValueError("ë¨¼ì € fit_documents()ë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.encode_text(query)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity([query_embedding], self.dense_embeddings)[0]
            
            # ì„ê³„ê°’ ì´ìƒë§Œ í•„í„°ë§
            valid_indices = np.where(similarities >= threshold)[0]
            if len(valid_indices) == 0:
                return []
            
            valid_similarities = similarities[valid_indices]
            
            # ìƒìœ„ kê°œ ì„ íƒ
            if len(valid_indices) > top_k:
                top_local_indices = np.argsort(valid_similarities)[::-1][:top_k]
                top_indices = valid_indices[top_local_indices]
                top_similarities = valid_similarities[top_local_indices]
            else:
                sort_order = np.argsort(valid_similarities)[::-1]
                top_indices = valid_indices[sort_order]
                top_similarities = valid_similarities[sort_order]
            
            # ê²°ê³¼ ìƒì„±
            results = []
            for idx, sim in zip(top_indices, top_similarities):
                if idx < len(self.document_paths):
                    results.append((self.document_paths[idx], float(sim)))
            
            return results
            
        except Exception as e:
            logger.error(f"ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def keyword_search(
        self, 
        query: str, 
        top_k: int = 10
    ) -> List[Tuple[str, float]]:
        """BM25 ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        if not self.is_fitted or self.bm25_model is None:
            logger.warning("BM25 ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ. ë¹ˆ ê²°ê³¼ ë°˜í™˜.")
            return []
        
        try:
            # ì¿¼ë¦¬ í† í°í™”
            query_tokens = query.lower().split()
            
            # BM25 ìŠ¤ì½”ì–´ ê³„ì‚°
            scores = self.bm25_model.get_scores(query_tokens)
            
            # ìƒìœ„ kê°œ ì„ íƒ
            top_indices = np.argsort(scores)[::-1][:top_k]
            
            # ê²°ê³¼ ìƒì„±
            results = []
            for idx in top_indices:
                if idx < len(self.document_paths) and scores[idx] > 0:
                    results.append((self.document_paths[idx], float(scores[idx])))
            
            return results
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def hybrid_search(
        self, 
        query: str, 
        top_k: int = 10,
        semantic_weight: float = 0.7,
        keyword_weight: float = 0.3,
        threshold: float = 0.0
    ) -> List[Tuple[str, float]]:
        """Hybrid search: Dense + Sparse (RRF ê¸°ë°˜ ìœµí•©)"""
        try:
            # ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼
            semantic_results = self.semantic_search(query, top_k=top_k*2, threshold=threshold)
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
            keyword_results = self.keyword_search(query, top_k=top_k*2)
            
            # ê²°ê³¼ ìœµí•© (RRF - Reciprocal Rank Fusion)
            final_scores = {}
            
            # ì˜ë¯¸ì  ê²€ìƒ‰ ì ìˆ˜ ë°˜ì˜
            for rank, (path, score) in enumerate(semantic_results):
                if path not in final_scores:
                    final_scores[path] = 0
                # RRF ìŠ¤ì½”ì–´: 1 / (rank + 60)
                rrf_score = 1.0 / (rank + 60)
                final_scores[path] += semantic_weight * rrf_score
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ì ìˆ˜ ë°˜ì˜
            for rank, (path, score) in enumerate(keyword_results):
                if path not in final_scores:
                    final_scores[path] = 0
                rrf_score = 1.0 / (rank + 60)
                final_scores[path] += keyword_weight * rrf_score
            
            # ìµœì¢… ê²°ê³¼ ì •ë ¬
            sorted_results = sorted(
                final_scores.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:top_k]
            
            return sorted_results
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # í´ë°±: ì˜ë¯¸ì  ê²€ìƒ‰ë§Œ ì‚¬ìš©
            return self.semantic_search(query, top_k, threshold)
    
    def calculate_similarity(
        self, 
        embedding1: np.ndarray, 
        embedding2: np.ndarray
    ) -> float:
        """ë‘ ì„ë² ë”© ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            similarity = cosine_similarity([embedding1], [embedding2])[0][0]
            return float(similarity)
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def calculate_similarities(
        self, 
        query_embedding: np.ndarray, 
        document_embeddings: np.ndarray
    ) -> np.ndarray:
        """ì¿¼ë¦¬ì™€ ì—¬ëŸ¬ ë¬¸ì„œ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            similarities = cosine_similarity([query_embedding], document_embeddings)[0]
            return similarities
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.zeros(len(document_embeddings))
    
    def find_most_similar(
        self,
        query_embedding: np.ndarray,
        document_embeddings: np.ndarray,
        top_k: int = 10
    ) -> List[Tuple[int, float]]:
        """ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì˜ ì¸ë±ìŠ¤ì™€ ìœ ì‚¬ë„ ë°˜í™˜"""
        try:
            similarities = self.calculate_similarities(query_embedding, document_embeddings)
            
            # ìƒìœ„ kê°œ ì„ íƒ
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = [(int(idx), float(similarities[idx])) for idx in top_indices]
            return results
        except Exception as e:
            logger.error(f"ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_documents(self, query: str, top_k: int = 10, threshold: float = 0.0) -> List[Tuple[str, float]]:
        """ê¸°ë³¸ ê²€ìƒ‰ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš©)"""
        return self.hybrid_search(query, top_k, threshold=threshold)
    
    def get_model_info(self) -> dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "embedding_dimension": self.embedding_dimension,
            "device": self.device,
            "use_fp16": self.use_fp16,
            "batch_size": self.batch_size,
            "cache_dir": self.cache_dir,
            "is_fitted": self.is_fitted,
            "document_count": len(self.document_paths) if self.document_paths else 0,
            "has_bm25": self.bm25_model is not None
        }
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # ê¸°ë³¸ì ì¸ ì „ì²˜ë¦¬
        text = text.strip()
        return text
    
    def save_model(self, filepath: str) -> None:
        """ëª¨ë¸ ì €ì¥ (ì„ë² ë”©ê³¼ BM25 ì¸ë±ìŠ¤ë§Œ)"""
        try:
            model_data = {
                'model_name': self.model_name,
                'document_paths': self.document_paths,
                'document_contents': self.document_contents,
                'dense_embeddings': self.dense_embeddings,
                'tokenized_docs': self.tokenized_docs,
                'is_fitted': self.is_fitted,
                'embedding_dimension': self.embedding_dimension,
                'device': self.device,
                'use_fp16': self.use_fp16,
                'batch_size': self.batch_size
            }
            
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            
            logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def load_model(self, filepath: str) -> None:
        """ëª¨ë¸ ë¡œë”©"""
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.model_name = model_data['model_name']
            self.document_paths = model_data['document_paths']
            self.document_contents = model_data['document_contents']
            self.dense_embeddings = model_data['dense_embeddings']
            self.tokenized_docs = model_data['tokenized_docs']
            self.is_fitted = model_data['is_fitted']
            self.embedding_dimension = model_data['embedding_dimension']
            
            # BM25 ëª¨ë¸ ì¬êµ¬ì¶•
            if self.tokenized_docs:
                self.bm25_model = BM25Okapi(self.tokenized_docs)
            
            logger.info(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {filepath}")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise


# ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
SentenceTransformerEngine = AdvancedEmbeddingEngine


def test_engine():
    """ì—”ì§„ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    try:
        # ì—”ì§„ ì´ˆê¸°í™”
        engine = AdvancedEmbeddingEngine()
        
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
        test_docs = [
            "TDDëŠ” í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ë¨¼ì € í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ê³  ì½”ë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.",
            "ë¦¬íŒ©í† ë§ì€ ì½”ë“œì˜ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ê¸°ëŠ¥ì€ ë³€ê²½í•˜ì§€ ì•Šê³  ì½”ë“œ í’ˆì§ˆì„ ë†’ì…ë‹ˆë‹¤.",
            "Clean CodeëŠ” ì½ê¸° ì‰¬ìš´ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì¢‹ì€ ë„¤ì´ë°ê³¼ ê°„ê²°í•œ í•¨ìˆ˜ê°€ í•µì‹¬ì…ë‹ˆë‹¤.",
            "Spring FrameworkëŠ” ìë°” ê¸°ë°˜ì˜ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
            "Pythonì€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤."
        ]
        test_paths = [f"doc_{i}.md" for i in range(len(test_docs))]
        
        # ë¬¸ì„œ í›ˆë ¨
        engine.fit_documents(test_docs, test_paths)
        print(f"âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {engine.embedding_dimension}ì°¨ì›")
        
        # ì˜ë¯¸ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        semantic_results = engine.semantic_search("í…ŒìŠ¤íŠ¸ ê°œë°œ ë°©ë²•", top_k=3)
        print(f"âœ… ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼:")
        for path, score in semantic_results:
            print(f"  - {path}: {score:.4f}")
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        keyword_results = engine.keyword_search("TDD í…ŒìŠ¤íŠ¸", top_k=3)
        print(f"âœ… í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼:")
        for path, score in keyword_results:
            print(f"  - {path}: {score:.4f}")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        hybrid_results = engine.hybrid_search("í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ", top_k=3)
        print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼:")
        for path, score in hybrid_results:
            print(f"  - {path}: {score:.4f}")
        
        # ë‹¨ì¼ ì„ë² ë”© í…ŒìŠ¤íŠ¸
        single_embedding = engine.encode_text("ê°œë°œ ë°©ë²•ë¡ ")
        print(f"âœ… ë‹¨ì¼ ì„ë² ë”© ì°¨ì›: {len(single_embedding)}")
        
        # ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
        emb1 = engine.encode_text("TDD í…ŒìŠ¤íŠ¸")
        emb2 = engine.encode_text("í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ")
        similarity = engine.calculate_similarity(emb1, emb2)
        print(f"âœ… ìœ ì‚¬ë„ ('TDD í…ŒìŠ¤íŠ¸' vs 'í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ'): {similarity:.4f}")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        model_info = engine.get_model_info()
        print(f"âœ… ëª¨ë¸ ì •ë³´: {model_info}")
        
        print("âœ… ì—”ì§„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì—”ì§„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    test_engine()