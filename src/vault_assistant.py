#!/usr/bin/env python3
"""
Vault Assistant - Unified CLI for Vault Intelligence System V2

ëª¨ë“  ê¸°ëŠ¥ì„ í†µí•©í•œ ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤
"""

import sys
import os
import argparse
import yaml
import logging
from typing import Optional, Dict, Any
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from src.core.sentence_transformer_engine import SentenceTransformerEngine
    from src.core.embedding_cache import EmbeddingCache
    from src.core.vault_processor import VaultProcessor
    from src.features.advanced_search import AdvancedSearchEngine, SearchQuery
    from src.features.duplicate_detector import DuplicateDetector
    from src.features.topic_analyzer import TopicAnalyzer
    from src.features.topic_collector import TopicCollector
    from src.features.knowledge_graph import KnowledgeGraphBuilder
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    print("í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ê±°ë‚˜ PYTHONPATHë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class VaultAssistant:
    """Vault ê´€ë¦¬ í†µí•© CLI"""
    
    def __init__(self, vault_path: str, config_path: Optional[str] = None):
        """
        Args:
            vault_path: Obsidian vault ê²½ë¡œ
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.vault_path = Path(vault_path)
        self.project_root = Path(__file__).parent.parent
        self.config = self._load_config(config_path)
        
        # ìºì‹œ ë””ë ‰í† ë¦¬
        self.cache_dir = self.project_root / "cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.search_engine = None
        self.duplicate_detector = None
        self.topic_analyzer = None
        self.topic_collector = None
        self.graph_builder = None
        
        logger.info(f"VaultAssistant ì´ˆê¸°í™”: {vault_path}")
    
    def _load_config(self, config_path: Optional[str] = None) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë”©"""
        if config_path is None:
            config_path = self.project_root / "config" / "settings.yaml"
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"ì„¤ì • íŒŒì¼ ë¡œë”© ì™„ë£Œ: {config_path}")
            return config
        except Exception as e:
            logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return self._default_config()
    
    def _default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì •"""
        return {
            "model": {
                "name": "paraphrase-multilingual-mpnet-base-v2",
                "dimension": 768,
                "batch_size": 32
            },
            "search": {
                "default_top_k": 10,
                "similarity_threshold": 0.3
            },
            "duplicates": {
                "similarity_threshold": 0.85,
                "min_word_count": 50
            },
            "clustering": {
                "default_n_clusters": 5,
                "algorithm": "kmeans"
            },
            "collection": {
                "min_documents": 3,
                "group_by_tags": True
            },
            "graph": {
                "similarity_threshold": 0.4,
                "include_tag_nodes": True
            }
        }
    
    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (lazy loading)"""
        if self.search_engine is None:
            logger.info("ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
            self.search_engine = AdvancedSearchEngine(
                str(self.vault_path),
                str(self.cache_dir),
                self.config
            )
            
            self.duplicate_detector = DuplicateDetector(self.search_engine, self.config)
            self.topic_analyzer = TopicAnalyzer(self.search_engine, self.config)
            self.topic_collector = TopicCollector(self.search_engine, self.config)
            self.graph_builder = KnowledgeGraphBuilder(self.search_engine, self.config)
    
    def build_index(self, force_rebuild: bool = False) -> bool:
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            self._initialize_components()
            
            def progress_callback(current, total):
                if current % 100 == 0:
                    print(f"ì§„í–‰ë¥ : {current}/{total} ({current/total*100:.1f}%)")
            
            print("ğŸ”„ ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
            success = self.search_engine.build_index(force_rebuild, progress_callback)
            
            if success:
                print("âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
                self._print_index_stats()
            else:
                print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨!")
            
            return success
            
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def search(
        self,
        query: str,
        top_k: Optional[int] = None,
        search_type: str = "hybrid",
        threshold: Optional[float] = None,
        output_file: Optional[str] = None
    ) -> bool:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            self._initialize_components()
            
            if not self.search_engine.indexed:
                print("âŒ ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € 'index' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            
            top_k = top_k or self.config['search']['default_top_k']
            threshold = threshold or self.config['search']['similarity_threshold']
            
            print(f"ğŸ” '{query}' ê²€ìƒ‰ ì¤‘ ({search_type})...")
            
            # ê²€ìƒ‰ íƒ€ì…ë³„ ì‹¤í–‰
            if search_type == "semantic":
                results = self.search_engine.semantic_search(query, top_k, threshold)
            elif search_type == "keyword":
                results = self.search_engine.keyword_search(query, top_k)
            elif search_type == "hybrid":
                results = self.search_engine.hybrid_search(query, top_k, threshold=threshold)
            else:
                print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ê²€ìƒ‰ íƒ€ì…: {search_type}")
                return False
            
            if not results:
                print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
            print("=" * 80)
            
            for result in results:
                print(f"\n{result.rank}. **{result.document.title}**")
                print(f"   ğŸ“„ ê²½ë¡œ: {result.document.path}")
                print(f"   ğŸ“Š ìœ ì‚¬ë„: {result.similarity_score:.4f} ({result.match_type})")
                print(f"   ğŸ“ ë‹¨ì–´ ìˆ˜: {result.document.word_count}")
                
                if result.matched_keywords:
                    print(f"   ğŸ” ë§¤ì¹­ í‚¤ì›Œë“œ: {', '.join(result.matched_keywords)}")
                
                if result.document.tags:
                    print(f"   ğŸ·ï¸ íƒœê·¸: {', '.join(result.document.tags[:5])}")
                
                if result.snippet:
                    print(f"   ğŸ’¬ ë¯¸ë¦¬ë³´ê¸°: {result.snippet}")
            
            # íŒŒì¼ ì €ì¥
            if output_file:
                self._save_search_results(results, query, output_file)
                print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def find_duplicates(
        self,
        threshold: Optional[float] = None,
        min_word_count: Optional[int] = None,
        output_file: Optional[str] = None
    ) -> bool:
        """ì¤‘ë³µ ë¬¸ì„œ ê°ì§€"""
        try:
            self._initialize_components()
            
            if not self.search_engine.indexed:
                print("âŒ ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € 'index' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            
            threshold = threshold or self.config['duplicates']['similarity_threshold']
            min_word_count = min_word_count or self.config['duplicates']['min_word_count']
            
            print(f"ğŸ” ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ ì¤‘ (ì„ê³„ê°’: {threshold})...")
            
            analysis = self.duplicate_detector.find_duplicates(threshold, min_word_count)
            
            print(f"\nğŸ“Š ì¤‘ë³µ ë¶„ì„ ê²°ê³¼:")
            print("=" * 60)
            print(f"ğŸ“ ì´ ë¬¸ì„œ: {analysis.total_documents}ê°œ")
            print(f"ğŸ”— ì¤‘ë³µ ê·¸ë£¹: {analysis.get_group_count()}ê°œ")
            print(f"ğŸ“‹ ì¤‘ë³µ ë¬¸ì„œ: {analysis.duplicate_count}ê°œ")
            print(f"ğŸ“„ ê³ ìœ  ë¬¸ì„œ: {analysis.unique_count}ê°œ")
            print(f"ğŸ“Š ì¤‘ë³µë¥ : {analysis.get_duplicate_ratio():.1%}")
            print(f"ğŸ’¾ ì ì¬ì  ì ˆì•½: {analysis.potential_savings_mb:.2f}MB")
            
            if analysis.duplicate_groups:
                print(f"\nğŸ”— ì¤‘ë³µ ê·¸ë£¹ ìƒì„¸:")
                
                for i, group in enumerate(analysis.duplicate_groups):
                    print(f"\nê·¸ë£¹ {i+1} (í‰ê·  ìœ ì‚¬ë„: {group.average_similarity:.3f}):")
                    print(f"   ğŸ‘‘ ë§ˆìŠ¤í„°: {group.master_document.title if group.master_document else 'None'}")
                    
                    for doc in group.documents:
                        marker = "ğŸ‘‘" if group.master_document and doc.path == group.master_document.path else "ğŸ“„"
                        print(f"   {marker} {doc.title} ({doc.word_count} ë‹¨ì–´)")
                    
                    # ë³‘í•© ì œì•ˆ
                    suggestions = self.duplicate_detector.generate_merge_suggestions(group)
                    if 'potential_savings_mb' in suggestions:
                        print(f"   ğŸ’¡ ì ˆì•½ ê°€ëŠ¥: {suggestions['potential_savings_mb']:.2f}MB")
            
            # íŒŒì¼ ì €ì¥
            if output_file:
                if self.duplicate_detector.export_analysis(analysis, output_file):
                    print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ê°ì§€ ì‹¤íŒ¨: {e}")
            print(f"âŒ ì¤‘ë³µ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def analyze_topics(
        self,
        query: Optional[str] = None,
        n_clusters: Optional[int] = None,
        algorithm: Optional[str] = None,
        output_file: Optional[str] = None,
        visualize: bool = False
    ) -> bool:
        """ì£¼ì œ ë¶„ì„"""
        try:
            self._initialize_components()
            
            if not self.search_engine.indexed:
                print("âŒ ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € 'index' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            
            n_clusters = n_clusters or self.config['clustering']['default_n_clusters']
            algorithm = algorithm or self.config['clustering']['algorithm']
            
            if query:
                print(f"ğŸ” '{query}' ì£¼ì œ ë¶„ì„ ì¤‘...")
            else:
                print(f"ğŸ” ì „ì²´ ë¬¸ì„œ ì£¼ì œ ë¶„ì„ ì¤‘...")
            
            analysis = self.topic_analyzer.analyze_topics(query, n_clusters, algorithm)
            
            print(f"\nğŸ“Š ì£¼ì œ ë¶„ì„ ê²°ê³¼:")
            print("=" * 60)
            print(f"ğŸ“ ë¶„ì„ ë¬¸ì„œ: {analysis.total_documents}ê°œ")
            print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„°: {analysis.get_cluster_count()}ê°œ")
            print(f"ğŸ“ˆ ì‹¤ë£¨ì—£ ì ìˆ˜: {analysis.silhouette_score:.3f}" if analysis.silhouette_score else "")
            print(f"ğŸ¤– ì•Œê³ ë¦¬ì¦˜: {analysis.algorithm}")
            
            if analysis.clusters:
                print(f"\nğŸ¯ í´ëŸ¬ìŠ¤í„° ìƒì„¸:")
                
                for i, cluster in enumerate(analysis.clusters):
                    print(f"\ní´ëŸ¬ìŠ¤í„° {i+1}: {cluster.label}")
                    print(f"   ğŸ“„ ë¬¸ì„œ: {cluster.get_document_count()}ê°œ")
                    print(f"   ğŸ“Š ì¼ê´€ì„±: {cluster.coherence_score:.3f}")
                    print(f"   ğŸ”— í‰ê·  ìœ ì‚¬ë„: {cluster.get_average_similarity():.3f}")
                    print(f"   ğŸ“ ì´ ë‹¨ì–´: {cluster.get_total_word_count():,}ê°œ")
                    
                    if cluster.keywords:
                        print(f"   ğŸ·ï¸ í‚¤ì›Œë“œ: {', '.join(cluster.keywords[:10])}")
                    
                    if cluster.representative_doc:
                        print(f"   ğŸ‘‘ ëŒ€í‘œ ë¬¸ì„œ: {cluster.representative_doc.title}")
                    
                    print("   ğŸ“‹ ë¬¸ì„œ ëª©ë¡:")
                    for doc in cluster.documents[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                        print(f"      - {doc.title} ({doc.word_count} ë‹¨ì–´)")
                    
                    if len(cluster.documents) > 5:
                        print(f"      ... ì™¸ {len(cluster.documents) - 5}ê°œ")
            
            # íŒŒì¼ ì €ì¥
            if output_file:
                if self.topic_analyzer.export_analysis(analysis, output_file):
                    print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")
            
            # ì‹œê°í™”
            if visualize:
                viz_file = output_file.replace('.json', '.png') if output_file else "cluster_visualization.png"
                if self.topic_analyzer.visualize_clusters(analysis, viz_file):
                    print(f"ğŸ“ˆ ì‹œê°í™” ì €ì¥: {viz_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            print(f"âŒ ì£¼ì œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def collect_topic(
        self,
        topic: str,
        top_k: Optional[int] = None,
        threshold: Optional[float] = None,
        output_file: Optional[str] = None,
        format_type: str = "markdown"
    ) -> bool:
        """ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ì§‘"""
        try:
            self._initialize_components()
            
            if not self.search_engine.indexed:
                print("âŒ ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € 'index' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            
            top_k = top_k or 100
            threshold = threshold or self.config['search']['similarity_threshold']
            
            print(f"ğŸ“š '{topic}' ì£¼ì œ ë¬¸ì„œ ìˆ˜ì§‘ ì¤‘...")
            
            collection = self.topic_collector.collect_topic(
                topic, top_k, threshold, output_file=output_file
            )
            
            print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
            print("=" * 50)
            print(f"ğŸ¯ ì£¼ì œ: {collection.metadata.topic}")
            print(f"ğŸ“„ ìˆ˜ì§‘ ë¬¸ì„œ: {collection.metadata.total_documents}ê°œ")
            print(f"ğŸ“ ì´ ë‹¨ì–´: {collection.metadata.total_word_count:,}ê°œ")
            print(f"ğŸ’¾ ì´ í¬ê¸°: {collection.metadata.total_size_bytes / (1024*1024):.2f}MB")
            print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{collection.metadata.search_query}'")
            
            if collection.metadata.tag_distribution:
                print(f"\nğŸ·ï¸ ì£¼ìš” íƒœê·¸:")
                for tag, count in list(collection.metadata.tag_distribution.items())[:10]:
                    print(f"   - {tag}: {count}ê°œ")
            
            if collection.grouped_documents:
                print(f"\nğŸ“ ê·¸ë£¹ë³„ ë¶„í¬:")
                for group, docs in collection.grouped_documents.items():
                    print(f"   - {group}: {len(docs)}ê°œ ë¬¸ì„œ")
            
            # ê¸°ë³¸ ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
            if not output_file:
                safe_topic = topic.lower().replace(' ', '_').replace('/', '_')
                output_file = f"{safe_topic}_collection.{format_type}"
            
            # íŒŒì¼ ì €ì¥
            if self.topic_collector.export_collection(collection, output_file, format_type):
                print(f"\nğŸ’¾ ì»¬ë ‰ì…˜ ì €ì¥: {output_file}")
            
            # ê´€ë ¨ ì£¼ì œ ì œì•ˆ
            suggestions = self.topic_collector.suggest_related_topics(topic, 5)
            if suggestions:
                print(f"\nğŸ”— ê´€ë ¨ ì£¼ì œ ì œì•ˆ:")
                for suggested_topic, count in suggestions:
                    print(f"   - {suggested_topic} ({count}íšŒ ì–¸ê¸‰)")
            
            return True
            
        except Exception as e:
            logger.error(f"ì£¼ì œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            print(f"âŒ ì£¼ì œ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def build_graph(
        self,
        similarity_threshold: Optional[float] = None,
        include_tags: bool = True,
        output_file: Optional[str] = None,
        visualize: bool = False
    ) -> bool:
        """ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•"""
        try:
            self._initialize_components()
            
            if not self.search_engine.indexed:
                print("âŒ ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € 'index' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            
            similarity_threshold = similarity_threshold or self.config['graph']['similarity_threshold']
            
            print(f"ğŸ•¸ï¸ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì¤‘ (ì„ê³„ê°’: {similarity_threshold})...")
            
            knowledge_graph = self.graph_builder.build_graph(
                similarity_threshold=similarity_threshold,
                include_tag_nodes=include_tags
            )
            
            print(f"\nğŸ“Š ì§€ì‹ ê·¸ë˜í”„ ê²°ê³¼:")
            print("=" * 60)
            print(f"ğŸ”µ ë…¸ë“œ: {knowledge_graph.get_node_count()}ê°œ")
            print(f"ğŸ”— ì—£ì§€: {knowledge_graph.get_edge_count()}ê°œ")
            print(f"ğŸ“Š ë°€ë„: {knowledge_graph.get_density():.3f}")
            
            # ë©”íŠ¸ë¦­ ì¶œë ¥
            if knowledge_graph.graph_metrics:
                metrics = knowledge_graph.graph_metrics
                print(f"ğŸ”„ ì—°ê²°ì„±: {'ì—°ê²°ë¨' if metrics.get('is_connected', False) else 'ë¶„ë¦¬ë¨'}")
                print(f"ğŸŒ ì»´í¬ë„ŒíŠ¸: {metrics.get('number_of_components', 0)}ê°œ")
                print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§: {metrics.get('average_clustering', 0):.3f}")
            
            # ì¤‘ì‹¬ì„± ì ìˆ˜ (ìƒìœ„ 10ê°œ)
            if knowledge_graph.centrality_scores:
                print(f"\nğŸ¯ ë†’ì€ ì¤‘ì‹¬ì„± ë…¸ë“œ:")
                sorted_centrality = sorted(
                    knowledge_graph.centrality_scores.items(),
                    key=lambda x: x[1], reverse=True
                )[:10]
                
                for node_id, score in sorted_centrality:
                    node = next((n for n in knowledge_graph.nodes if n.id == node_id), None)
                    if node:
                        node_type = "ğŸ“„" if node.node_type == "document" else "ğŸ·ï¸"
                        print(f"   {node_type} {node.title}: {score:.3f}")
            
            # ì»¤ë®¤ë‹ˆí‹° ì •ë³´
            if knowledge_graph.communities:
                print(f"\nğŸ˜ï¸ ì»¤ë®¤ë‹ˆí‹°: {len(knowledge_graph.communities)}ê°œ")
                for i, community in enumerate(knowledge_graph.communities[:5]):  # ìƒìœ„ 5ê°œë§Œ
                    community_nodes = [
                        n.title for n in knowledge_graph.nodes if n.id in community
                    ][:3]
                    print(f"   ì»¤ë®¤ë‹ˆí‹° {i+1}: {len(community)}ê°œ ë…¸ë“œ")
                    print(f"      - {', '.join(community_nodes)}...")
            
            # íŒŒì¼ ì €ì¥
            if not output_file:
                output_file = "knowledge_graph.json"
            
            if self.graph_builder.export_graph(knowledge_graph, output_file):
                print(f"\nğŸ’¾ ê·¸ë˜í”„ ì €ì¥: {output_file}")
            
            # ì‹œê°í™”
            if visualize:
                viz_file = output_file.replace('.json', '.png')
                if self.graph_builder.visualize_graph(knowledge_graph, viz_file):
                    print(f"ğŸ“ˆ ì‹œê°í™” ì €ì¥: {viz_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            print(f"âŒ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def get_stats(self) -> bool:
        """ì‹œìŠ¤í…œ í†µê³„"""
        try:
            print("ğŸ“Š Vault Intelligence System V2 í†µê³„")
            print("=" * 60)
            
            # Vault ì •ë³´
            processor = VaultProcessor(str(self.vault_path))
            vault_stats = processor.get_vault_statistics()
            
            print(f"ğŸ“ Vault ì •ë³´:")
            print(f"   ê²½ë¡œ: {vault_stats['vault_path']}")
            print(f"   íŒŒì¼: {vault_stats['total_files']}ê°œ")
            print(f"   í¬ê¸°: {vault_stats['total_size_mb']}MB")
            
            # ìºì‹œ ì •ë³´
            cache = EmbeddingCache(str(self.cache_dir))
            cache_stats = cache.get_statistics()
            
            print(f"\nğŸ’¾ ìºì‹œ ì •ë³´:")
            print(f"   ìºì‹œëœ ì„ë² ë”©: {cache_stats['total_embeddings']}ê°œ")
            print(f"   DB í¬ê¸°: {cache_stats['db_size'] / (1024*1024):.2f}MB")
            
            # ëª¨ë¸ ì •ë³´
            if self.search_engine and hasattr(self.search_engine, 'engine'):
                model_info = self.search_engine.engine.get_model_info()
                print(f"\nğŸ¤– ëª¨ë¸ ì •ë³´:")
                print(f"   ëª¨ë¸: {model_info['model_name']}")
                print(f"   ì°¨ì›: {model_info['embedding_dimension']}")
                print(f"   ì¥ì¹˜: {model_info['device']}")
            
            # ê²€ìƒ‰ ì—”ì§„ ì •ë³´
            if self.search_engine and self.search_engine.indexed:
                print(f"\nğŸ” ê²€ìƒ‰ ì—”ì§„:")
                print(f"   ì¸ë±ì‹±ëœ ë¬¸ì„œ: {len(self.search_engine.documents)}ê°œ")
                print(f"   ì„ë² ë”© í˜•íƒœ: {self.search_engine.embeddings.shape}")
                print(f"   ì¸ë±ìŠ¤ ìƒíƒœ: âœ… êµ¬ì¶•ë¨")
            else:
                print(f"\nğŸ” ê²€ìƒ‰ ì—”ì§„:")
                print(f"   ì¸ë±ìŠ¤ ìƒíƒœ: âŒ ë¯¸êµ¬ì¶•")
            
            # ì„¤ì • ì •ë³´
            print(f"\nâš™ï¸ í˜„ì¬ ì„¤ì •:")
            print(f"   ê²€ìƒ‰ ì„ê³„ê°’: {self.config['search']['similarity_threshold']}")
            print(f"   ì¤‘ë³µ ì„ê³„ê°’: {self.config['duplicates']['similarity_threshold']}")
            print(f"   ê¸°ë³¸ í´ëŸ¬ìŠ¤í„°: {self.config['clustering']['default_n_clusters']}ê°œ")
            
            return True
            
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            print(f"âŒ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _save_search_results(self, results, query: str, output_file: str):
        """ê²€ìƒ‰ ê²°ê³¼ ì €ì¥"""
        try:
            content = f"""---
tags:
  - vault-intelligence/search-results
  - search/{query.lower().replace(' ', '-')}
generated: {datetime.now().isoformat()}
query: "{query}"
---

# "{query}" ê²€ìƒ‰ ê²°ê³¼

ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(results)}ê°œ

## ë¬¸ì„œ ëª©ë¡

"""
            
            for result in results:
                content += f"### {result.rank}. [[{result.document.path}]]\n\n"
                content += f"- **ì œëª©**: {result.document.title}\n"
                content += f"- **ìœ ì‚¬ë„**: {result.similarity_score:.4f} ({result.match_type})\n"
                content += f"- **ë‹¨ì–´ ìˆ˜**: {result.document.word_count}\n"
                
                if result.document.tags:
                    content += f"- **íƒœê·¸**: {', '.join(result.document.tags)}\n"
                
                if result.matched_keywords:
                    content += f"- **ë§¤ì¹­ í‚¤ì›Œë“œ**: {', '.join(result.matched_keywords)}\n"
                
                if result.snippet:
                    content += f"- **ë¯¸ë¦¬ë³´ê¸°**: {result.snippet}\n"
                
                content += "\n"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _print_index_stats(self):
        """ì¸ë±ìŠ¤ í†µê³„ ì¶œë ¥"""
        try:
            if self.search_engine and self.search_engine.indexed:
                stats = self.search_engine.get_search_statistics()
                print(f"ğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
                print(f"   ë¬¸ì„œ: {stats['indexed_documents']}ê°œ")
                print(f"   ì„ë² ë”© ì°¨ì›: {stats['embedding_dimension']}")
                print(f"   ëª¨ë¸: {stats['model_name']}")
        except:
            pass


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Vault Intelligence System V2 - Sentence Transformers ê¸°ë°˜ ì§€ëŠ¥í˜• Vault ê´€ë¦¬"
    )
    
    # ê³µí†µ ì¸ì
    parser.add_argument(
        "--vault-path",
        default="/Users/msbaek/DocumentsLocal/msbaek_vault",
        help="Vault ê²½ë¡œ"
    )
    parser.add_argument(
        "--config",
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥"
    )
    
    # ì„œë¸Œì»¤ë§¨ë“œ
    subparsers = parser.add_subparsers(dest="command", help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´")
    
    # index ëª…ë ¹
    index_parser = subparsers.add_parser("index", help="ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•")
    index_parser.add_argument("--force", action="store_true", help="ê°•ì œ ì¬êµ¬ì¶•")
    
    # search ëª…ë ¹
    search_parser = subparsers.add_parser("search", help="ë¬¸ì„œ ê²€ìƒ‰")
    search_parser.add_argument("query", help="ê²€ìƒ‰ ì¿¼ë¦¬")
    search_parser.add_argument("--type", choices=["semantic", "keyword", "hybrid"], 
                              default="hybrid", help="ê²€ìƒ‰ íƒ€ì…")
    search_parser.add_argument("--top-k", type=int, help="ê²°ê³¼ ê°œìˆ˜")
    search_parser.add_argument("--threshold", type=float, help="ìœ ì‚¬ë„ ì„ê³„ê°’")
    search_parser.add_argument("--output", help="ê²°ê³¼ ì €ì¥ íŒŒì¼")
    
    # duplicates ëª…ë ¹
    dup_parser = subparsers.add_parser("duplicates", help="ì¤‘ë³µ ë¬¸ì„œ ê°ì§€")
    dup_parser.add_argument("--threshold", type=float, help="ìœ ì‚¬ë„ ì„ê³„ê°’")
    dup_parser.add_argument("--min-words", type=int, help="ìµœì†Œ ë‹¨ì–´ ìˆ˜")
    dup_parser.add_argument("--output", help="ê²°ê³¼ ì €ì¥ íŒŒì¼")
    
    # analyze ëª…ë ¹
    analyze_parser = subparsers.add_parser("analyze", help="ì£¼ì œ ë¶„ì„")
    analyze_parser.add_argument("topic", nargs="?", help="ë¶„ì„í•  ì£¼ì œ (ì„ íƒì‚¬í•­)")
    analyze_parser.add_argument("--clusters", type=int, help="í´ëŸ¬ìŠ¤í„° ìˆ˜")
    analyze_parser.add_argument("--algorithm", choices=["kmeans", "dbscan", "hierarchical"], help="í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜")
    analyze_parser.add_argument("--output", help="ê²°ê³¼ ì €ì¥ íŒŒì¼")
    analyze_parser.add_argument("--visualize", action="store_true", help="ì‹œê°í™” ìƒì„±")
    
    # collect ëª…ë ¹
    collect_parser = subparsers.add_parser("collect", help="ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ì§‘")
    collect_parser.add_argument("topic", help="ìˆ˜ì§‘í•  ì£¼ì œ")
    collect_parser.add_argument("--top-k", type=int, help="ìµœëŒ€ ë¬¸ì„œ ìˆ˜")
    collect_parser.add_argument("--threshold", type=float, help="ìœ ì‚¬ë„ ì„ê³„ê°’")
    collect_parser.add_argument("--output", help="ê²°ê³¼ ì €ì¥ íŒŒì¼")
    collect_parser.add_argument("--format", choices=["markdown", "json"], default="markdown", help="ì¶œë ¥ í˜•ì‹")
    
    # graph ëª…ë ¹
    graph_parser = subparsers.add_parser("graph", help="ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•")
    graph_parser.add_argument("--threshold", type=float, help="ìœ ì‚¬ë„ ì„ê³„ê°’")
    graph_parser.add_argument("--no-tags", action="store_true", help="íƒœê·¸ ë…¸ë“œ ì œì™¸")
    graph_parser.add_argument("--output", help="ê²°ê³¼ ì €ì¥ íŒŒì¼")
    graph_parser.add_argument("--visualize", action="store_true", help="ì‹œê°í™” ìƒì„±")
    
    # stats ëª…ë ¹
    subparsers.add_parser("stats", help="ì‹œìŠ¤í…œ í†µê³„")
    
    # ì¸ì íŒŒì‹±
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # VaultAssistant ì´ˆê¸°í™”
    try:
        assistant = VaultAssistant(args.vault_path, args.config)
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    success = True
    
    if args.command == "index":
        success = assistant.build_index(args.force)
    
    elif args.command == "search":
        success = assistant.search(
            args.query, args.top_k, args.type, args.threshold, args.output
        )
    
    elif args.command == "duplicates":
        success = assistant.find_duplicates(
            args.threshold, args.min_words, args.output
        )
    
    elif args.command == "analyze":
        success = assistant.analyze_topics(
            args.topic, args.clusters, args.algorithm, args.output, args.visualize
        )
    
    elif args.command == "collect":
        success = assistant.collect_topic(
            args.topic, args.top_k, args.threshold, args.output, args.format
        )
    
    elif args.command == "graph":
        success = assistant.build_graph(
            args.threshold, not args.no_tags, args.output, args.visualize
        )
    
    elif args.command == "stats":
        success = assistant.get_stats()
    
    else:
        parser.print_help()
        success = False
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()