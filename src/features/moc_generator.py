#!/usr/bin/env python3
"""
MOC(Map of Content) Generator for Vault Intelligence System V2

ì£¼ì œë³„ ìë™ ëª©ì°¨ ë¬¸ì„œ ìƒì„± ì‹œìŠ¤í…œ
"""

import os
import re
import logging
from typing import List, Dict, Optional, Tuple, Set
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import numpy as np

from ..core.vault_processor import Document
from ..features.advanced_search import SearchResult
from .topic_collector import TopicCollector, DocumentCollection

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class DocumentCategory:
    """ë¬¸ì„œ ì¹´í…Œê³ ë¦¬"""
    name: str
    description: str
    keywords: List[str]
    documents: List[Document]
    priority: int = 0


@dataclass
class LearningPathStep:
    """í•™ìŠµ ê²½ë¡œ ë‹¨ê³„"""
    step: int
    title: str
    documents: List[Document]
    description: str
    difficulty_level: str  # "ì…ë¬¸", "ê¸°ì´ˆ", "ì¤‘ê¸‰", "ê³ ê¸‰"


@dataclass
class DocumentRelationship:
    """ë¬¸ì„œ ê°„ ê´€ê³„"""
    source_doc: Document
    target_doc: Document
    relationship_type: str  # "references", "similar", "prerequisite"
    strength: float  # ê´€ê³„ ê°•ë„ (0-1)


@dataclass
class MOCData:
    """MOC ë°ì´í„° êµ¬ì¡°"""
    topic: str
    overview: str
    total_documents: int
    core_documents: List[Document]
    categories: List[DocumentCategory]
    learning_path: List[LearningPathStep]
    related_topics: List[Tuple[str, int]]
    recent_updates: List[Document]
    relationships: List[DocumentRelationship]
    generation_date: datetime
    statistics: Dict


class MOCGenerator:
    """MOC(Map of Content) ìƒì„±ê¸°"""
    
    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì •ì˜
    CATEGORY_KEYWORDS = {
        "ì…ë¬¸/ê¸°ì´ˆ": {
            "keywords": ["ì…ë¬¸", "ì‹œì‘", "ê¸°ì´ˆ", "ê¸°ë³¸", "ì†Œê°œ", "ê°œìš”", "introduction", "basic", "fundamental", "getting started", "overview", "primer"],
            "description": "ì£¼ì œì— ëŒ€í•œ ì²« ê±¸ìŒê³¼ ê¸°ë³¸ ê°œë…",
            "priority": 1
        },
        "ê°œë…/ì´ë¡ ": {
            "keywords": ["ê°œë…", "ì´ë¡ ", "ì›ë¦¬", "ì •ì˜", "concept", "theory", "principle", "definition", "model", "framework"],
            "description": "í•µì‹¬ ê°œë…ê³¼ ì´ë¡ ì  ë°°ê²½",
            "priority": 2
        },
        "ì‹¤ìŠµ/ì˜ˆì œ": {
            "keywords": ["ì‹¤ìŠµ", "ì˜ˆì œ", "ì—°ìŠµ", "ì‹¤ì „", "êµ¬í˜„", "practice", "example", "exercise", "implementation", "hands-on", "tutorial", "workshop"],
            "description": "ì‹¤ì œ ì ìš© ì‚¬ë¡€ì™€ ì—°ìŠµ ë¬¸ì œ",
            "priority": 3
        },
        "ì‹¬í™”/ê³ ê¸‰": {
            "keywords": ["ì‹¬í™”", "ê³ ê¸‰", "ì „ë¬¸", "ìƒì„¸", "advanced", "deep", "expert", "in-depth", "professional", "detailed"],
            "description": "ì „ë¬¸ì ì´ê³  ê¹Šì´ ìˆëŠ” ë‚´ìš©",
            "priority": 4
        },
        "ë„êµ¬/ê¸°ìˆ ": {
            "keywords": ["ë„êµ¬", "íˆ´", "ê¸°ìˆ ", "ë°©ë²•", "tool", "technique", "method", "approach", "framework", "library"],
            "description": "ê´€ë ¨ ë„êµ¬ì™€ ê¸°ìˆ  ìŠ¤íƒ",
            "priority": 3
        },
        "ì°¸ê³ ìë£Œ": {
            "keywords": ["ì°¸ê³ ", "ìë£Œ", "ë¦¬ì†ŒìŠ¤", "ë¬¸ì„œ", "reference", "resource", "documentation", "guide", "manual", "cheat sheet"],
            "description": "ì¶”ê°€ í•™ìŠµì„ ìœ„í•œ ì°¸ê³  ìë£Œ",
            "priority": 5
        }
    }
    
    def __init__(
        self,
        search_engine,
        config: Dict = None
    ):
        """
        Args:
            search_engine: AdvancedSearchEngine ì¸ìŠ¤í„´ìŠ¤
            config: MOC ìƒì„± ì„¤ì •
        """
        self.search_engine = search_engine
        self.config = config or {}
        
        # TopicCollector ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.topic_collector = TopicCollector(search_engine, config)
        
        # MOC ìƒì„± ì„¤ì •
        self.max_core_documents = self.config.get('moc', {}).get('max_core_documents', 5)
        self.max_category_documents = self.config.get('moc', {}).get('max_category_documents', 10)
        self.recent_days = self.config.get('moc', {}).get('recent_days', 30)
        self.min_similarity_threshold = self.config.get('moc', {}).get('min_similarity_threshold', 0.3)
        self.relationship_threshold = self.config.get('moc', {}).get('relationship_threshold', 0.6)
        
        logger.info("MOC ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def generate_moc(
        self,
        topic: str,
        top_k: int = 100,
        threshold: float = 0.3,
        include_orphans: bool = False,
        use_expansion: bool = True,
        output_file: Optional[str] = None
    ) -> MOCData:
        """ì£¼ì œë³„ MOC ìƒì„±"""
        try:
            logger.info(f"MOC ìƒì„± ì‹œì‘: '{topic}'")
            
            # 1. ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘
            logger.info("1ï¸âƒ£ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘ ì¤‘...")
            collection = self.topic_collector.collect_topic(
                topic=topic,
                top_k=top_k,
                threshold=threshold,
                use_expansion=use_expansion,
                include_synonyms=True,
                include_hyde=True
            )
            
            if not collection.documents:
                logger.warning(f"ì£¼ì œ '{topic}'ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return self._create_empty_moc(topic)
            
            logger.info(f"ìˆ˜ì§‘ëœ ë¬¸ì„œ: {len(collection.documents)}ê°œ")
            
            # 2. ê°œìš” ìƒì„±
            logger.info("2ï¸âƒ£ ì£¼ì œ ê°œìš” ìƒì„± ì¤‘...")
            overview = self._create_overview(topic, collection)
            
            # 3. í•µì‹¬ ë¬¸ì„œ ì„ ì •
            logger.info("3ï¸âƒ£ í•µì‹¬ ë¬¸ì„œ ì„ ì • ì¤‘...")
            core_documents = self._select_core_documents(collection.documents)
            
            # 4. ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
            logger.info("4ï¸âƒ£ ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ ì¤‘...")
            categories = self._classify_documents(collection.documents)
            
            # 5. í•™ìŠµ ê²½ë¡œ ìƒì„±
            logger.info("5ï¸âƒ£ í•™ìŠµ ê²½ë¡œ ìƒì„± ì¤‘...")
            learning_path = self._create_learning_path(collection.documents, categories)
            
            # 6. ê´€ë ¨ ì£¼ì œ ì¶”ì¶œ
            logger.info("6ï¸âƒ£ ê´€ë ¨ ì£¼ì œ ì¶”ì¶œ ì¤‘...")
            related_topics = self._extract_related_topics(collection)
            
            # 7. ìµœê·¼ ì—…ë°ì´íŠ¸ ë¬¸ì„œ
            logger.info("7ï¸âƒ£ ìµœê·¼ ì—…ë°ì´íŠ¸ ë¬¸ì„œ ë¶„ì„ ì¤‘...")
            recent_updates = self._get_recent_updates(collection.documents)
            
            # 8. ë¬¸ì„œ ê°„ ê´€ê³„ ë¶„ì„
            logger.info("8ï¸âƒ£ ë¬¸ì„œ ê°„ ê´€ê³„ ë¶„ì„ ì¤‘...")
            relationships = self._analyze_document_relationships(collection.documents)
            
            # 9. í†µê³„ ì •ë³´ ìƒì„±
            statistics = self._generate_moc_statistics(collection.documents, categories, relationships)
            
            # MOC ë°ì´í„° ìƒì„±
            moc_data = MOCData(
                topic=topic,
                overview=overview,
                total_documents=len(collection.documents),
                core_documents=core_documents,
                categories=categories,
                learning_path=learning_path,
                related_topics=related_topics,
                recent_updates=recent_updates,
                relationships=relationships,
                generation_date=datetime.now(),
                statistics=statistics
            )
            
            # ì¶œë ¥ íŒŒì¼ ìƒì„±
            if output_file:
                self._export_moc(moc_data, output_file)
                logger.info(f"MOC íŒŒì¼ ì €ì¥: {output_file}")
            
            logger.info(f"MOC ìƒì„± ì™„ë£Œ: {len(collection.documents)}ê°œ ë¬¸ì„œ, {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬")
            return moc_data
            
        except Exception as e:
            logger.error(f"MOC ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_empty_moc(topic)
    
    def _create_overview(self, topic: str, collection: DocumentCollection) -> str:
        """ì£¼ì œ ê°œìš” ìƒì„±"""
        try:
            # ê¸°ë³¸ í†µê³„
            total_docs = len(collection.documents)
            total_words = sum(doc.word_count for doc in collection.documents)
            avg_words = total_words // total_docs if total_docs > 0 else 0
            
            # ì£¼ìš” íƒœê·¸ ì¶”ì¶œ
            top_tags = list(collection.metadata.tag_distribution.items())[:5]
            tags_str = ", ".join([f"#{tag}" for tag, _ in top_tags])
            
            # ê¸°ê°„ ë¶„ì„
            if collection.documents:
                dates = [doc.modified_at for doc in collection.documents]
                oldest = min(dates)
                newest = max(dates)
                span_days = (newest - oldest).days
            else:
                span_days = 0
            
            overview = f"""ì´ Map of ContentëŠ” '{topic}' ì£¼ì œì— ëŒ€í•œ ì¢…í•©ì ì¸ íƒìƒ‰ ê°€ì´ë“œì…ë‹ˆë‹¤.

**ğŸ“Š ì»¬ë ‰ì…˜ í†µê³„:**
- ì´ ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ
- ì´ ë‹¨ì–´ ìˆ˜: {total_words:,}ê°œ
- í‰ê·  ë¬¸ì„œ ê¸¸ì´: {avg_words:,}ê°œ ë‹¨ì–´
- ì—…ë°ì´íŠ¸ ê¸°ê°„: {span_days}ì¼ê°„"""

            if tags_str:
                overview += f"\n- ì£¼ìš” íƒœê·¸: {tags_str}"
            
            overview += f"""

ì´ MOCë¥¼ í†µí•´ {topic} ê´€ë ¨ ì§€ì‹ì„ ì²´ê³„ì ìœ¼ë¡œ íƒìƒ‰í•˜ê³  í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
            
            return overview
            
        except Exception as e:
            logger.error(f"ê°œìš” ìƒì„± ì‹¤íŒ¨: {e}")
            return f"'{topic}' ì£¼ì œì— ëŒ€í•œ ì¢…í•©ì ì¸ ì§€ì‹ ëª¨ìŒì…ë‹ˆë‹¤."
    
    def _select_core_documents(self, documents: List[Document]) -> List[Document]:
        """í•µì‹¬ ë¬¸ì„œ ì„ ì •"""
        try:
            # ì ìˆ˜ ê¸°ë°˜ í•µì‹¬ ë¬¸ì„œ ì„ ì •
            scored_docs = []
            
            for doc in documents:
                score = 0.0
                
                # ë‹¨ì–´ ìˆ˜ ì ìˆ˜ (ì ë‹¹í•œ ê¸¸ì´ ì„ í˜¸)
                word_score = min(doc.word_count / 1000, 1.0) if doc.word_count else 0
                score += word_score * 0.3
                
                # íƒœê·¸ ìˆ˜ ì ìˆ˜ (íƒœê·¸ê°€ ë§ì„ìˆ˜ë¡ ì²´ê³„ì )
                tag_score = min(len(doc.tags) / 5, 1.0) if doc.tags else 0
                score += tag_score * 0.2
                
                # ìµœê·¼ì„± ì ìˆ˜ (ìµœê·¼ ìˆ˜ì •ëœ ë¬¸ì„œ ì„ í˜¸)
                if doc.modified_at:
                    days_ago = (datetime.now() - doc.modified_at).days
                    recency_score = max(0, 1 - days_ago / 365)  # 1ë…„ ê¸°ì¤€
                    score += recency_score * 0.2
                
                # ì œëª© í’ˆì§ˆ ì ìˆ˜ (êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì œëª©)
                title_score = min(len(doc.title.split()) / 10, 1.0)
                score += title_score * 0.3
                
                scored_docs.append((doc, score))
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ë¬¸ì„œ ì„ íƒ
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            core_docs = [doc for doc, score in scored_docs[:self.max_core_documents]]
            
            logger.info(f"í•µì‹¬ ë¬¸ì„œ {len(core_docs)}ê°œ ì„ ì •")
            return core_docs
            
        except Exception as e:
            logger.error(f"í•µì‹¬ ë¬¸ì„œ ì„ ì • ì‹¤íŒ¨: {e}")
            return documents[:self.max_core_documents]
    
    def _classify_documents(self, documents: List[Document]) -> List[DocumentCategory]:
        """ë¬¸ì„œë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        try:
            categories = []
            
            for category_name, config in self.CATEGORY_KEYWORDS.items():
                category_docs = []
                keywords = config["keywords"]
                
                for doc in documents:
                    # ì œëª©ê³¼ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
                    text_to_check = f"{doc.title} {doc.content[:500]}".lower()
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                    matches = sum(1 for keyword in keywords if keyword.lower() in text_to_check)
                    
                    # ë§¤ì¹­ ì ìˆ˜ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ì¶”ê°€
                    if matches > 0:
                        category_docs.append((doc, matches))
                
                # ë§¤ì¹­ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ë¬¸ì„œë“¤ ì„ íƒ
                category_docs.sort(key=lambda x: x[1], reverse=True)
                selected_docs = [doc for doc, _ in category_docs[:self.max_category_documents]]
                
                if selected_docs:
                    category = DocumentCategory(
                        name=category_name,
                        description=config["description"],
                        keywords=keywords,
                        documents=selected_docs,
                        priority=config["priority"]
                    )
                    categories.append(category)
            
            # ë¶„ë¥˜ë˜ì§€ ì•Šì€ ë¬¸ì„œë“¤ì„ "ê¸°íƒ€" ì¹´í…Œê³ ë¦¬ë¡œ ì²˜ë¦¬
            all_categorized = set()
            for category in categories:
                all_categorized.update(doc.path for doc in category.documents)
            
            uncategorized = [doc for doc in documents if doc.path not in all_categorized]
            if uncategorized:
                other_category = DocumentCategory(
                    name="ê¸°íƒ€",
                    description="ê¸°íƒ€ ê´€ë ¨ ë¬¸ì„œë“¤",
                    keywords=[],
                    documents=uncategorized[:self.max_category_documents],
                    priority=6
                )
                categories.append(other_category)
            
            # ìš°ì„ ìˆœìœ„ì™€ ë¬¸ì„œ ìˆ˜ë¡œ ì •ë ¬
            categories.sort(key=lambda c: (c.priority, -len(c.documents)))
            
            logger.info(f"ë¬¸ì„œ ë¶„ë¥˜ ì™„ë£Œ: {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬")
            return categories
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return []
    
    def _create_learning_path(self, documents: List[Document], categories: List[DocumentCategory]) -> List[LearningPathStep]:
        """í•™ìŠµ ê²½ë¡œ ìƒì„±"""
        try:
            learning_path = []
            step_counter = 1
            
            # ì¹´í…Œê³ ë¦¬ ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ í•™ìŠµ ë‹¨ê³„ ìƒì„±
            for category in categories:
                if not category.documents:
                    continue
                
                # ë‚œì´ë„ ë ˆë²¨ ë§¤í•‘
                difficulty_map = {
                    "ì…ë¬¸/ê¸°ì´ˆ": "ì…ë¬¸",
                    "ê°œë…/ì´ë¡ ": "ê¸°ì´ˆ",
                    "ì‹¤ìŠµ/ì˜ˆì œ": "ì¤‘ê¸‰",
                    "ë„êµ¬/ê¸°ìˆ ": "ì¤‘ê¸‰",
                    "ì‹¬í™”/ê³ ê¸‰": "ê³ ê¸‰",
                    "ì°¸ê³ ìë£Œ": "ì°¸ê³ "
                }
                
                difficulty = difficulty_map.get(category.name, "ì¤‘ê¸‰")
                
                # í•™ìŠµ ë‹¨ê³„ ì„¤ëª… ìƒì„±
                if category.name == "ì…ë¬¸/ê¸°ì´ˆ":
                    description = "ì£¼ì œì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ì™€ ê°œë… í•™ìŠµ"
                elif category.name == "ê°œë…/ì´ë¡ ":
                    description = "í•µì‹¬ ê°œë…ê³¼ ì´ë¡ ì  ë°°ê²½ ì´í•´"
                elif category.name == "ì‹¤ìŠµ/ì˜ˆì œ":
                    description = "ì‹¤ì œ ì‚¬ë¡€ë¥¼ í†µí•œ ì‹¤ìŠµê³¼ ì—°ìŠµ"
                elif category.name == "ë„êµ¬/ê¸°ìˆ ":
                    description = "ê´€ë ¨ ë„êµ¬ì™€ ê¸°ìˆ  ìŠ¤íƒ í•™ìŠµ"
                elif category.name == "ì‹¬í™”/ê³ ê¸‰":
                    description = "ì „ë¬¸ì ì´ê³  ê¹Šì´ ìˆëŠ” ì§€ì‹ ìŠµë“"
                else:
                    description = "ì¶”ê°€ í•™ìŠµê³¼ ì°¸ê³ ë¥¼ ìœ„í•œ ìë£Œ"
                
                # ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ ë¬¸ì„œë¥¼ ë‹¨ì–´ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì‰¬ìš´ ê²ƒë¶€í„°)
                sorted_docs = sorted(category.documents, key=lambda d: d.word_count)
                
                step = LearningPathStep(
                    step=step_counter,
                    title=f"{step_counter}ë‹¨ê³„: {category.name}",
                    documents=sorted_docs[:5],  # ìµœëŒ€ 5ê°œ ë¬¸ì„œ
                    description=description,
                    difficulty_level=difficulty
                )
                
                learning_path.append(step)
                step_counter += 1
            
            logger.info(f"í•™ìŠµ ê²½ë¡œ ìƒì„±: {len(learning_path)}ë‹¨ê³„")
            return learning_path
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ê²½ë¡œ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_related_topics(self, collection: DocumentCollection) -> List[Tuple[str, int]]:
        """ê´€ë ¨ ì£¼ì œ ì¶”ì¶œ"""
        try:
            # TopicCollectorì˜ ê¸°ëŠ¥ í™œìš©
            related_topics = self.topic_collector.suggest_related_topics(
                collection.metadata.topic, 
                top_k=10
            )
            
            logger.info(f"ê´€ë ¨ ì£¼ì œ {len(related_topics)}ê°œ ì¶”ì¶œ")
            return related_topics
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ ì£¼ì œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_recent_updates(self, documents: List[Document]) -> List[Document]:
        """ìµœê·¼ ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œ ëª©ë¡"""
        try:
            # ìµœê·¼ Nì¼ ì´ë‚´ ìˆ˜ì •ëœ ë¬¸ì„œë“¤
            cutoff_date = datetime.now() - timedelta(days=self.recent_days)
            recent_docs = [
                doc for doc in documents 
                if doc.modified_at and doc.modified_at > cutoff_date
            ]
            
            # ìˆ˜ì •ì¼ ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
            recent_docs.sort(key=lambda d: d.modified_at, reverse=True)
            
            logger.info(f"ìµœê·¼ {self.recent_days}ì¼ ì´ë‚´ ì—…ë°ì´íŠ¸: {len(recent_docs)}ê°œ")
            return recent_docs[:10]  # ìµœëŒ€ 10ê°œ
            
        except Exception as e:
            logger.error(f"ìµœê·¼ ì—…ë°ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _analyze_document_relationships(self, documents: List[Document]) -> List[DocumentRelationship]:
        """ë¬¸ì„œ ê°„ ê´€ê³„ ë¶„ì„"""
        try:
            relationships = []
            
            # ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê¸°ë°˜ ê´€ê³„ ë¶„ì„
            for i, doc1 in enumerate(documents):
                for doc2 in documents[i+1:]:
                    # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (ì œëª©ê³¼ íƒœê·¸ ê¸°ë°˜)
                    similarity = self._calculate_document_similarity(doc1, doc2)
                    
                    if similarity > self.relationship_threshold:
                        relationship = DocumentRelationship(
                            source_doc=doc1,
                            target_doc=doc2,
                            relationship_type="similar",
                            strength=similarity
                        )
                        relationships.append(relationship)
            
            # ê°•í•œ ê´€ê³„ ìˆœìœ¼ë¡œ ì •ë ¬
            relationships.sort(key=lambda r: r.strength, reverse=True)
            
            logger.info(f"ë¬¸ì„œ ê´€ê³„ {len(relationships)}ê°œ ë°œê²¬")
            return relationships[:20]  # ìµœëŒ€ 20ê°œ ê´€ê³„
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_document_similarity(self, doc1: Document, doc2: Document) -> float:
        """ë‘ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            similarity = 0.0
            
            # íƒœê·¸ ìœ ì‚¬ë„
            if doc1.tags and doc2.tags:
                tags1 = set(tag.lower() for tag in doc1.tags)
                tags2 = set(tag.lower() for tag in doc2.tags)
                
                if tags1 and tags2:
                    tag_similarity = len(tags1.intersection(tags2)) / len(tags1.union(tags2))
                    similarity += tag_similarity * 0.7
            
            # ì œëª© ìœ ì‚¬ë„ (ê°„ë‹¨í•œ ë‹¨ì–´ ê²¹ì¹¨ ê¸°ë°˜)
            title1_words = set(doc1.title.lower().split())
            title2_words = set(doc2.title.lower().split())
            
            if title1_words and title2_words:
                title_similarity = len(title1_words.intersection(title2_words)) / len(title1_words.union(title2_words))
                similarity += title_similarity * 0.3
            
            return similarity
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _generate_moc_statistics(self, documents: List[Document], categories: List[DocumentCategory], relationships: List[DocumentRelationship]) -> Dict:
        """MOC í†µê³„ ì •ë³´ ìƒì„±"""
        try:
            statistics = {
                "total_documents": len(documents),
                "total_categories": len(categories),
                "total_relationships": len(relationships),
                "category_distribution": {
                    cat.name: len(cat.documents) for cat in categories
                },
                "word_count_distribution": {
                    "total": sum(doc.word_count for doc in documents),
                    "average": sum(doc.word_count for doc in documents) // len(documents) if documents else 0,
                    "min": min(doc.word_count for doc in documents) if documents else 0,
                    "max": max(doc.word_count for doc in documents) if documents else 0
                },
                "tag_coverage": len(set(tag for doc in documents for tag in doc.tags)) if documents else 0
            }
            
            return statistics
            
        except Exception as e:
            logger.error(f"í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _create_empty_moc(self, topic: str) -> MOCData:
        """ë¹ˆ MOC ë°ì´í„° ìƒì„±"""
        return MOCData(
            topic=topic,
            overview=f"'{topic}' ì£¼ì œì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            total_documents=0,
            core_documents=[],
            categories=[],
            learning_path=[],
            related_topics=[],
            recent_updates=[],
            relationships=[],
            generation_date=datetime.now(),
            statistics={}
        )
    
    def _export_moc(self, moc_data: MOCData, output_file: str) -> bool:
        """MOCë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            content = self._format_as_markdown(moc_data)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info(f"MOC íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"MOC íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _format_as_markdown(self, moc_data: MOCData) -> str:
        """MOC ë°ì´í„°ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            lines = []
            
            # í—¤ë”ì™€ ë©”íƒ€ë°ì´í„°
            lines.append("---")
            lines.append("tags:")
            lines.append("  - vault-intelligence/moc")
            lines.append(f"  - topic/{moc_data.topic.lower().replace(' ', '-')}")
            lines.append(f"generated: {moc_data.generation_date.isoformat()}")
            lines.append(f"topic: {moc_data.topic}")
            lines.append(f"total_documents: {moc_data.total_documents}")
            lines.append("---")
            lines.append("")
            
            # ì œëª©
            lines.append(f"# ğŸ“š {moc_data.topic} Map of Content")
            lines.append("")
            
            # ìƒì„± ì •ë³´
            lines.append(f"*ìƒì„±ì¼: {moc_data.generation_date.strftime('%Y-%m-%d %H:%M:%S')}*")
            lines.append("")
            
            # ê°œìš”
            lines.append("## ğŸ¯ ê°œìš”")
            lines.append("")
            lines.append(moc_data.overview)
            lines.append("")
            
            # í•µì‹¬ ë¬¸ì„œ
            if moc_data.core_documents:
                lines.append("## ğŸŒŸ í•µì‹¬ ë¬¸ì„œ")
                lines.append("")
                lines.append("ì´ ì£¼ì œì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê³  í•µì‹¬ì ì¸ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤:")
                lines.append("")
                for i, doc in enumerate(moc_data.core_documents, 1):
                    lines.append(f"{i}. **[[{doc.path}]]**")
                    if doc.word_count:
                        lines.append(f"   - {doc.word_count:,} ë‹¨ì–´")
                    if doc.tags:
                        tags_str = ", ".join(f"#{tag}" for tag in doc.tags[:3])
                        lines.append(f"   - {tags_str}")
                    lines.append("")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
            if moc_data.categories:
                lines.append("## ğŸ“– ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜")
                lines.append("")
                
                for category in moc_data.categories:
                    lines.append(f"### {category.name}")
                    lines.append("")
                    lines.append(f"*{category.description}*")
                    lines.append("")
                    
                    for doc in category.documents:
                        lines.append(f"- **[[{doc.path}]]**")
                        info_parts = []
                        if doc.word_count:
                            info_parts.append(f"{doc.word_count:,} ë‹¨ì–´")
                        if doc.tags:
                            tags_str = ", ".join(f"#{tag}" for tag in doc.tags[:2])
                            info_parts.append(tags_str)
                        
                        if info_parts:
                            lines.append(f"  - {' | '.join(info_parts)}")
                    
                    lines.append("")
            
            # í•™ìŠµ ê²½ë¡œ
            if moc_data.learning_path:
                lines.append("## ğŸ›¤ï¸ ì¶”ì²œ í•™ìŠµ ê²½ë¡œ")
                lines.append("")
                lines.append("ì²´ê³„ì ì¸ í•™ìŠµì„ ìœ„í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œì…ë‹ˆë‹¤:")
                lines.append("")
                
                for step in moc_data.learning_path:
                    lines.append(f"### {step.title}")
                    lines.append("")
                    lines.append(f"**ë‚œì´ë„**: {step.difficulty_level}  ")
                    lines.append(f"**ì„¤ëª…**: {step.description}")
                    lines.append("")
                    lines.append("**ì¶”ì²œ ë¬¸ì„œ:**")
                    
                    for doc in step.documents:
                        lines.append(f"- [[{doc.path}]]")
                    
                    lines.append("")
            
            # ê´€ë ¨ ì£¼ì œ
            if moc_data.related_topics:
                lines.append("## ğŸ”— ê´€ë ¨ ì£¼ì œ")
                lines.append("")
                lines.append("ì´ ì£¼ì œì™€ ì—°ê´€ëœ ë‹¤ë¥¸ ì£¼ì œë“¤:")
                lines.append("")
                
                for topic, count in moc_data.related_topics:
                    lines.append(f"- **{topic}** ({count}ê°œ ë¬¸ì„œ)")
                
                lines.append("")
            
            # ìµœê·¼ ì—…ë°ì´íŠ¸
            if moc_data.recent_updates:
                lines.append("## ğŸ“… ìµœê·¼ ì—…ë°ì´íŠ¸")
                lines.append("")
                lines.append(f"ìµœê·¼ {self.recent_days}ì¼ ì´ë‚´ì— ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œë“¤:")
                lines.append("")
                
                for doc in moc_data.recent_updates:
                    update_date = doc.modified_at.strftime('%Y-%m-%d')
                    lines.append(f"- **[[{doc.path}]]** ({update_date})")
                
                lines.append("")
            
            # ë¬¸ì„œ ê´€ê³„ë„
            if moc_data.relationships:
                lines.append("## ğŸ“Š ë¬¸ì„œ ê´€ê³„ë„")
                lines.append("")
                lines.append("ì£¼ìš” ë¬¸ì„œë“¤ ê°„ì˜ ê´€ë ¨ì„±:")
                lines.append("")
                
                for rel in moc_data.relationships[:10]:  # ìƒìœ„ 10ê°œë§Œ
                    strength_emoji = "ğŸ”—" if rel.strength > 0.8 else "â†”ï¸"
                    lines.append(f"- {strength_emoji} [[{rel.source_doc.path}]] â†” [[{rel.target_doc.path}]] ({rel.strength:.2f})")
                
                lines.append("")
            
            # í†µê³„ ì •ë³´
            if moc_data.statistics:
                stats = moc_data.statistics
                lines.append("## ğŸ“ˆ í†µê³„")
                lines.append("")
                lines.append(f"- **ì´ ë¬¸ì„œ**: {stats.get('total_documents', 0)}ê°œ")
                lines.append(f"- **ì´ ì¹´í…Œê³ ë¦¬**: {stats.get('total_categories', 0)}ê°œ")
                lines.append(f"- **ë¬¸ì„œ ê´€ê³„**: {stats.get('total_relationships', 0)}ê°œ")
                
                word_stats = stats.get('word_count_distribution', {})
                if word_stats:
                    lines.append(f"- **ì´ ë‹¨ì–´ ìˆ˜**: {word_stats.get('total', 0):,}ê°œ")
                    lines.append(f"- **í‰ê·  ë‹¨ì–´ ìˆ˜**: {word_stats.get('average', 0):,}ê°œ")
                
                lines.append("")
            
            # í‘¸í„°
            lines.append("---")
            lines.append("")
            lines.append("*ì´ MOCëŠ” Vault Intelligence System V2ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*  ")
            lines.append(f"*ìƒì„± ì‹œê°„: {moc_data.generation_date.strftime('%Y-%m-%d %H:%M:%S')}*")
            
            return "\n".join(lines)
            
        except Exception as e:
            logger.error(f"ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return f"# {moc_data.topic} MOC\n\nMOC ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


def test_moc_generator():
    """MOC ìƒì„±ê¸° í…ŒìŠ¤íŠ¸"""
    import tempfile
    import shutil
    from ..features.advanced_search import AdvancedSearchEngine
    
    try:
        print("ğŸ§ª MOC ìƒì„±ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ì„ì‹œ vault ë° ìºì‹œ ìƒì„±
        temp_vault = tempfile.mkdtemp()
        temp_cache = tempfile.mkdtemp()
        
        # ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì˜ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤ ìƒì„±
        test_docs = [
            ("tdd-introduction.md", """---
tags:
  - development/tdd
  - learning/basic
---

# TDD ì…ë¬¸ ê°€ì´ë“œ

TDD(Test-Driven Development)ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ê³  êµ¬í˜„í•˜ëŠ” ê°œë°œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.
ì´ˆë³´ìë¥¼ ìœ„í•œ ê¸°ë³¸ì ì¸ ê°œë…ê³¼ ì‹œì‘ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
"""),
            
            ("tdd-concepts.md", """---
tags:
  - development/tdd
  - theory/concept
---

# TDD í•µì‹¬ ê°œë…

Red-Green-Refactor ì‚¬ì´í´ì˜ ì´ë¡ ì  ë°°ê²½ê³¼ 
TDDì˜ í•µì‹¬ ì›ë¦¬ë“¤ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.
"""),
            
            ("tdd-practice.md", """---
tags:
  - development/tdd
  - practice/example
---

# TDD ì‹¤ìŠµ ì˜ˆì œ

ì‹¤ì œ ì½”ë“œë¥¼ í†µí•œ TDD ì‹¤ìŠµ ê°€ì´ë“œì…ë‹ˆë‹¤.
ë‹¨ê³„ë³„ ì˜ˆì œì™€ ì—°ìŠµ ë¬¸ì œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""),
            
            ("tdd-advanced.md", """---
tags:
  - development/tdd
  - advanced/expert
---

# ê³ ê¸‰ TDD ê¸°ë²•

ë³µì¡í•œ ìƒí™©ì—ì„œì˜ TDD ì ìš© ë°©ë²•ê³¼
ì „ë¬¸ì ì¸ í…ŒìŠ¤íŒ… íŒ¨í„´ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤.
"""),
            
            ("junit-guide.md", """---
tags:
  - tools/junit
  - testing/framework
---

# JUnit ì‚¬ìš© ê°€ì´ë“œ

Java TDDë¥¼ ìœ„í•œ JUnit í”„ë ˆì„ì›Œí¬ ì‚¬ìš©ë²•ê³¼
ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ë„êµ¬ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.
"""),
            
            ("testing-resources.md", """---
tags:
  - reference/testing
  - resources/documentation
---

# í…ŒìŠ¤íŒ… ì°¸ê³  ìë£Œ

TDDì™€ í…ŒìŠ¤íŒ…ì— ê´€í•œ ì¶”ê°€ í•™ìŠµ ìë£Œì™€
ìœ ìš©í•œ ì°¸ê³  ë¬¸ì„œë“¤ì˜ ëª¨ìŒì…ë‹ˆë‹¤.
""")
        ]
        
        for filename, content in test_docs:
            with open(Path(temp_vault) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ë° ì¸ë±ì‹±
        config = {
            "model": {"name": "BAAI/bge-m3"},
            "vault": {"excluded_dirs": [], "file_extensions": [".md"]},
            "moc": {
                "max_core_documents": 3,
                "max_category_documents": 5,
                "recent_days": 30,
                "min_similarity_threshold": 0.3,
                "relationship_threshold": 0.5
            }
        }
        
        search_engine = AdvancedSearchEngine(temp_vault, temp_cache, config)
        print("ğŸ” ê²€ìƒ‰ ì—”ì§„ ì¸ë±ì‹± ì¤‘...")
        if not search_engine.build_index():
            print("âŒ ì¸ë±ì‹± ì‹¤íŒ¨")
            return False
        
        # MOC ìƒì„±ê¸° ì´ˆê¸°í™”
        moc_generator = MOCGenerator(search_engine, config)
        
        # TDD MOC ìƒì„± í…ŒìŠ¤íŠ¸
        print("\nğŸ“š 'TDD' MOC ìƒì„± ì¤‘...")
        moc_data = moc_generator.generate_moc(
            topic="TDD",
            top_k=20,
            threshold=0.2,
            use_expansion=True
        )
        
        print(f"\nğŸ“Š MOC ìƒì„± ê²°ê³¼:")
        print(f"- ì£¼ì œ: {moc_data.topic}")
        print(f"- ì´ ë¬¸ì„œ: {moc_data.total_documents}ê°œ")
        print(f"- í•µì‹¬ ë¬¸ì„œ: {len(moc_data.core_documents)}ê°œ")
        print(f"- ì¹´í…Œê³ ë¦¬: {len(moc_data.categories)}ê°œ")
        print(f"- í•™ìŠµ ë‹¨ê³„: {len(moc_data.learning_path)}ê°œ")
        print(f"- ë¬¸ì„œ ê´€ê³„: {len(moc_data.relationships)}ê°œ")
        
        if moc_data.categories:
            print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ:")
            for category in moc_data.categories:
                print(f"  {category.name}: {len(category.documents)}ê°œ ë¬¸ì„œ")
        
        if moc_data.learning_path:
            print(f"\nğŸ›¤ï¸ í•™ìŠµ ê²½ë¡œ:")
            for step in moc_data.learning_path:
                print(f"  {step.step}. {step.title} ({step.difficulty_level})")
        
        # ë§ˆí¬ë‹¤ìš´ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
        output_file = Path(temp_cache) / "TDD-MOC.md"
        if moc_generator._export_moc(moc_data, str(output_file)):
            print(f"\nğŸ’¾ MOC íŒŒì¼ ì €ì¥: {output_file}")
            
            # ìƒì„±ëœ íŒŒì¼ ì¼ë¶€ í™•ì¸
            with open(output_file, 'r', encoding='utf-8') as f:
                content = f.read()
                print(f"\nğŸ“ ìƒì„±ëœ MOC íŒŒì¼ ì¼ë¶€:")
                print(content[:800] + "..." if len(content) > 800 else content)
        
        # ì •ë¦¬
        shutil.rmtree(temp_vault)
        shutil.rmtree(temp_cache)
        
        print("âœ… MOC ìƒì„±ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ MOC ìƒì„±ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_moc_generator()