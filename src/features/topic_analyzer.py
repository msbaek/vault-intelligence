#!/usr/bin/env python3
"""
Topic Analyzer for Vault Intelligence System V2

Sentence Transformers ì„ë² ë”©ì„ í™œìš©í•œ ì£¼ì œ ë¶„ì„, í´ëŸ¬ìŠ¤í„°ë§, ì§€ì‹ ê·¸ë˜í”„ ìƒì„±
"""

import os
import json
import logging
from typing import List, Dict, Optional, Tuple, Set, Union
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
import numpy as np
from collections import Counter, defaultdict

try:
    from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
    from sklearn.decomposition import PCA
    from sklearn.manifold import TSNE
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False

from ..core.vault_processor import Document

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class TopicCluster:
    """ì£¼ì œë³„ í´ëŸ¬ìŠ¤í„°"""
    id: str
    label: str
    documents: List[Document]
    centroid: Optional[np.ndarray] = None
    coherence_score: Optional[float] = None
    keywords: Optional[List[str]] = None
    representative_doc: Optional[Document] = None
    created_at: Optional[datetime] = None
    
    def get_document_count(self) -> int:
        return len(self.documents)
    
    def get_total_word_count(self) -> int:
        return sum(doc.word_count for doc in self.documents)
    
    def get_average_similarity(self) -> float:
        """í´ëŸ¬ìŠ¤í„° ë‚´ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°"""
        if len(self.documents) < 2:
            return 1.0
        
        similarities = []
        for i, doc1 in enumerate(self.documents):
            for j, doc2 in enumerate(self.documents[i+1:], i+1):
                if doc1.embedding is not None and doc2.embedding is not None:
                    sim = np.dot(doc1.embedding, doc2.embedding)
                    similarities.append(sim)
        
        return np.mean(similarities) if similarities else 0.0
    
    def get_common_tags(self, top_k: int = 5) -> List[Tuple[str, int]]:
        """í´ëŸ¬ìŠ¤í„° ë‚´ ê³µí†µ íƒœê·¸ ì¶”ì¶œ"""
        tag_counter = Counter()
        for doc in self.documents:
            if doc.tags:
                tag_counter.update(doc.tags)
        return tag_counter.most_common(top_k)


@dataclass
class TopicAnalysis:
    """ì£¼ì œ ë¶„ì„ ê²°ê³¼"""
    total_documents: int
    clusters: List[TopicCluster]
    algorithm: str
    n_clusters: int
    silhouette_score: Optional[float] = None
    analysis_date: Optional[datetime] = None
    parameters: Optional[Dict] = None
    
    def get_cluster_count(self) -> int:
        return len(self.clusters)
    
    def get_clustered_document_count(self) -> int:
        return sum(cluster.get_document_count() for cluster in self.clusters)
    
    def get_largest_cluster(self) -> Optional[TopicCluster]:
        if not self.clusters:
            return None
        return max(self.clusters, key=lambda c: c.get_document_count())
    
    def get_cluster_distribution(self) -> Dict[str, int]:
        """í´ëŸ¬ìŠ¤í„°ë³„ ë¬¸ì„œ ìˆ˜ ë¶„í¬"""
        return {cluster.id: cluster.get_document_count() for cluster in self.clusters}


class TopicAnalyzer:
    """ì£¼ì œ ë¶„ì„ê¸°"""
    
    def __init__(
        self,
        search_engine,
        config: Dict = None
    ):
        """
        Args:
            search_engine: AdvancedSearchEngine ì¸ìŠ¤í„´ìŠ¤
            config: ë¶„ì„ ì„¤ì •
        """
        self.search_engine = search_engine
        self.config = config or {}
        
        # ê¸°ë³¸ ì„¤ì •
        self.default_n_clusters = self.config.get('clustering', {}).get(
            'default_n_clusters', 5
        )
        self.min_cluster_size = self.config.get('clustering', {}).get(
            'min_cluster_size', 3
        )
        self.algorithm = self.config.get('clustering', {}).get(
            'algorithm', 'kmeans'
        )
        self.random_state = self.config.get('clustering', {}).get(
            'random_state', 42
        )
        
        # ê°œë°œ ê´€ë ¨ ì£¼ìš” ì£¼ì œë“¤ ì •ì˜
        self.development_topics = [
            "TDD", "refactoring", "AI", "Spring", "DDD", "Clean Code", 
            "Architecture", "Testing", "Docker", "Kubernetes", "React",
            "JavaScript", "Python", "Java", "Database", "API",
            "Microservices", "Design Patterns", "Agile", "DevOps",
            "Kent Beck", "Uncle Bob", "Martin Fowler"
        ]
        
        logger.info(f"ì£¼ì œ ë¶„ì„ê¸° ì´ˆê¸°í™” - ê¸°ë³¸ í´ëŸ¬ìŠ¤í„° ìˆ˜: {self.default_n_clusters}")
    
    def analyze_topics(
        self,
        topic_query: Optional[str] = None,
        n_clusters: Optional[int] = None,
        algorithm: Optional[str] = None,
        min_cluster_size: Optional[int] = None
    ) -> TopicAnalysis:
        """ì£¼ì œë³„ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„"""
        try:
            if not self.search_engine.indexed:
                logger.warning("ê²€ìƒ‰ ì—”ì§„ì´ ì¸ë±ì‹±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return self._create_empty_analysis()
            
            # ì„¤ì •ê°’ ì‚¬ìš©
            n_clusters = n_clusters or self.default_n_clusters
            algorithm = algorithm or self.algorithm
            min_cluster_size = min_cluster_size or self.min_cluster_size
            
            logger.info(f"ì£¼ì œ ë¶„ì„ ì‹œì‘ - ì•Œê³ ë¦¬ì¦˜: {algorithm}, í´ëŸ¬ìŠ¤í„°: {n_clusters}")
            
            # ë¶„ì„í•  ë¬¸ì„œ í•„í„°ë§
            documents = self._filter_documents_for_analysis(topic_query)
            
            if len(documents) < n_clusters:
                logger.warning(f"ë¬¸ì„œê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {len(documents)} < {n_clusters}")
                n_clusters = max(1, len(documents) // 2)
            
            logger.info(f"ë¶„ì„ ëŒ€ìƒ ë¬¸ì„œ: {len(documents)}ê°œ")
            
            # ì„ë² ë”© ì¶”ì¶œ
            embeddings = self._extract_embeddings(documents)
            
            if embeddings is None or len(embeddings) == 0:
                logger.error("ìœ íš¨í•œ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return self._create_empty_analysis()
            
            # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
            cluster_labels = self._perform_clustering(
                embeddings, algorithm, n_clusters
            )
            
            # í´ëŸ¬ìŠ¤í„° ìƒì„±
            clusters = self._create_clusters(documents, embeddings, cluster_labels)
            
            # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
            silhouette = self._calculate_silhouette_score(embeddings, cluster_labels)
            
            logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ - {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°, ì‹¤ë£¨ì—£: {silhouette:.3f}")
            
            # ë¶„ì„ ê²°ê³¼ ìƒì„±
            analysis = TopicAnalysis(
                total_documents=len(documents),
                clusters=clusters,
                algorithm=algorithm,
                n_clusters=len(clusters),
                silhouette_score=silhouette,
                analysis_date=datetime.now(),
                parameters={
                    "n_clusters": n_clusters,
                    "min_cluster_size": min_cluster_size,
                    "topic_query": topic_query
                }
            )
            
            return analysis
            
        except Exception as e:
            logger.error(f"ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._create_empty_analysis()
    
    def analyze_topics_by_predefined_subjects(self, min_docs_per_topic: int = 10) -> TopicAnalysis:
        """ë¯¸ë¦¬ ì •ì˜ëœ ì£¼ì œë“¤ë¡œ ë¬¸ì„œë¥¼ ë¶„ë¥˜í•˜ëŠ” ì£¼ì œ ê¸°ë°˜ ë¶„ì„"""
        try:
            logger.info(f"ì£¼ì œ ê¸°ë°˜ ë¶„ì„ ì‹œì‘ - {len(self.development_topics)}ê°œ ì£¼ì œ")
            
            topic_clusters = []
            analyzed_docs = set()  # ì´ë¯¸ ë¶„ì„ëœ ë¬¸ì„œë“¤ ì¶”ì 
            
            # ê° ì£¼ì œë³„ë¡œ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘
            for topic in self.development_topics:
                logger.info(f"ì£¼ì œ '{topic}' ë¶„ì„ ì¤‘...")
                
                # collect ê¸°ëŠ¥ì„ í™œìš©í•´ì„œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
                try:
                    from ..features.topic_collector import TopicCollector
                    collector = TopicCollector(self.search_engine, self.config)
                    collection = collector.collect_topic(topic, top_k=min_docs_per_topic * 2)
                    
                    if collection and len(collection.documents) >= min_docs_per_topic:
                        # ì´ë¯¸ ë‹¤ë¥¸ ì£¼ì œì— ë¶„ë¥˜ëœ ë¬¸ì„œëŠ” ì œì™¸ (ì¤‘ë³µ ë°©ì§€)
                        unique_docs = []
                        for doc in collection.documents[:min_docs_per_topic * 3]:  # ì—¬ìœ ìˆê²Œ ê°€ì ¸ì™€ì„œ í•„í„°ë§
                            if doc.path not in analyzed_docs:
                                unique_docs.append(doc)
                                analyzed_docs.add(doc.path)
                            if len(unique_docs) >= min_docs_per_topic:
                                break
                        
                        if len(unique_docs) >= min_docs_per_topic:
                            # í´ëŸ¬ìŠ¤í„° ìƒì„±
                            cluster = TopicCluster(
                                id=f"topic_{topic.lower().replace(' ', '_')}",
                                label=topic,
                                documents=unique_docs
                            )
                            
                            # í´ëŸ¬ìŠ¤í„° ë¶„ì„
                            self._analyze_cluster(cluster)
                            topic_clusters.append(cluster)
                            
                            logger.info(f"ì£¼ì œ '{topic}': {len(unique_docs)}ê°œ ë¬¸ì„œ ë°œê²¬")
                        else:
                            logger.info(f"ì£¼ì œ '{topic}': ë¬¸ì„œ ìˆ˜ ë¶€ì¡± ({len(unique_docs)}ê°œ < {min_docs_per_topic}ê°œ)")
                    else:
                        logger.info(f"ì£¼ì œ '{topic}': ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
                        
                except Exception as e:
                    logger.warning(f"ì£¼ì œ '{topic}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            # ë¶„ì„ ê²°ê³¼ ìƒì„±
            total_analyzed = sum(len(cluster.documents) for cluster in topic_clusters)
            
            analysis = TopicAnalysis(
                clusters=topic_clusters,
                total_documents=total_analyzed,
                algorithm="topic-based",
                n_clusters=len(topic_clusters),  # ì‹¤ì œ ìƒì„±ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜
                silhouette_score=None,  # ì£¼ì œ ê¸°ë°˜ ë¶„ì„ì—ì„œëŠ” ì ìš©ë˜ì§€ ì•ŠìŒ
                analysis_date=datetime.now(),  # ë¶„ì„ ë‚ ì§œ ì¶”ê°€
                parameters={
                    "min_docs_per_topic": min_docs_per_topic,
                    "total_topics_analyzed": len(self.development_topics),
                    "topics_found": len(topic_clusters),
                    "analysis_coverage": f"{total_analyzed}/{len(self.search_engine.documents)} ({total_analyzed/len(self.search_engine.documents)*100:.1f}%)"
                }
            )
            
            logger.info(f"ì£¼ì œ ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ: {len(topic_clusters)}ê°œ ì£¼ì œ, {total_analyzed}ê°œ ë¬¸ì„œ")
            return analysis
            
        except Exception as e:
            logger.error(f"ì£¼ì œ ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._create_empty_analysis()
    
    def _analyze_cluster(self, cluster: TopicCluster):
        """í´ëŸ¬ìŠ¤í„°ì˜ í‚¤ì›Œë“œ, ëŒ€í‘œ ë¬¸ì„œ, ì¼ê´€ì„± ì ìˆ˜ ë“±ì„ ë¶„ì„"""
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            cluster.keywords = self._extract_cluster_keywords(cluster.documents)
            
            # ëŒ€í‘œ ë¬¸ì„œ ì„ ì • (ê°€ì¥ ê¸´ ë¬¸ì„œ ë˜ëŠ” ì²« ë²ˆì§¸ ë¬¸ì„œ)
            if cluster.documents:
                # ë‹¨ì–´ ìˆ˜ê°€ ê°€ì¥ ë§ì€ ë¬¸ì„œë¥¼ ëŒ€í‘œ ë¬¸ì„œë¡œ ì„ ì •
                cluster.representative_doc = max(cluster.documents, key=lambda doc: doc.word_count or 0)
            
            # ìƒì„± ì‹œê°„ ì„¤ì •
            cluster.created_at = datetime.now()
            
            # ì„ë² ë”©ì´ ìˆëŠ” ê²½ìš° ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
            embeddings = []
            for doc in cluster.documents:
                if hasattr(doc, 'embedding') and doc.embedding is not None:
                    embeddings.append(doc.embedding)
            
            if len(embeddings) > 1:
                cluster.coherence_score = self._calculate_cluster_coherence(np.array(embeddings))
            else:
                cluster.coherence_score = 1.0
                
        except Exception as e:
            logger.warning(f"í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ê°’ ì„¤ì •
            cluster.keywords = []
            cluster.representative_doc = cluster.documents[0] if cluster.documents else None
            cluster.coherence_score = 0.0
            cluster.created_at = datetime.now()
    
    def _filter_documents_for_analysis(self, topic_query: Optional[str]) -> List[Document]:
        """ë¶„ì„í•  ë¬¸ì„œ í•„í„°ë§"""
        documents = self.search_engine.documents
        
        if topic_query:
            # ì£¼ì œ ê´€ë ¨ ë¬¸ì„œë§Œ í•„í„°ë§
            filtered_docs = []
            query_lower = topic_query.lower()
            
            for doc in documents:
                # ì œëª©, íƒœê·¸, ë‚´ìš©ì—ì„œ ì£¼ì œ ê²€ìƒ‰
                if (query_lower in doc.title.lower() or
                    any(query_lower in tag.lower() for tag in doc.tags) or
                    query_lower in doc.content.lower()):
                    filtered_docs.append(doc)
            
            documents = filtered_docs
            logger.info(f"ì£¼ì œ '{topic_query}' ê´€ë ¨ ë¬¸ì„œ: {len(documents)}ê°œ")
        
        # ì„ë² ë”©ì´ ìˆëŠ” ë¬¸ì„œë§Œ
        valid_docs = []
        total_checked = 0
        has_embedding_attr = 0
        embedding_not_none = 0
        
        for doc in documents:
            total_checked += 1
            if hasattr(doc, 'embedding'):
                has_embedding_attr += 1
                if doc.embedding is not None:
                    embedding_not_none += 1
                    # numpy ë°°ì—´ì¸ì§€ í™•ì¸í•˜ê³  0ì´ ì•„ë‹Œì§€ ì²´í¬
                    if isinstance(doc.embedding, np.ndarray) and doc.embedding.size > 0:
                        if not np.allclose(doc.embedding, 0):
                            valid_docs.append(doc)
                    elif hasattr(doc.embedding, '__len__') and len(doc.embedding) > 0:
                        # ë¦¬ìŠ¤íŠ¸ë‚˜ ë‹¤ë¥¸ í˜•íƒœì˜ ë°°ì—´ì¸ ê²½ìš°
                        valid_docs.append(doc)
        
        logger.info(f"ì„ë² ë”© ì²´í¬ ê²°ê³¼: ì „ì²´={total_checked}, embedding ì†ì„±={has_embedding_attr}, "
                   f"Noneì´ ì•„ë‹˜={embedding_not_none}, ìœ íš¨={len(valid_docs)}")
        
        logger.info(f"ìœ íš¨í•œ ì„ë² ë”©ì„ ê°€ì§„ ë¬¸ì„œ: {len(valid_docs)}ê°œ")
        return valid_docs
    
    def _extract_embeddings(self, documents: List[Document]) -> Optional[np.ndarray]:
        """ë¬¸ì„œë“¤ì˜ ì„ë² ë”© ì¶”ì¶œ"""
        try:
            embeddings = []
            for doc in documents:
                if doc.embedding is not None:
                    embeddings.append(doc.embedding)
                else:
                    # ë¹ˆ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´
                    empty_embedding = np.zeros(self.search_engine.engine.embedding_dimension)
                    embeddings.append(empty_embedding)
            
            return np.array(embeddings) if embeddings else None
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _perform_clustering(
        self,
        embeddings: np.ndarray,
        algorithm: str,
        n_clusters: int
    ) -> np.ndarray:
        """í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰"""
        try:
            if not SKLEARN_AVAILABLE:
                logger.error("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return np.zeros(len(embeddings))
            
            if algorithm.lower() == "kmeans":
                clusterer = KMeans(
                    n_clusters=n_clusters,
                    random_state=self.random_state,
                    n_init=10
                )
            elif algorithm.lower() == "dbscan":
                # DBSCANì˜ ê²½ìš° eps ìë™ ì¶”ì •
                eps = self._estimate_dbscan_eps(embeddings)
                clusterer = DBSCAN(eps=eps, min_samples=self.min_cluster_size)
            elif algorithm.lower() == "hierarchical":
                clusterer = AgglomerativeClustering(
                    n_clusters=n_clusters,
                    linkage='ward'
                )
            else:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì•Œê³ ë¦¬ì¦˜: {algorithm}, KMeans ì‚¬ìš©")
                clusterer = KMeans(
                    n_clusters=n_clusters,
                    random_state=self.random_state
                )
            
            cluster_labels = clusterer.fit_predict(embeddings)
            logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(set(cluster_labels))}ê°œ í´ëŸ¬ìŠ¤í„°")
            
            return cluster_labels
            
        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return np.zeros(len(embeddings))
    
    def _estimate_dbscan_eps(self, embeddings: np.ndarray) -> float:
        """DBSCAN eps íŒŒë¼ë¯¸í„° ìë™ ì¶”ì •"""
        try:
            from sklearn.neighbors import NearestNeighbors
            
            # k-distance ê·¸ë˜í”„ë¡œ eps ì¶”ì •
            k = min(4, len(embeddings) - 1)
            nbrs = NearestNeighbors(n_neighbors=k).fit(embeddings)
            distances, _ = nbrs.kneighbors(embeddings)
            
            # kë²ˆì§¸ ê°€ê¹Œìš´ ì´ì›ƒê¹Œì§€ì˜ í‰ê·  ê±°ë¦¬
            k_distances = np.sort(distances[:, k-1])
            
            # ê¸‰ê²©í•œ ë³€í™”ê°€ ìˆëŠ” ì§€ì ì„ epsë¡œ ì‚¬ìš©
            # ê°„ë‹¨í•œ ë°©ë²•: ìƒìœ„ 20% ì§€ì 
            eps = k_distances[int(len(k_distances) * 0.8)]
            
            logger.info(f"DBSCAN eps ì¶”ì •ê°’: {eps:.4f}")
            return eps
            
        except Exception as e:
            logger.error(f"DBSCAN eps ì¶”ì • ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    def _create_clusters(
        self,
        documents: List[Document],
        embeddings: np.ndarray,
        cluster_labels: np.ndarray
    ) -> List[TopicCluster]:
        """í´ëŸ¬ìŠ¤í„° ê°ì²´ ìƒì„±"""
        try:
            clusters = []
            unique_labels = set(cluster_labels)
            
            for label in unique_labels:
                if label == -1:  # DBSCAN ë…¸ì´ì¦ˆ
                    continue
                
                # í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë¬¸ì„œë“¤
                cluster_indices = np.where(cluster_labels == label)[0]
                cluster_docs = [documents[i] for i in cluster_indices]
                cluster_embeddings = embeddings[cluster_indices]
                
                # ìµœì†Œ í¬ê¸° í™•ì¸
                if len(cluster_docs) < self.min_cluster_size:
                    continue
                
                # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ê³„ì‚°
                centroid = np.mean(cluster_embeddings, axis=0)
                
                # ëŒ€í‘œ ë¬¸ì„œ ì„ ì • (ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì„œ)
                representative_doc = self._find_representative_document(
                    cluster_docs, cluster_embeddings, centroid
                )
                
                # í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ìƒì„±
                cluster_label = self._generate_cluster_label(cluster_docs, representative_doc)
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self._extract_cluster_keywords(cluster_docs)
                
                # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
                coherence_score = self._calculate_cluster_coherence(cluster_embeddings)
                
                cluster = TopicCluster(
                    id=f"cluster_{label}",
                    label=cluster_label,
                    documents=cluster_docs,
                    centroid=centroid,
                    coherence_score=coherence_score,
                    keywords=keywords,
                    representative_doc=representative_doc,
                    created_at=datetime.now()
                )
                
                clusters.append(cluster)
            
            # í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬
            clusters.sort(key=lambda c: c.get_document_count(), reverse=True)
            
            return clusters
            
        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _find_representative_document(
        self,
        docs: List[Document],
        embeddings: np.ndarray,
        centroid: np.ndarray
    ) -> Document:
        """ëŒ€í‘œ ë¬¸ì„œ ì°¾ê¸° (ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì„œ)"""
        try:
            similarities = np.dot(embeddings, centroid)
            best_idx = np.argmax(similarities)
            return docs[best_idx]
        except Exception as e:
            logger.error(f"ëŒ€í‘œ ë¬¸ì„œ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return docs[0] if docs else None
    
    def _generate_cluster_label(
        self,
        docs: List[Document],
        representative_doc: Document
    ) -> str:
        """í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ìƒì„±"""
        try:
            # 1ìˆœìœ„: ëŒ€í‘œ ë¬¸ì„œ ì œëª©
            if representative_doc and representative_doc.title:
                return representative_doc.title[:50]
            
            # 2ìˆœìœ„: ê°€ì¥ ë¹ˆë²ˆí•œ íƒœê·¸
            tag_counter = Counter()
            for doc in docs:
                if doc.tags:
                    tag_counter.update(doc.tags)
            
            if tag_counter:
                most_common_tag = tag_counter.most_common(1)[0][0]
                return f"ì£¼ì œ: {most_common_tag}"
            
            # 3ìˆœìœ„: ì²« ë²ˆì§¸ ë¬¸ì„œ ì œëª©
            if docs and docs[0].title:
                return docs[0].title[:50]
            
            return "ë¯¸ë¶„ë¥˜ ì£¼ì œ"
            
        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì•Œ ìˆ˜ ì—†ëŠ” ì£¼ì œ"
    
    def _extract_cluster_keywords(self, docs: List[Document]) -> List[str]:
        """í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # íƒœê·¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            tag_counter = Counter()
            for doc in docs:
                if doc.tags:
                    tag_counter.update(doc.tags)
            
            keywords = [tag for tag, count in tag_counter.most_common(10)]
            
            # ì œëª©ì—ì„œ ê³µí†µ ë‹¨ì–´ ì¶”ì¶œ
            title_words = []
            for doc in docs:
                if doc.title:
                    words = doc.title.split()
                    title_words.extend([w.lower().strip('.,!?') for w in words if len(w) > 2])
            
            word_counter = Counter(title_words)
            common_words = [word for word, count in word_counter.most_common(5) if count > 1]
            
            keywords.extend(common_words)
            
            return list(set(keywords))[:15]  # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 15ê°œ
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_cluster_coherence(self, embeddings: np.ndarray) -> float:
        """í´ëŸ¬ìŠ¤í„° ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            if len(embeddings) < 2:
                return 1.0
            
            # í´ëŸ¬ìŠ¤í„° ë‚´ í‰ê·  ìœ ì‚¬ë„
            similarities = []
            for i in range(len(embeddings)):
                for j in range(i + 1, len(embeddings)):
                    sim = np.dot(embeddings[i], embeddings[j])
                    similarities.append(sim)
            
            return np.mean(similarities) if similarities else 0.0
            
        except Exception as e:
            logger.error(f"ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_silhouette_score(
        self,
        embeddings: np.ndarray,
        labels: np.ndarray
    ) -> float:
        """ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°"""
        try:
            if not SKLEARN_AVAILABLE:
                return 0.0
            
            if len(set(labels)) < 2:
                return 0.0
            
            return silhouette_score(embeddings, labels)
            
        except Exception as e:
            logger.error(f"ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _create_empty_analysis(self) -> TopicAnalysis:
        """ë¹ˆ ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        return TopicAnalysis(
            total_documents=0,
            clusters=[],
            algorithm=self.algorithm,
            n_clusters=0,
            analysis_date=datetime.now()
        )
    
    def visualize_clusters(
        self,
        analysis: TopicAnalysis,
        output_file: Optional[str] = None,
        method: str = "pca"
    ) -> bool:
        """í´ëŸ¬ìŠ¤í„° ì‹œê°í™”"""
        try:
            if not SKLEARN_AVAILABLE:
                logger.error("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
            
            if not MATPLOTLIB_AVAILABLE:
                logger.error("matplotlibì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
            
            if not analysis.clusters:
                logger.warning("ì‹œê°í™”í•  í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ëª¨ë“  ì„ë² ë”©ê³¼ ë¼ë²¨ ìˆ˜ì§‘
            all_embeddings = []
            all_labels = []
            all_titles = []
            
            for i, cluster in enumerate(analysis.clusters):
                for doc in cluster.documents:
                    if doc.embedding is not None:
                        all_embeddings.append(doc.embedding)
                        all_labels.append(i)
                        all_titles.append(doc.title[:30])
            
            if not all_embeddings:
                logger.warning("ì‹œê°í™”í•  ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            embeddings_array = np.array(all_embeddings)
            
            # ì°¨ì› ì¶•ì†Œ
            if method.lower() == "pca":
                reducer = PCA(n_components=2, random_state=self.random_state)
            elif method.lower() == "tsne":
                reducer = TSNE(n_components=2, random_state=self.random_state, perplexity=min(30, len(embeddings_array)-1))
            else:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì°¨ì›ì¶•ì†Œ ë°©ë²•: {method}, PCA ì‚¬ìš©")
                reducer = PCA(n_components=2, random_state=self.random_state)
            
            reduced_embeddings = reducer.fit_transform(embeddings_array)
            
            # í”Œë¡¯ ìƒì„±
            plt.figure(figsize=(12, 8))
            
            colors = plt.cm.Set3(np.linspace(0, 1, len(analysis.clusters)))
            
            for i, cluster in enumerate(analysis.clusters):
                cluster_points = reduced_embeddings[np.array(all_labels) == i]
                if len(cluster_points) > 0:
                    plt.scatter(
                        cluster_points[:, 0],
                        cluster_points[:, 1],
                        c=[colors[i]],
                        label=f"{cluster.label} ({len(cluster.documents)}ê°œ)",
                        alpha=0.7,
                        s=50
                    )
            
            plt.title(f'ë¬¸ì„œ í´ëŸ¬ìŠ¤í„° ì‹œê°í™” ({method.upper()})\n'
                     f'ì´ {analysis.total_documents}ê°œ ë¬¸ì„œ, {len(analysis.clusters)}ê°œ í´ëŸ¬ìŠ¤í„°')
            plt.xlabel(f'{method.upper()} Component 1')
            plt.ylabel(f'{method.upper()} Component 2')
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            if output_file:
                plt.savefig(output_file, dpi=300, bbox_inches='tight')
                logger.info(f"í´ëŸ¬ìŠ¤í„° ì‹œê°í™” ì €ì¥: {output_file}")
            else:
                plt.show()
            
            plt.close()
            return True
            
        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„° ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def export_analysis(self, analysis: TopicAnalysis, output_file: str) -> bool:
        """ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            export_data = {
                "analysis_info": {
                    "total_documents": analysis.total_documents,
                    "cluster_count": analysis.get_cluster_count(),
                    "clustered_documents": analysis.get_clustered_document_count(),
                    "algorithm": analysis.algorithm,
                    "silhouette_score": analysis.silhouette_score,
                    "analysis_date": analysis.analysis_date.isoformat() if analysis.analysis_date else None,
                    "parameters": analysis.parameters
                },
                "clusters": []
            }
            
            for cluster in analysis.clusters:
                cluster_data = {
                    "id": cluster.id,
                    "label": cluster.label,
                    "document_count": cluster.get_document_count(),
                    "total_word_count": cluster.get_total_word_count(),
                    "coherence_score": cluster.coherence_score,
                    "keywords": cluster.keywords,
                    "representative_document": {
                        "path": cluster.representative_doc.path,
                        "title": cluster.representative_doc.title
                    } if cluster.representative_doc else None,
                    "common_tags": cluster.get_common_tags(10),
                    "average_similarity": cluster.get_average_similarity(),
                    "documents": [
                        {
                            "path": doc.path,
                            "title": doc.title,
                            "word_count": doc.word_count,
                            "tags": doc.tags,
                            "modified_at": doc.modified_at.isoformat()
                        }
                        for doc in cluster.documents
                    ]
                }
                export_data["clusters"].append(cluster_data)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ì£¼ì œ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def export_markdown_report(self, analysis: TopicAnalysis, output_file: str) -> bool:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            from datetime import datetime
            
            # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
            markdown_content = []
            
            # í—¤ë”
            markdown_content.append("# ğŸ“Š Vault ì£¼ì œ ë¶„ì„ ë³´ê³ ì„œ")
            markdown_content.append("")
            markdown_content.append(f"**ìƒì„±ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            markdown_content.append("")
            
            # ì „ì²´ í†µê³„
            markdown_content.append("## ğŸ“ˆ ì „ì²´ í†µê³„")
            markdown_content.append("")
            markdown_content.append(f"- **ë¶„ì„ ë¬¸ì„œ ìˆ˜**: {analysis.total_documents:,}ê°œ")
            markdown_content.append(f"- **ë°œê²¬ í´ëŸ¬ìŠ¤í„° ìˆ˜**: {analysis.get_cluster_count()}ê°œ")
            markdown_content.append(f"- **í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜**: {analysis.algorithm}")
            if analysis.silhouette_score is not None:
                markdown_content.append(f"- **ì‹¤ë£¨ì—£ ì ìˆ˜**: {analysis.silhouette_score:.3f}")
            markdown_content.append("")
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬
            if analysis.clusters:
                markdown_content.append("## ğŸ“Š í´ëŸ¬ìŠ¤í„° ë¶„í¬")
                markdown_content.append("")
                markdown_content.append("| ìˆœìœ„ | í´ëŸ¬ìŠ¤í„°ëª… | ë¬¸ì„œ ìˆ˜ | ë¹„ìœ¨ | ì¼ê´€ì„± ì ìˆ˜ |")
                markdown_content.append("|------|------------|---------|------|-------------|")
                
                for i, cluster in enumerate(analysis.clusters, 1):
                    percentage = (cluster.get_document_count() / analysis.total_documents) * 100
                    coherence = f"{cluster.coherence_score:.3f}" if cluster.coherence_score else "N/A"
                    cluster_name = cluster.label[:50] + ("..." if len(cluster.label) > 50 else "")
                    markdown_content.append(
                        f"| {i} | {cluster_name} | {cluster.get_document_count():,} | {percentage:.1f}% | {coherence} |"
                    )
                
                markdown_content.append("")
                
                # ê° í´ëŸ¬ìŠ¤í„° ìƒì„¸ ì •ë³´
                markdown_content.append("## ğŸ·ï¸ í´ëŸ¬ìŠ¤í„° ìƒì„¸ ë¶„ì„")
                markdown_content.append("")
                
                for i, cluster in enumerate(analysis.clusters, 1):
                    markdown_content.append(f"### {i}. {cluster.label}")
                    markdown_content.append("")
                    markdown_content.append(f"**ğŸ“Š ê¸°ë³¸ ì •ë³´**")
                    markdown_content.append(f"- ë¬¸ì„œ ìˆ˜: **{cluster.get_document_count():,}ê°œ**")
                    markdown_content.append(f"- ì´ ë‹¨ì–´ ìˆ˜: {cluster.get_total_word_count():,}ê°œ")
                    if cluster.coherence_score is not None:
                        markdown_content.append(f"- ì¼ê´€ì„± ì ìˆ˜: {cluster.coherence_score:.3f}")
                    markdown_content.append("")
                    
                    # ì£¼ìš” í‚¤ì›Œë“œ
                    if cluster.keywords:
                        markdown_content.append("**ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ**")
                        keywords_str = ", ".join([f"`{kw}`" for kw in cluster.keywords[:10]])
                        markdown_content.append(f"{keywords_str}")
                        markdown_content.append("")
                    
                    # ëŒ€í‘œ ë¬¸ì„œ
                    if cluster.representative_doc:
                        markdown_content.append("**ğŸ“„ ëŒ€í‘œ ë¬¸ì„œ**")
                        markdown_content.append(f"- [{cluster.representative_doc.title}]({cluster.representative_doc.path})")
                        markdown_content.append("")
                    
                    # ê³µí†µ íƒœê·¸
                    common_tags = cluster.get_common_tags(10)
                    if common_tags:
                        markdown_content.append("**ğŸ·ï¸ ê³µí†µ íƒœê·¸**")
                        tags_info = []
                        for tag, count in common_tags:
                            tags_info.append(f"`{tag}` ({count})")
                        markdown_content.append(", ".join(tags_info))
                        markdown_content.append("")
                    
                    # ë¬¸ì„œ ëª©ë¡ (ìƒìœ„ 10ê°œ)
                    markdown_content.append("**ğŸ“š ì£¼ìš” ë¬¸ì„œë“¤** (ìƒìœ„ 10ê°œ)")
                    markdown_content.append("")
                    for j, doc in enumerate(cluster.documents[:10], 1):
                        word_count = f"({doc.word_count}ë‹¨ì–´)" if doc.word_count else ""
                        markdown_content.append(f"{j}. [{doc.title}]({doc.path}) {word_count}")
                    
                    if len(cluster.documents) > 10:
                        remaining = len(cluster.documents) - 10
                        markdown_content.append(f"... ì™¸ {remaining}ê°œ ë¬¸ì„œ")
                    
                    markdown_content.append("")
                    markdown_content.append("---")
                    markdown_content.append("")
            
            # ë¶„ì„ ë§¤ê°œë³€ìˆ˜
            if analysis.parameters:
                markdown_content.append("## âš™ï¸ ë¶„ì„ ë§¤ê°œë³€ìˆ˜")
                markdown_content.append("")
                for key, value in analysis.parameters.items():
                    markdown_content.append(f"- **{key}**: {value}")
                markdown_content.append("")
            
            # íŒŒì¼ì— ì €ì¥
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(markdown_content))
            
            logger.info(f"ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return False


def test_topic_analyzer():
    """ì£¼ì œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    import tempfile
    import shutil
    from ..features.advanced_search import AdvancedSearchEngine
    
    try:
        # ì„ì‹œ vault ë° ìºì‹œ ìƒì„±
        temp_vault = tempfile.mkdtemp()
        temp_cache = tempfile.mkdtemp()
        
        # ë‹¤ì–‘í•œ ì£¼ì œì˜ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤ ìƒì„±
        test_docs = [
            ("tdd1.md", "# TDD ê¸°ë³¸ ê°œë…\n\nTDDëŠ” í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.\nRed-Green-Refactor ì‚¬ì´í´ì„ ë”°ë¦…ë‹ˆë‹¤."),
            ("tdd2.md", "# TDD ì‹¤ìŠµ\n\ní…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ê³  êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì—°ìŠµí•©ë‹ˆë‹¤.\në‹¨ìœ„ í…ŒìŠ¤íŠ¸ì™€ í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."),
            ("refactoring1.md", "# ë¦¬íŒ©í† ë§ ê¸°ë²•\n\nì½”ë“œì˜ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì…ë‹ˆë‹¤.\në©”ì„œë“œ ì¶”ì¶œ, í´ë˜ìŠ¤ ë¶„ë¦¬ ë“±ì˜ ê¸°ë²•ì´ ìˆìŠµë‹ˆë‹¤."),
            ("refactoring2.md", "# ì•ˆì „í•œ ë¦¬íŒ©í† ë§\n\ní…ŒìŠ¤íŠ¸ê°€ ìˆì„ ë•Œ ì•ˆì „í•˜ê²Œ ë¦¬íŒ©í† ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\níšŒê·€ ë²„ê·¸ë¥¼ ë°©ì§€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."),
            ("clean_code1.md", "# Clean Code ì›ì¹™\n\nê¹¨ë—í•œ ì½”ë“œ ì‘ì„±ì˜ ê¸°ë³¸ ì›ì¹™ë“¤ì…ë‹ˆë‹¤.\nì˜ë¯¸ ìˆëŠ” ì´ë¦„, ì‘ì€ í•¨ìˆ˜, ëª…í™•í•œ ì˜ë„ í‘œí˜„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤."),
            ("clean_code2.md", "# ì½”ë“œ í’ˆì§ˆ\n\nì¢‹ì€ ì½”ë“œëŠ” ì½ê¸° ì‰½ê³  ì´í•´í•˜ê¸° ì‰¬ì›Œì•¼ í•©ë‹ˆë‹¤.\nì£¼ì„ë³´ë‹¤ëŠ” ì½”ë“œ ìì²´ê°€ ì„¤ëª…ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."),
            ("architecture1.md", "# ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…ì²˜\n\nì‹œìŠ¤í…œì˜ ì „ì²´ì ì¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\nê³„ì¸µí™”, ëª¨ë“ˆí™”, ì˜ì¡´ì„± ê´€ë¦¬ê°€ í•µì‹¬ì…ë‹ˆë‹¤."),
            ("architecture2.md", "# ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜\n\nì‘ì€ ì„œë¹„ìŠ¤ë“¤ë¡œ ì‹œìŠ¤í…œì„ êµ¬ì„±í•˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤.\në…ë¦½ì  ë°°í¬ì™€ í™•ì¥ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        ]
        
        for filename, content in test_docs:
            with open(Path(temp_vault) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ë° ì¸ë±ì‹±
        config = {
            "model": {"name": "paraphrase-multilingual-mpnet-base-v2"},
            "vault": {"excluded_dirs": [], "file_extensions": [".md"]},
            "clustering": {
                "default_n_clusters": 4,
                "min_cluster_size": 2,
                "algorithm": "kmeans"
            }
        }
        
        search_engine = AdvancedSearchEngine(temp_vault, temp_cache, config)
        print("ê²€ìƒ‰ ì—”ì§„ ì¸ë±ì‹± ì¤‘...")
        if not search_engine.build_index():
            print("âŒ ì¸ë±ì‹± ì‹¤íŒ¨")
            return False
        
        # ì£¼ì œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = TopicAnalyzer(search_engine, config)
        
        # ì „ì²´ ë¬¸ì„œ ì£¼ì œ ë¶„ì„
        print("\nğŸ” ì „ì²´ ë¬¸ì„œ ì£¼ì œ ë¶„ì„ ì¤‘...")
        analysis = analyzer.analyze_topics(n_clusters=4)
        
        print(f"\nğŸ“Š ì£¼ì œ ë¶„ì„ ê²°ê³¼:")
        print(f"- ì´ ë¬¸ì„œ: {analysis.total_documents}ê°œ")
        print(f"- í´ëŸ¬ìŠ¤í„°: {analysis.get_cluster_count()}ê°œ")
        print(f"- ì‹¤ë£¨ì—£ ì ìˆ˜: {analysis.silhouette_score:.3f}")
        
        # í´ëŸ¬ìŠ¤í„° ì„¸ë¶€ ì •ë³´
        for i, cluster in enumerate(analysis.clusters):
            print(f"\nğŸ“ í´ëŸ¬ìŠ¤í„° {i+1}: {cluster.label}")
            print(f"   ë¬¸ì„œ ìˆ˜: {cluster.get_document_count()}ê°œ")
            print(f"   ì¼ê´€ì„±: {cluster.coherence_score:.3f}")
            print(f"   í‚¤ì›Œë“œ: {cluster.keywords[:5]}")
            print(f"   ë¬¸ì„œë“¤:")
            for doc in cluster.documents:
                print(f"     - {doc.path}")
        
        # íŠ¹ì • ì£¼ì œ ë¶„ì„
        print(f"\nğŸ” 'TDD' ì£¼ì œ ë¶„ì„ ì¤‘...")
        tdd_analysis = analyzer.analyze_topics(topic_query="TDD", n_clusters=2)
        
        print(f"\nğŸ“Š TDD ì£¼ì œ ë¶„ì„ ê²°ê³¼:")
        print(f"- TDD ê´€ë ¨ ë¬¸ì„œ: {tdd_analysis.total_documents}ê°œ")
        print(f"- í´ëŸ¬ìŠ¤í„°: {tdd_analysis.get_cluster_count()}ê°œ")
        
        for cluster in tdd_analysis.clusters:
            print(f"  - {cluster.label}: {cluster.get_document_count()}ê°œ ë¬¸ì„œ")
        
        # ê²°ê³¼ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
        output_file = Path(temp_cache) / "topic_analysis.json"
        if analyzer.export_analysis(analysis, str(output_file)):
            print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ì‹œê°í™” í…ŒìŠ¤íŠ¸ (matplotlib ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if SKLEARN_AVAILABLE:
            viz_file = Path(temp_cache) / "cluster_visualization.png"
            try:
                if analyzer.visualize_clusters(analysis, str(viz_file)):
                    print(f"ğŸ“ˆ í´ëŸ¬ìŠ¤í„° ì‹œê°í™” ì €ì¥: {viz_file}")
            except:
                print("ğŸ“ˆ ì‹œê°í™”ëŠ” ê±´ë„ˆëœ€ (matplotlib ì—†ìŒ)")
        
        # ì •ë¦¬
        shutil.rmtree(temp_vault)
        shutil.rmtree(temp_cache)
        
        print("âœ… ì£¼ì œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì£¼ì œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_topic_analyzer()