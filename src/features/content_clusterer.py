#!/usr/bin/env python3
"""
Content Clusterer for Vault Intelligence System V2 - Phase 9

BGE-M3 ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ì  ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ
"""

import os
import logging
import numpy as np
from typing import List, Dict, Optional, Tuple, Union
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
from collections import Counter, defaultdict

try:
    from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
    from sklearn.metrics import silhouette_score
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

from ..core.vault_processor import Document
from ..core.sentence_transformer_engine import SentenceTransformerEngine
from ..core.embedding_cache import EmbeddingCache

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class DocumentCluster:
    """ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°"""
    id: str
    label: str
    documents: List[Document]
    representative_doc: Optional[Document] = None
    keywords: List[str] = None
    similarity_score: float = 0.0
    centroid: Optional[np.ndarray] = None
    size: int = 0
    
    def __post_init__(self):
        self.size = len(self.documents)


@dataclass
class ClusteringResult:
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼"""
    clusters: List[DocumentCluster]
    algorithm: str
    n_clusters: int
    silhouette_score: float
    parameters: Dict
    total_documents: int
    timestamp: datetime
    
    def get_cluster_count(self) -> int:
        """í´ëŸ¬ìŠ¤í„° ìˆ˜ ë°˜í™˜"""
        return len(self.clusters)
    
    def get_largest_cluster(self) -> Optional[DocumentCluster]:
        """ê°€ì¥ í° í´ëŸ¬ìŠ¤í„° ë°˜í™˜"""
        if not self.clusters:
            return None
        return max(self.clusters, key=lambda x: x.size)
    
    def get_cluster_summary(self) -> Dict:
        """í´ëŸ¬ìŠ¤í„°ë§ ìš”ì•½ ì •ë³´"""
        return {
            "algorithm": self.algorithm,
            "total_documents": self.total_documents,
            "n_clusters": self.n_clusters,
            "silhouette_score": self.silhouette_score,
            "cluster_sizes": [cluster.size for cluster in self.clusters],
            "avg_cluster_size": sum(cluster.size for cluster in self.clusters) / len(self.clusters),
            "timestamp": self.timestamp.isoformat()
        }


class ContentClusterer:
    """BGE-M3 ê¸°ë°˜ ì˜ë¯¸ì  ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§"""
    
    def __init__(self, 
                 embedding_engine: SentenceTransformerEngine,
                 embedding_cache: EmbeddingCache,
                 config: dict):
        """
        ContentClusterer ì´ˆê¸°í™”
        
        Args:
            embedding_engine: BGE-M3 ì„ë² ë”© ì—”ì§„
            embedding_cache: ì„ë² ë”© ìºì‹œ
            config: ì„¤ì • ì •ë³´
        """
        if not SKLEARN_AVAILABLE:
            raise ImportError("scikit-learnì´ í•„ìš”í•©ë‹ˆë‹¤: pip install scikit-learn")
            
        self.embedding_engine = embedding_engine
        self.embedding_cache = embedding_cache
        self.config = config
        
        # í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì • ë¡œë“œ
        clustering_config = config.get('clustering', {})
        self.default_algorithm = clustering_config.get('default_algorithm', 'kmeans')
        self.auto_k = clustering_config.get('auto_k', True)
        self.min_cluster_size = clustering_config.get('min_cluster_size', 2)
        self.max_clusters = clustering_config.get('max_clusters', 20)
        self.silhouette_threshold = clustering_config.get('silhouette_threshold', 0.5)
        
        logger.info(f"ContentClusterer ì´ˆê¸°í™” ì™„ë£Œ - ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜: {self.default_algorithm}")
    
    def cluster_documents(self,
                         documents: List[Document],
                         algorithm: str = None,
                         n_clusters: int = None,
                         **kwargs) -> ClusteringResult:
        """
        ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        
        Args:
            documents: í´ëŸ¬ìŠ¤í„°ë§í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            algorithm: í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ('kmeans', 'dbscan', 'agglomerative')
            n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜ (Noneì´ë©´ ìë™ ê²°ì •)
            **kwargs: ì•Œê³ ë¦¬ì¦˜ë³„ ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            ClusteringResult: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        if len(documents) < self.min_cluster_size:
            logger.warning(f"ë¬¸ì„œ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ {self.min_cluster_size}ê°œ í•„ìš”): {len(documents)}ê°œ")
            return self._create_single_cluster(documents)
        
        algorithm = algorithm or self.default_algorithm
        logger.info(f"ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ, {algorithm} ì•Œê³ ë¦¬ì¦˜")
        
        # 1. ì„ë² ë”© ë²¡í„° ì¤€ë¹„
        embeddings = self._get_document_embeddings(documents)
        
        # 2. í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • (í•„ìš”í•œ ê²½ìš°)
        if n_clusters is None and algorithm in ['kmeans', 'agglomerative']:
            n_clusters = self._determine_optimal_clusters(embeddings, documents)
            logger.info(f"ìë™ ê²°ì •ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}")
        
        # 3. í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        cluster_labels = self._perform_clustering(embeddings, algorithm, n_clusters, **kwargs)
        
        # 4. í´ëŸ¬ìŠ¤í„° ìƒì„± ë° ë¶„ì„
        clusters = self._create_clusters(documents, embeddings, cluster_labels)
        
        # 5. ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
        silhouette_avg = self._calculate_silhouette_score(embeddings, cluster_labels)
        
        # 6. ê²°ê³¼ ë°˜í™˜
        result = ClusteringResult(
            clusters=clusters,
            algorithm=algorithm,
            n_clusters=len(clusters),
            silhouette_score=silhouette_avg,
            parameters=kwargs,
            total_documents=len(documents),
            timestamp=datetime.now()
        )
        
        logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°, ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_avg:.3f}")
        return result
    
    def _get_document_embeddings(self, documents: List[Document]) -> np.ndarray:
        """ë¬¸ì„œë“¤ì˜ ì„ë² ë”© ë²¡í„° ê°€ì ¸ì˜¤ê¸°"""
        embeddings = []
        
        logger.info("ë¬¸ì„œ ì„ë² ë”© ë²¡í„° ë¡œë”© ì¤‘...")
        for doc in documents:
            try:
                # ìºì‹œì—ì„œ ì„ë² ë”© ì¡°íšŒ
                cached_embedding = self.embedding_cache.get_embedding(doc.path)
                if cached_embedding:
                    embeddings.append(cached_embedding.embedding)
                else:
                    # ì„ë² ë”©ì´ ì—†ëŠ” ê²½ìš° ìƒì„±
                    logger.warning(f"ì„ë² ë”©ì´ ì—†ëŠ” ë¬¸ì„œ: {doc.path}")
                    embedding = self.embedding_engine.encode([doc.content])[0]
                    embeddings.append(embedding)
                    
            except Exception as e:
                logger.error(f"ì„ë² ë”© ë¡œë”© ì‹¤íŒ¨ ({doc.path}): {e}")
                # ê¸°ë³¸ê°’ìœ¼ë¡œ ì˜ë²¡í„° ì‚¬ìš©
                embeddings.append(np.zeros(self.embedding_engine.model.get_sentence_embedding_dimension()))
        
        return np.array(embeddings)
    
    def _determine_optimal_clusters(self, embeddings: np.ndarray, documents: List[Document]) -> int:
        """ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ìë™ ê²°ì • (Elbow method + Silhouette analysis)"""
        n_docs = len(documents)
        max_k = min(self.max_clusters, n_docs // 2)  # ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì œí•œ
        
        if max_k < 2:
            return 2
        
        logger.info(f"ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • ì¤‘ (2~{max_k})...")
        
        # K-meansë¡œ ì—¬ëŸ¬ kê°’ í…ŒìŠ¤íŠ¸
        silhouette_scores = []
        inertias = []
        k_range = range(2, max_k + 1)
        
        for k in k_range:
            try:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(embeddings)
                
                # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
                silhouette_avg = silhouette_score(embeddings, cluster_labels)
                silhouette_scores.append(silhouette_avg)
                inertias.append(kmeans.inertia_)
                
                logger.debug(f"k={k}: ì‹¤ë£¨ì—£ ì ìˆ˜ {silhouette_avg:.3f}")
                
            except Exception as e:
                logger.warning(f"k={k}ì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
                silhouette_scores.append(0.0)
                inertias.append(float('inf'))
        
        # ìµœê³  ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ ê°€ì§„ k ì„ íƒ
        if silhouette_scores:
            best_k_idx = np.argmax(silhouette_scores)
            best_k = k_range[best_k_idx]
            best_score = silhouette_scores[best_k_idx]
            
            logger.info(f"ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_k} (ì‹¤ë£¨ì—£ ì ìˆ˜: {best_score:.3f})")
            
            # ì„ê³„ê°’ ì´í•˜ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            if best_score < self.silhouette_threshold:
                logger.warning(f"ì‹¤ë£¨ì—£ ì ìˆ˜ê°€ ë‚®ìŒ ({best_score:.3f} < {self.silhouette_threshold}), ê¸°ë³¸ê°’ ì‚¬ìš©")
                return min(5, max_k)  # ê¸°ë³¸ê°’
            
            return best_k
        
        return min(5, max_k)  # ê¸°ë³¸ê°’
    
    def _perform_clustering(self, embeddings: np.ndarray, algorithm: str, n_clusters: int, **kwargs) -> np.ndarray:
        """ì‹¤ì œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        logger.info(f"{algorithm} í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘...")
        
        if algorithm == 'kmeans':
            clusterer = KMeans(
                n_clusters=n_clusters, 
                random_state=42, 
                n_init=10,
                **kwargs
            )
        elif algorithm == 'dbscan':
            eps = kwargs.get('eps', 0.5)
            min_samples = kwargs.get('min_samples', 5)
            clusterer = DBSCAN(eps=eps, min_samples=min_samples, **kwargs)
        elif algorithm == 'agglomerative':
            clusterer = AgglomerativeClustering(n_clusters=n_clusters, **kwargs)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•Œê³ ë¦¬ì¦˜: {algorithm}")
        
        try:
            cluster_labels = clusterer.fit_predict(embeddings)
            return cluster_labels
        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹
            return np.zeros(len(embeddings), dtype=int)
    
    def _create_clusters(self, documents: List[Document], embeddings: np.ndarray, cluster_labels: np.ndarray) -> List[DocumentCluster]:
        """í´ëŸ¬ìŠ¤í„° ê°ì²´ ìƒì„± ë° ë¶„ì„"""
        clusters_dict = defaultdict(list)
        embeddings_dict = defaultdict(list)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë¬¸ì„œì™€ ì„ë² ë”© ê·¸ë£¹í™”
        for doc, embedding, label in zip(documents, embeddings, cluster_labels):
            if label != -1:  # DBSCANì˜ ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ì œì™¸
                clusters_dict[label].append(doc)
                embeddings_dict[label].append(embedding)
        
        clusters = []
        for cluster_id, cluster_docs in clusters_dict.items():
            cluster_embeddings = np.array(embeddings_dict[cluster_id])
            
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ê³„ì‚°
            centroid = np.mean(cluster_embeddings, axis=0)
            
            # ëŒ€í‘œ ë¬¸ì„œ ì„ ì • (ì¤‘ì‹¬ì ì— ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì„œ)
            distances = [np.linalg.norm(emb - centroid) for emb in cluster_embeddings]
            representative_idx = np.argmin(distances)
            representative_doc = cluster_docs[representative_idx]
            
            # í´ëŸ¬ìŠ¤í„° ë‚´ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_score = self._calculate_cluster_similarity(cluster_embeddings)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_cluster_keywords(cluster_docs)
            
            # í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ìƒì„±
            label = self._generate_cluster_label(representative_doc, keywords)
            
            cluster = DocumentCluster(
                id=f"cluster_{cluster_id}",
                label=label,
                documents=cluster_docs,
                representative_doc=representative_doc,
                keywords=keywords,
                similarity_score=similarity_score,
                centroid=centroid
            )
            
            clusters.append(cluster)
        
        # í´ëŸ¬ìŠ¤í„° í¬ê¸°ìˆœ ì •ë ¬
        clusters.sort(key=lambda x: x.size, reverse=True)
        
        return clusters
    
    def _calculate_silhouette_score(self, embeddings: np.ndarray, cluster_labels: np.ndarray) -> float:
        """ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸(-1) ì œê±°
            valid_indices = cluster_labels != -1
            if np.sum(valid_indices) < 2:
                return 0.0
            
            valid_embeddings = embeddings[valid_indices]
            valid_labels = cluster_labels[valid_indices]
            
            # ìœ íš¨í•œ í´ëŸ¬ìŠ¤í„°ê°€ 2ê°œ ì´ìƒì¸ì§€ í™•ì¸
            unique_labels = np.unique(valid_labels)
            if len(unique_labels) < 2:
                return 0.0
            
            return silhouette_score(valid_embeddings, valid_labels)
        except Exception as e:
            logger.warning(f"ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_cluster_similarity(self, cluster_embeddings: np.ndarray) -> float:
        """í´ëŸ¬ìŠ¤í„° ë‚´ ë¬¸ì„œ ê°„ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°"""
        if len(cluster_embeddings) < 2:
            return 1.0
        
        try:
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(cluster_embeddings)
            # ëŒ€ê°ì„ (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„) ì œì™¸í•˜ê³  í‰ê·  ê³„ì‚°
            mask = ~np.eye(similarities.shape[0], dtype=bool)
            return np.mean(similarities[mask])
        except Exception as e:
            logger.warning(f"í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _extract_cluster_keywords(self, documents: List[Document], top_k: int = 5) -> List[str]:
        """í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œ ì¶”ì¶œ (TF-IDF ê¸°ë°˜)"""
        try:
            # ëª¨ë“  ë¬¸ì„œì˜ ì œëª©ê³¼ íƒœê·¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            all_text = []
            for doc in documents:
                text_parts = [doc.title]
                if doc.tags:
                    text_parts.extend(doc.tags)
                all_text.append(" ".join(text_parts))
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
            word_freq = Counter()
            for text in all_text:
                words = text.lower().split()
                # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œ (ê¸¸ì´ 2 ì´ìƒ)
                words = [w for w in words if len(w) >= 2 and not w.startswith('#')]
                word_freq.update(words)
            
            # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
            return [word for word, freq in word_freq.most_common(top_k)]
            
        except Exception as e:
            logger.warning(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _generate_cluster_label(self, representative_doc: Document, keywords: List[str]) -> str:
        """í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ìƒì„±"""
        # ëŒ€í‘œ ë¬¸ì„œ ì œëª© ê¸°ë°˜ + í‚¤ì›Œë“œ
        title_words = representative_doc.title.split()[:3]  # ì œëª©ì˜ ì²˜ìŒ 3ë‹¨ì–´
        
        if keywords:
            # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¡°í•©
            label_parts = title_words + keywords[:2]
        else:
            label_parts = title_words
        
        return " ".join(label_parts)
    
    def _create_single_cluster(self, documents: List[Document]) -> ClusteringResult:
        """ë¬¸ì„œê°€ ë¶€ì¡±í•  ë•Œ ë‹¨ì¼ í´ëŸ¬ìŠ¤í„° ìƒì„±"""
        cluster = DocumentCluster(
            id="cluster_0",
            label="ì „ì²´ ë¬¸ì„œ",
            documents=documents,
            representative_doc=documents[0] if documents else None,
            keywords=[],
            similarity_score=1.0
        )
        
        return ClusteringResult(
            clusters=[cluster],
            algorithm="single",
            n_clusters=1,
            silhouette_score=1.0,
            parameters={},
            total_documents=len(documents),
            timestamp=datetime.now()
        )


def test_content_clusterer():
    """ContentClusterer í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ContentClusterer í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    if not SKLEARN_AVAILABLE:
        print("âŒ scikit-learnì´ í•„ìš”í•©ë‹ˆë‹¤")
        return False
    
    try:
        # í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ìƒì„±
        from datetime import datetime
        test_documents = [
            Document(
                path="test1.md",
                title="TDD ê¸°ì´ˆ",
                content="í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œì˜ ê¸°ë³¸ ì›ì¹™",
                tags=["tdd", "testing"],
                frontmatter={},
                word_count=10,
                char_count=50,
                file_size=100,
                modified_at=datetime.now(),
                file_hash="test1_hash"
            ),
            Document(
                path="test2.md", 
                title="ë¦¬íŒ©í† ë§ ê¸°ë²•",
                content="ì½”ë“œ ê°œì„ ì„ ìœ„í•œ ë¦¬íŒ©í† ë§ ë°©ë²•",
                tags=["refactoring", "clean-code"],
                frontmatter={},
                word_count=12,
                char_count=60,
                file_size=120,
                modified_at=datetime.now(),
                file_hash="test2_hash"
            ),
            Document(
                path="test3.md",
                title="ë‹¨ìœ„ í…ŒìŠ¤íŠ¸",
                content="íš¨ê³¼ì ì¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±ë²•",
                tags=["testing", "unit-test"],
                frontmatter={},
                word_count=8,
                char_count=40,
                file_size=80,
                modified_at=datetime.now(),
                file_hash="test3_hash"
            )
        ]
        
        # Mock ì„¤ì • (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ê°ì²´ ì‚¬ìš©)
        class MockEmbeddingEngine:
            def encode(self, texts):
                # ë”ë¯¸ ì„ë² ë”© (ì‹¤ì œë¡œëŠ” BGE-M3)
                return [np.random.rand(1024) for _ in texts]
            
            @property
            def model(self):
                class MockModel:
                    def get_sentence_embedding_dimension(self):
                        return 1024
                return MockModel()
        
        class MockEmbeddingCache:
            def get_embedding(self, path):
                # ë”ë¯¸ ìºì‹œëœ ì„ë² ë”©
                from ..core.embedding_cache import CachedEmbedding
                return CachedEmbedding(
                    file_path=path,
                    file_hash="dummy",
                    embedding=np.random.rand(1024),
                    model_name="test",
                    embedding_dimension=1024,
                    created_at=datetime.now(),
                    file_size=100
                )
        
        # ContentClusterer ìƒì„±
        config = {
            'clustering': {
                'default_algorithm': 'kmeans',
                'auto_k': True,
                'min_cluster_size': 2,
                'max_clusters': 10,
                'silhouette_threshold': 0.3
            }
        }
        
        clusterer = ContentClusterer(
            embedding_engine=MockEmbeddingEngine(),
            embedding_cache=MockEmbeddingCache(),
            config=config
        )
        
        # í´ëŸ¬ìŠ¤í„°ë§ í…ŒìŠ¤íŠ¸
        result = clusterer.cluster_documents(test_documents, n_clusters=2)
        
        # ê²°ê³¼ ê²€ì¦
        assert len(result.clusters) > 0, "í´ëŸ¬ìŠ¤í„°ê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ"
        assert result.total_documents == len(test_documents), "ë¬¸ì„œ ìˆ˜ ë¶ˆì¼ì¹˜"
        assert result.silhouette_score >= 0, "ì‹¤ë£¨ì—£ ì ìˆ˜ ì˜¤ë¥˜"
        
        print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì„±ê³µ: {result.n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
        print(f"   ì‹¤ë£¨ì—£ ì ìˆ˜: {result.silhouette_score:.3f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ContentClusterer í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_content_clusterer()