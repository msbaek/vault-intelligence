#!/usr/bin/env python3
"""
Duplicate Document Detector for Vault Intelligence System V2

Sentence Transformers ì„ë² ë”©ì„ í™œìš©í•œ ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ ë° ë¶„ì„
"""

import logging
from typing import List, Dict, Optional, Tuple, Set
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
import numpy as np
from collections import defaultdict
import json

try:
    from sklearn.cluster import DBSCAN
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

from ..core.sentence_transformer_engine import SentenceTransformerEngine
from ..core.vault_processor import Document

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class DuplicateGroup:
    """ì¤‘ë³µ ë¬¸ì„œ ê·¸ë£¹"""
    id: str
    documents: List[Document]
    similarity_scores: List[List[float]]  # ê·¸ë£¹ ë‚´ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤
    master_document: Optional[Document] = None
    average_similarity: float = 0.0
    created_at: Optional[datetime] = None
    
    def get_document_count(self) -> int:
        return len(self.documents)
    
    def get_total_word_count(self) -> int:
        return sum(doc.word_count for doc in self.documents)
    
    def get_paths(self) -> List[str]:
        return [doc.path for doc in self.documents]


@dataclass
class DuplicateAnalysis:
    """ì¤‘ë³µ ë¶„ì„ ê²°ê³¼"""
    total_documents: int
    duplicate_groups: List[DuplicateGroup]
    duplicate_count: int
    unique_count: int
    similarity_threshold: float
    analysis_date: datetime
    potential_savings_mb: float = 0.0
    
    def get_group_count(self) -> int:
        return len(self.duplicate_groups)
    
    def get_duplicate_ratio(self) -> float:
        if self.total_documents == 0:
            return 0.0
        return self.duplicate_count / self.total_documents


class DuplicateDetector:
    """ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ê¸°"""
    
    def __init__(
        self,
        search_engine,
        config: Dict = None
    ):
        """
        Args:
            search_engine: AdvancedSearchEngine ì¸ìŠ¤í„´ìŠ¤
            config: ì¤‘ë³µ ê°ì§€ ì„¤ì •
        """
        self.search_engine = search_engine
        self.config = config or {}
        
        # ê¸°ë³¸ ì„¤ì •
        self.similarity_threshold = self.config.get('duplicates', {}).get(
            'similarity_threshold', 0.85
        )
        self.min_word_count = self.config.get('duplicates', {}).get(
            'min_word_count', 50
        )
        self.group_threshold = self.config.get('duplicates', {}).get(
            'group_threshold', 0.9
        )
        
        logger.info(f"ì¤‘ë³µ ê°ì§€ê¸° ì´ˆê¸°í™” - ì„ê³„ê°’: {self.similarity_threshold}")
    
    def find_duplicates(
        self,
        threshold: Optional[float] = None,
        min_word_count: Optional[int] = None
    ) -> DuplicateAnalysis:
        """ì¤‘ë³µ ë¬¸ì„œ ê°ì§€"""
        try:
            # ì„¤ì •ê°’ ì‚¬ìš©
            threshold = threshold or self.similarity_threshold
            min_word_count = min_word_count or self.min_word_count
            
            if not self.search_engine.indexed:
                logger.warning("ê²€ìƒ‰ ì—”ì§„ì´ ì¸ë±ì‹±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return self._create_empty_analysis(threshold)
            
            logger.info(f"ì¤‘ë³µ ê°ì§€ ì‹œì‘ - ì„ê³„ê°’: {threshold}")
            
            # í•„í„°ë§ëœ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
            documents = self._filter_documents(min_word_count)
            logger.info(f"ë¶„ì„ ëŒ€ìƒ ë¬¸ì„œ: {len(documents)}ê°œ")
            
            if len(documents) < 2:
                logger.info("ë¶„ì„í•  ë¬¸ì„œê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return self._create_empty_analysis(threshold)
            
            # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            similarity_matrix = self._calculate_similarity_matrix(documents)
            
            # ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°
            duplicate_groups = self._find_duplicate_groups(
                documents, similarity_matrix, threshold
            )
            
            logger.info(f"ë°œê²¬ëœ ì¤‘ë³µ ê·¸ë£¹: {len(duplicate_groups)}ê°œ")
            
            # ë¶„ì„ ê²°ê³¼ ìƒì„±
            analysis = self._create_analysis(
                documents, duplicate_groups, threshold
            )
            
            logger.info(f"ì¤‘ë³µ ë¶„ì„ ì™„ë£Œ - ì¤‘ë³µë¥ : {analysis.get_duplicate_ratio():.1%}")
            return analysis
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ê°ì§€ ì‹¤íŒ¨: {e}")
            return self._create_empty_analysis(threshold or self.similarity_threshold)
    
    def _filter_documents(self, min_word_count: int) -> List[Document]:
        """ë¬¸ì„œ í•„í„°ë§"""
        filtered = []
        
        for doc in self.search_engine.documents:
            # ìµœì†Œ ë‹¨ì–´ ìˆ˜ í™•ì¸
            if doc.word_count < min_word_count:
                continue
            
            # ì„ë² ë”© ì¡´ì¬ í™•ì¸
            if doc.embedding is None or np.allclose(doc.embedding, 0):
                continue
            
            filtered.append(doc)
        
        return filtered
    
    def _calculate_similarity_matrix(self, documents: List[Document]) -> np.ndarray:
        """ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        try:
            embeddings = np.array([doc.embedding for doc in documents])
            
            if SKLEARN_AVAILABLE:
                # sklearn ì‚¬ìš©
                similarity_matrix = cosine_similarity(embeddings)
            else:
                # ìˆ˜ë™ ê³„ì‚°
                n = len(embeddings)
                similarity_matrix = np.zeros((n, n))
                
                for i in range(n):
                    for j in range(i, n):
                        if i == j:
                            similarity_matrix[i, j] = 1.0
                        else:
                            sim = self.search_engine.engine.calculate_similarity(
                                embeddings[i], embeddings[j]
                            )
                            similarity_matrix[i, j] = sim
                            similarity_matrix[j, i] = sim
            
            return similarity_matrix
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.zeros((len(documents), len(documents)))
    
    def _find_duplicate_groups(
        self,
        documents: List[Document],
        similarity_matrix: np.ndarray,
        threshold: float
    ) -> List[DuplicateGroup]:
        """ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°"""
        try:
            n = len(documents)
            visited = set()
            groups = []
            
            for i in range(n):
                if i in visited:
                    continue
                
                # í˜„ì¬ ë¬¸ì„œì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ ì°¾ê¸°
                similar_indices = []
                for j in range(n):
                    if i != j and similarity_matrix[i, j] >= threshold:
                        similar_indices.append(j)
                
                if similar_indices:
                    # ê·¸ë£¹ ìƒì„±
                    group_indices = [i] + similar_indices
                    group_docs = [documents[idx] for idx in group_indices]
                    
                    # ê·¸ë£¹ ë‚´ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì¶”ì¶œ
                    group_similarity = similarity_matrix[np.ix_(group_indices, group_indices)]
                    
                    # ë§ˆìŠ¤í„° ë¬¸ì„œ ì„ ì •
                    master_doc = self._select_master_document(group_docs)
                    
                    # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
                    avg_sim = self._calculate_average_similarity(group_similarity)
                    
                    group = DuplicateGroup(
                        id=f"group_{len(groups) + 1}",
                        documents=group_docs,
                        similarity_scores=group_similarity.tolist(),
                        master_document=master_doc,
                        average_similarity=avg_sim,
                        created_at=datetime.now()
                    )
                    
                    groups.append(group)
                    
                    # ë°©ë¬¸ í‘œì‹œ
                    for idx in group_indices:
                        visited.add(idx)
            
            return groups
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    def _select_master_document(self, documents: List[Document]) -> Document:
        """ë§ˆìŠ¤í„° ë¬¸ì„œ ì„ ì • (ê°€ì¥ ìµœì‹ ì´ë©´ì„œ ë‚´ìš©ì´ ë§ì€ ë¬¸ì„œ)"""
        try:
            # ìš°ì„ ìˆœìœ„: 1) ë‹¨ì–´ ìˆ˜, 2) ìˆ˜ì • ë‚ ì§œ
            return max(documents, key=lambda doc: (doc.word_count, doc.modified_at))
        except Exception as e:
            logger.error(f"ë§ˆìŠ¤í„° ë¬¸ì„œ ì„ ì • ì‹¤íŒ¨: {e}")
            return documents[0] if documents else None
    
    def _calculate_average_similarity(self, similarity_matrix: np.ndarray) -> float:
        """ê·¸ë£¹ ë‚´ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            # ëŒ€ê°ì„  ì œì™¸ (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ 1.0 ì œì™¸)
            n = similarity_matrix.shape[0]
            if n <= 1:
                return 1.0
            
            total_sum = 0.0
            count = 0
            
            for i in range(n):
                for j in range(i + 1, n):
                    total_sum += similarity_matrix[i, j]
                    count += 1
            
            return total_sum / count if count > 0 else 0.0
            
        except Exception as e:
            logger.error(f"í‰ê·  ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _create_analysis(
        self,
        documents: List[Document],
        duplicate_groups: List[DuplicateGroup],
        threshold: float
    ) -> DuplicateAnalysis:
        """ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        try:
            # ì¤‘ë³µ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
            duplicate_docs = set()
            for group in duplicate_groups:
                for doc in group.documents:
                    duplicate_docs.add(doc.path)
            
            duplicate_count = len(duplicate_docs)
            unique_count = len(documents) - duplicate_count
            
            # ì ì¬ì  ì ˆì•½ ê³µê°„ ê³„ì‚° (ì¤‘ë³µ ë¬¸ì„œë“¤ì˜ íŒŒì¼ í¬ê¸° í•©)
            potential_savings = 0
            for group in duplicate_groups:
                # ë§ˆìŠ¤í„° ë¬¸ì„œ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ë¬¸ì„œë“¤ì˜ í¬ê¸° í•©
                if group.master_document:
                    for doc in group.documents:
                        if doc.path != group.master_document.path:
                            potential_savings += doc.file_size
            
            potential_savings_mb = potential_savings / (1024 * 1024)
            
            return DuplicateAnalysis(
                total_documents=len(documents),
                duplicate_groups=duplicate_groups,
                duplicate_count=duplicate_count,
                unique_count=unique_count,
                similarity_threshold=threshold,
                analysis_date=datetime.now(),
                potential_savings_mb=potential_savings_mb
            )
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_empty_analysis(threshold)
    
    def _create_empty_analysis(self, threshold: float) -> DuplicateAnalysis:
        """ë¹ˆ ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        return DuplicateAnalysis(
            total_documents=0,
            duplicate_groups=[],
            duplicate_count=0,
            unique_count=0,
            similarity_threshold=threshold,
            analysis_date=datetime.now()
        )
    
    def find_near_duplicates(
        self,
        document_path: str,
        threshold: float = 0.8,
        top_k: int = 5
    ) -> List[Tuple[Document, float]]:
        """íŠ¹ì • ë¬¸ì„œì˜ ìœ ì‚¬ ë¬¸ì„œ ì°¾ê¸°"""
        try:
            if not self.search_engine.indexed:
                logger.warning("ê²€ìƒ‰ ì—”ì§„ì´ ì¸ë±ì‹±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            # ëŒ€ìƒ ë¬¸ì„œ ì°¾ê¸°
            target_doc = None
            target_idx = None
            
            for i, doc in enumerate(self.search_engine.documents):
                if doc.path == document_path:
                    target_doc = doc
                    target_idx = i
                    break
            
            if target_doc is None:
                logger.warning(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_path}")
                return []
            
            # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
            query_embedding = target_doc.embedding
            if query_embedding is None:
                logger.warning(f"ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤: {document_path}")
                return []
            
            results = self.search_engine.engine.find_most_similar(
                query_embedding,
                self.search_engine.embeddings,
                top_k=top_k + 1  # ìê¸° ìì‹  ì œì™¸
            )
            
            # ìê¸° ìì‹  ì œì™¸í•˜ê³  ì„ê³„ê°’ ì´ìƒë§Œ ë°˜í™˜
            similar_docs = []
            for idx, similarity in results:
                if idx != target_idx and similarity >= threshold:
                    similar_docs.append((self.search_engine.documents[idx], similarity))
            
            return similar_docs[:top_k]
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ ë¬¸ì„œ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    def generate_merge_suggestions(
        self, 
        duplicate_group: DuplicateGroup
    ) -> Dict:
        """ì¤‘ë³µ ê·¸ë£¹ì— ëŒ€í•œ ë³‘í•© ì œì•ˆ"""
        try:
            if not duplicate_group.master_document:
                return {"error": "ë§ˆìŠ¤í„° ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤"}
            
            master = duplicate_group.master_document
            duplicates = [doc for doc in duplicate_group.documents if doc.path != master.path]
            
            suggestions = {
                "master_document": {
                    "path": master.path,
                    "title": master.title,
                    "word_count": master.word_count,
                    "modified_at": master.modified_at.isoformat(),
                    "reason": "ê°€ì¥ ë§ì€ ë‚´ìš©ê³¼ ìµœì‹  ìˆ˜ì • ë‚ ì§œ"
                },
                "duplicates": [],
                "merge_strategy": "content_comparison",
                "potential_savings_mb": 0.0
            }
            
            total_size = 0
            for doc in duplicates:
                suggestions["duplicates"].append({
                    "path": doc.path,
                    "title": doc.title,
                    "word_count": doc.word_count,
                    "file_size_mb": doc.file_size / (1024 * 1024),
                    "similarity_to_master": self._get_similarity_to_master(
                        duplicate_group, doc, master
                    )
                })
                total_size += doc.file_size
            
            suggestions["potential_savings_mb"] = total_size / (1024 * 1024)
            
            # ë³‘í•© ì „ëµ ì œì•ˆ
            if len(duplicates) == 1:
                suggestions["merge_strategy"] = "simple_replacement"
            else:
                suggestions["merge_strategy"] = "content_consolidation"
            
            return suggestions
            
        except Exception as e:
            logger.error(f"ë³‘í•© ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _get_similarity_to_master(
        self,
        group: DuplicateGroup,
        doc: Document,
        master: Document
    ) -> float:
        """ë§ˆìŠ¤í„° ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ì¡°íšŒ"""
        try:
            doc_idx = group.documents.index(doc)
            master_idx = group.documents.index(master)
            return group.similarity_scores[doc_idx][master_idx]
        except (ValueError, IndexError):
            return 0.0
    
    def export_analysis(self, analysis: DuplicateAnalysis, output_file: str) -> bool:
        """ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            export_data = {
                "analysis_info": {
                    "total_documents": analysis.total_documents,
                    "duplicate_count": analysis.duplicate_count,
                    "unique_count": analysis.unique_count,
                    "duplicate_ratio": analysis.get_duplicate_ratio(),
                    "similarity_threshold": analysis.similarity_threshold,
                    "analysis_date": analysis.analysis_date.isoformat(),
                    "potential_savings_mb": analysis.potential_savings_mb
                },
                "duplicate_groups": []
            }
            
            for group in analysis.duplicate_groups:
                group_data = {
                    "id": group.id,
                    "document_count": group.get_document_count(),
                    "average_similarity": group.average_similarity,
                    "total_word_count": group.get_total_word_count(),
                    "master_document": group.master_document.path if group.master_document else None,
                    "documents": [
                        {
                            "path": doc.path,
                            "title": doc.title,
                            "word_count": doc.word_count,
                            "file_size_bytes": doc.file_size,
                            "modified_at": doc.modified_at.isoformat()
                        }
                        for doc in group.documents
                    ],
                    "merge_suggestions": self.generate_merge_suggestions(group)
                }
                export_data["duplicate_groups"].append(group_data)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False


def test_duplicate_detector():
    """ì¤‘ë³µ ê°ì§€ê¸° í…ŒìŠ¤íŠ¸"""
    import tempfile
    import shutil
    from ..features.advanced_search import AdvancedSearchEngine
    
    try:
        # ì„ì‹œ vault ë° ìºì‹œ ìƒì„±
        temp_vault = tempfile.mkdtemp()
        temp_cache = tempfile.mkdtemp()
        
        # ìœ ì‚¬í•œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤ ìƒì„±
        test_docs = [
            ("tdd1.md", "# TDD ê¸°ë³¸ ê°œë…\n\nTDDëŠ” í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.\nRed-Green-Refactor ì‚¬ì´í´ì„ ë”°ë¦…ë‹ˆë‹¤."),
            ("tdd2.md", "# TDD ê°œë…\n\nTDDëŠ” í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œì…ë‹ˆë‹¤.\nRed-Green-Refactor ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤."),  # ìœ ì‚¬
            ("refactoring.md", "# ë¦¬íŒ©í† ë§\n\nì½”ë“œì˜ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\ní…ŒìŠ¤íŠ¸ê°€ ìˆì–´ì•¼ ì•ˆì „í•©ë‹ˆë‹¤."),
            ("clean_code.md", "# Clean Code\n\nê¹¨ë—í•œ ì½”ë“œ ì‘ì„± ë°©ë²•ë¡ ì…ë‹ˆë‹¤.\nì½ê¸° ì‰¬ìš´ ì½”ë“œë¥¼ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤."),
            ("duplicate_clean.md", "# ê¹¨ë—í•œ ì½”ë“œ\n\nClean CodeëŠ” ì½ê¸° ì‰¬ìš´ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\nì´í•´í•˜ê¸° ì‰¬ìš´ ì½”ë“œê°€ ëª©í‘œì…ë‹ˆë‹¤.")  # ìœ ì‚¬
        ]
        
        for filename, content in test_docs:
            with open(Path(temp_vault) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ë° ì¸ë±ì‹±
        config = {
            "model": {"name": "paraphrase-multilingual-mpnet-base-v2"},
            "vault": {"excluded_dirs": [], "file_extensions": [".md"]},
            "duplicates": {"similarity_threshold": 0.7, "min_word_count": 10}
        }
        
        search_engine = AdvancedSearchEngine(temp_vault, temp_cache, config)
        print("ê²€ìƒ‰ ì—”ì§„ ì¸ë±ì‹± ì¤‘...")
        if not search_engine.build_index():
            print("âŒ ì¸ë±ì‹± ì‹¤íŒ¨")
            return False
        
        # ì¤‘ë³µ ê°ì§€ê¸° ì´ˆê¸°í™”
        detector = DuplicateDetector(search_engine, config)
        
        # ì¤‘ë³µ ê°ì§€ ì‹¤í–‰
        print("\nğŸ” ì¤‘ë³µ ë¬¸ì„œ ê°ì§€ ì¤‘...")
        analysis = detector.find_duplicates(threshold=0.7)
        
        print(f"\nğŸ“Š ì¤‘ë³µ ë¶„ì„ ê²°ê³¼:")
        print(f"- ì´ ë¬¸ì„œ: {analysis.total_documents}ê°œ")
        print(f"- ì¤‘ë³µ ê·¸ë£¹: {analysis.get_group_count()}ê°œ")
        print(f"- ì¤‘ë³µ ë¬¸ì„œ: {analysis.duplicate_count}ê°œ")
        print(f"- ì¤‘ë³µë¥ : {analysis.get_duplicate_ratio():.1%}")
        print(f"- ì ì¬ì  ì ˆì•½: {analysis.potential_savings_mb:.2f}MB")
        
        # ì¤‘ë³µ ê·¸ë£¹ ì„¸ë¶€ ì •ë³´
        for i, group in enumerate(analysis.duplicate_groups):
            print(f"\nğŸ”— ì¤‘ë³µ ê·¸ë£¹ {i+1} (í‰ê·  ìœ ì‚¬ë„: {group.average_similarity:.3f}):")
            print(f"  ë§ˆìŠ¤í„°: {group.master_document.path if group.master_document else 'None'}")
            for doc in group.documents:
                print(f"  - {doc.path} ({doc.word_count} ë‹¨ì–´)")
        
        # íŠ¹ì • ë¬¸ì„œì˜ ìœ ì‚¬ ë¬¸ì„œ ì°¾ê¸° í…ŒìŠ¤íŠ¸
        print(f"\nğŸ” 'tdd1.md'ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ:")
        similar_docs = detector.find_near_duplicates("tdd1.md", threshold=0.5, top_k=3)
        for doc, similarity in similar_docs:
            print(f"  - {doc.path} (ìœ ì‚¬ë„: {similarity:.3f})")
        
        # ê²°ê³¼ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
        output_file = Path(temp_cache) / "duplicate_analysis.json"
        if detector.export_analysis(analysis, str(output_file)):
            print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ì •ë¦¬
        shutil.rmtree(temp_vault)
        shutil.rmtree(temp_cache)
        
        print("âœ… ì¤‘ë³µ ê°ì§€ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì¤‘ë³µ ê°ì§€ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_duplicate_detector()