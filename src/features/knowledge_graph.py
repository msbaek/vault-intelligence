#!/usr/bin/env python3
"""
Knowledge Graph Builder for Vault Intelligence System V2

ë¬¸ì„œ ê°„ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ì§€ì‹ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ê³  ì‹œê°í™”
"""

import os
import json
import logging
from typing import List, Dict, Optional, Tuple, Set, Any
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
import numpy as np
from collections import defaultdict, Counter

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    
try:
    import matplotlib.pyplot as plt
    import matplotlib.patches as mpatches
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

from ..core.vault_processor import Document

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class GraphNode:
    """ê·¸ë˜í”„ ë…¸ë“œ (ë¬¸ì„œ)"""
    id: str
    title: str
    path: str
    tags: List[str]
    word_count: int
    centrality_score: Optional[float] = None
    cluster_id: Optional[str] = None
    node_type: str = "document"  # document, topic, tag
    
    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass 
class GraphEdge:
    """ê·¸ë˜í”„ ì—£ì§€ (ê´€ê³„)"""
    source: str
    target: str
    weight: float
    relationship_type: str  # similarity, tag, reference
    metadata: Optional[Dict] = None
    
    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class KnowledgeGraph:
    """ì§€ì‹ ê·¸ë˜í”„"""
    nodes: List[GraphNode]
    edges: List[GraphEdge]
    communities: Optional[List[List[str]]] = None
    centrality_scores: Optional[Dict[str, float]] = None
    graph_metrics: Optional[Dict] = None
    created_at: Optional[datetime] = None
    
    def get_node_count(self) -> int:
        return len(self.nodes)
    
    def get_edge_count(self) -> int:
        return len(self.edges)
    
    def get_density(self) -> float:
        """ê·¸ë˜í”„ ë°€ë„ ê³„ì‚°"""
        n = len(self.nodes)
        if n < 2:
            return 0.0
        max_edges = n * (n - 1) / 2
        return len(self.edges) / max_edges if max_edges > 0 else 0.0


class KnowledgeGraphBuilder:
    """ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•ê¸°"""
    
    def __init__(
        self,
        search_engine,
        config: Dict = None
    ):
        """
        Args:
            search_engine: AdvancedSearchEngine ì¸ìŠ¤í„´ìŠ¤
            config: ê·¸ë˜í”„ êµ¬ì¶• ì„¤ì •
        """
        self.search_engine = search_engine
        self.config = config or {}
        
        # ê¸°ë³¸ ì„¤ì •
        self.similarity_threshold = self.config.get('graph', {}).get(
            'similarity_threshold', 0.4
        )
        self.max_edges_per_node = self.config.get('graph', {}).get(
            'max_edges_per_node', 10
        )
        self.include_tag_nodes = self.config.get('graph', {}).get(
            'include_tag_nodes', True
        )
        self.min_word_count = self.config.get('graph', {}).get(
            'min_word_count', 50
        )
        
        logger.info(f"ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•ê¸° ì´ˆê¸°í™” - ì„ê³„ê°’: {self.similarity_threshold}")
    
    def build_graph(
        self,
        documents: Optional[List[Document]] = None,
        similarity_threshold: Optional[float] = None,
        include_tag_nodes: Optional[bool] = None
    ) -> KnowledgeGraph:
        """ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•"""
        try:
            if not self.search_engine.indexed:
                logger.warning("ê²€ìƒ‰ ì—”ì§„ì´ ì¸ë±ì‹±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return self._create_empty_graph()
            
            # ì„¤ì •ê°’ ì‚¬ìš©
            similarity_threshold = similarity_threshold or self.similarity_threshold
            include_tag_nodes = include_tag_nodes if include_tag_nodes is not None else self.include_tag_nodes
            
            # ë¬¸ì„œ í•„í„°ë§
            if documents is None:
                documents = self._filter_documents(self.search_engine.documents)
            else:
                documents = self._filter_documents(documents)
            
            if len(documents) < 2:
                logger.warning("ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ê¸°ì— ë¬¸ì„œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                return self._create_empty_graph()
            
            logger.info(f"ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì‹œì‘ - {len(documents)}ê°œ ë¬¸ì„œ")
            
            # ë…¸ë“œ ìƒì„±
            nodes = self._create_document_nodes(documents)
            
            # íƒœê·¸ ë…¸ë“œ ì¶”ê°€
            if include_tag_nodes:
                tag_nodes = self._create_tag_nodes(documents)
                nodes.extend(tag_nodes)
            
            # ì—£ì§€ ìƒì„±
            edges = []
            
            # ìœ ì‚¬ë„ ê¸°ë°˜ ì—£ì§€
            similarity_edges = self._create_similarity_edges(documents, similarity_threshold)
            edges.extend(similarity_edges)
            
            # íƒœê·¸ ê¸°ë°˜ ì—£ì§€
            if include_tag_nodes:
                tag_edges = self._create_tag_edges(documents, tag_nodes)
                edges.extend(tag_edges)
            
            # ì°¸ì¡° ê¸°ë°˜ ì—£ì§€ (Obsidian ë§í¬)
            reference_edges = self._create_reference_edges(documents)
            edges.extend(reference_edges)
            
            logger.info(f"ê·¸ë˜í”„ êµ¬ì¶• ì™„ë£Œ - ë…¸ë“œ: {len(nodes)}ê°œ, ì—£ì§€: {len(edges)}ê°œ")
            
            # ê·¸ë˜í”„ ë¶„ì„
            graph_metrics = self._analyze_graph(nodes, edges)
            centrality_scores = self._calculate_centrality(nodes, edges)
            communities = self._detect_communities(nodes, edges)
            
            # ì§€ì‹ ê·¸ë˜í”„ ìƒì„±
            knowledge_graph = KnowledgeGraph(
                nodes=nodes,
                edges=edges,
                communities=communities,
                centrality_scores=centrality_scores,
                graph_metrics=graph_metrics,
                created_at=datetime.now()
            )
            
            return knowledge_graph
            
        except Exception as e:
            logger.error(f"ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            return self._create_empty_graph()
    
    def _filter_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œ í•„í„°ë§"""
        filtered = []
        
        for doc in documents:
            logger.info(f"ë¬¸ì„œ ì²´í¬: {doc.title} - ë‹¨ì–´ìˆ˜: {doc.word_count}, ì„ë² ë”©: {doc.embedding is not None}")
            
            # ìµœì†Œ ë‹¨ì–´ ìˆ˜ í™•ì¸
            if doc.word_count < self.min_word_count:
                logger.info(f"  -> ë‹¨ì–´ìˆ˜ ë¶€ì¡± (ìµœì†Œ: {self.min_word_count})")
                continue
            
            # ì„ë² ë”© ì¡´ì¬ í™•ì¸
            if doc.embedding is None:
                logger.info(f"  -> ì„ë² ë”© ì—†ìŒ")
                continue
            
            if np.allclose(doc.embedding, 0):
                logger.info(f"  -> ì„ë² ë”©ì´ ëª¨ë‘ 0")
                continue
            
            filtered.append(doc)
            logger.info(f"  -> í•„í„°ë§ í†µê³¼")
        
        logger.info(f"í•„í„°ë§ëœ ë¬¸ì„œ: {len(filtered)}ê°œ (ì›ë³¸: {len(documents)}ê°œ)")
        return filtered
    
    def _create_document_nodes(self, documents: List[Document]) -> List[GraphNode]:
        """ë¬¸ì„œ ë…¸ë“œ ìƒì„±"""
        nodes = []
        
        for doc in documents:
            node = GraphNode(
                id=f"doc_{len(nodes)}",
                title=doc.title,
                path=doc.path,
                tags=doc.tags,
                word_count=doc.word_count,
                node_type="document"
            )
            nodes.append(node)
        
        return nodes
    
    def _create_tag_nodes(self, documents: List[Document]) -> List[GraphNode]:
        """íƒœê·¸ ë…¸ë“œ ìƒì„±"""
        tag_counter = Counter()
        
        # íƒœê·¸ ë¹ˆë„ ê³„ì‚°
        for doc in documents:
            if doc.tags:
                tag_counter.update(doc.tags)
        
        # ë¹ˆë„ 2 ì´ìƒì¸ íƒœê·¸ë§Œ ë…¸ë“œë¡œ ìƒì„±
        tag_nodes = []
        for tag, count in tag_counter.items():
            if count >= 2:
                node = GraphNode(
                    id=f"tag_{tag.replace('/', '_')}",
                    title=f"#{tag}",
                    path=f"tag:{tag}",
                    tags=[],
                    word_count=count,  # íƒœê·¸ì˜ ê²½ìš° ì–¸ê¸‰ íšŸìˆ˜
                    node_type="tag"
                )
                tag_nodes.append(node)
        
        logger.info(f"íƒœê·¸ ë…¸ë“œ ìƒì„±: {len(tag_nodes)}ê°œ")
        return tag_nodes
    
    def _create_similarity_edges(
        self, 
        documents: List[Document], 
        threshold: float
    ) -> List[GraphEdge]:
        """ìœ ì‚¬ë„ ê¸°ë°˜ ì—£ì§€ ìƒì„±"""
        edges = []
        
        try:
            # ì„ë² ë”© ì¶”ì¶œ
            embeddings = np.array([doc.embedding for doc in documents])
            
            # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            similarities = np.dot(embeddings, embeddings.T)
            
            for i in range(len(documents)):
                # í˜„ì¬ ë¬¸ì„œì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ ì°¾ê¸°
                similar_indices = []
                for j in range(len(documents)):
                    if i != j and similarities[i, j] >= threshold:
                        similar_indices.append((j, similarities[i, j]))
                
                # ìƒìœ„ kê°œë§Œ ì„ íƒ (ë„ˆë¬´ ë§ì€ ì—£ì§€ ë°©ì§€)
                similar_indices.sort(key=lambda x: x[1], reverse=True)
                similar_indices = similar_indices[:self.max_edges_per_node]
                
                for j, similarity in similar_indices:
                    edge = GraphEdge(
                        source=f"doc_{i}",
                        target=f"doc_{j}",
                        weight=float(similarity),
                        relationship_type="similarity",
                        metadata={
                            "source_title": documents[i].title,
                            "target_title": documents[j].title,
                            "similarity_score": float(similarity)
                        }
                    )
                    edges.append(edge)
            
            logger.info(f"ìœ ì‚¬ë„ ì—£ì§€ ìƒì„±: {len(edges)}ê°œ")
            return edges
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ ì—£ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _create_tag_edges(
        self, 
        documents: List[Document], 
        tag_nodes: List[GraphNode]
    ) -> List[GraphEdge]:
        """íƒœê·¸ ê¸°ë°˜ ì—£ì§€ ìƒì„±"""
        edges = []
        
        try:
            # íƒœê·¸ ID ë§¤í•‘
            tag_id_map = {node.title[1:]: node.id for node in tag_nodes}  # '#' ì œê±°
            
            for i, doc in enumerate(documents):
                if doc.tags:
                    for tag in doc.tags:
                        if tag in tag_id_map:
                            edge = GraphEdge(
                                source=f"doc_{i}",
                                target=tag_id_map[tag],
                                weight=1.0,
                                relationship_type="tag",
                                metadata={
                                    "document_title": doc.title,
                                    "tag": tag
                                }
                            )
                            edges.append(edge)
            
            logger.info(f"íƒœê·¸ ì—£ì§€ ìƒì„±: {len(edges)}ê°œ")
            return edges
            
        except Exception as e:
            logger.error(f"íƒœê·¸ ì—£ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _create_reference_edges(self, documents: List[Document]) -> List[GraphEdge]:
        """ì°¸ì¡° ê¸°ë°˜ ì—£ì§€ ìƒì„± (Obsidian ë‚´ë¶€ ë§í¬)"""
        edges = []
        
        try:
            # ë¬¸ì„œ ê²½ë¡œ to ì¸ë±ìŠ¤ ë§¤í•‘
            path_to_index = {doc.path: i for i, doc in enumerate(documents)}
            
            for i, doc in enumerate(documents):
                # ë‚´ë¶€ ë§í¬ íŒ¨í„´ ì°¾ê¸° [[ë¬¸ì„œëª…]]
                import re
                
                link_pattern = r'\[\[([^\]]+)\]\]'
                matches = re.findall(link_pattern, doc.content)
                
                for link in matches:
                    # ë§í¬ë¥¼ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€í™˜
                    possible_paths = [
                        f"{link}.md",
                        link,
                        f"{link}.markdown"
                    ]
                    
                    target_index = None
                    for path in possible_paths:
                        if path in path_to_index:
                            target_index = path_to_index[path]
                            break
                    
                    if target_index is not None and target_index != i:
                        edge = GraphEdge(
                            source=f"doc_{i}",
                            target=f"doc_{target_index}",
                            weight=2.0,  # ëª…ì‹œì  ì°¸ì¡°ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜
                            relationship_type="reference",
                            metadata={
                                "source_title": doc.title,
                                "target_title": documents[target_index].title,
                                "link_text": link
                            }
                        )
                        edges.append(edge)
            
            logger.info(f"ì°¸ì¡° ì—£ì§€ ìƒì„±: {len(edges)}ê°œ")
            return edges
            
        except Exception as e:
            logger.error(f"ì°¸ì¡° ì—£ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _analyze_graph(self, nodes: List[GraphNode], edges: List[GraphEdge]) -> Dict:
        """ê·¸ë˜í”„ ë¶„ì„"""
        try:
            if not NETWORKX_AVAILABLE:
                logger.warning("NetworkXê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê·¸ë˜í”„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return {}
            
            # NetworkX ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()
            
            # ë…¸ë“œ ì¶”ê°€
            for node in nodes:
                G.add_node(node.id, **node.to_dict())
            
            # ì—£ì§€ ì¶”ê°€
            for edge in edges:
                G.add_edge(edge.source, edge.target, weight=edge.weight)
            
            # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = {
                "node_count": G.number_of_nodes(),
                "edge_count": G.number_of_edges(),
                "density": nx.density(G),
                "is_connected": nx.is_connected(G),
                "number_of_components": nx.number_connected_components(G),
                "average_clustering": nx.average_clustering(G),
                "transitivity": nx.transitivity(G)
            }
            
            # ì—°ê²°ëœ ê·¸ë˜í”„ì¸ ê²½ìš° ì¶”ê°€ ë©”íŠ¸ë¦­
            if nx.is_connected(G):
                metrics["diameter"] = nx.diameter(G)
                metrics["average_shortest_path_length"] = nx.average_shortest_path_length(G)
            
            return metrics
            
        except Exception as e:
            logger.error(f"ê·¸ë˜í”„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_centrality(
        self, 
        nodes: List[GraphNode], 
        edges: List[GraphEdge]
    ) -> Dict[str, float]:
        """ì¤‘ì‹¬ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            if not NETWORKX_AVAILABLE:
                return {}
            
            # NetworkX ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()
            for node in nodes:
                G.add_node(node.id)
            for edge in edges:
                G.add_edge(edge.source, edge.target, weight=edge.weight)
            
            # ë‹¤ì–‘í•œ ì¤‘ì‹¬ì„± ê³„ì‚°
            centrality_scores = {}
            
            # ì°¨ìˆ˜ ì¤‘ì‹¬ì„±
            degree_centrality = nx.degree_centrality(G)
            
            # ê·¼ì ‘ ì¤‘ì‹¬ì„± (ì—°ê²°ëœ ê·¸ë˜í”„ì¸ ê²½ìš°ë§Œ)
            if nx.is_connected(G):
                closeness_centrality = nx.closeness_centrality(G)
                betweenness_centrality = nx.betweenness_centrality(G)
            else:
                closeness_centrality = {}
                betweenness_centrality = {}
            
            # PageRank
            pagerank = nx.pagerank(G, weight='weight')
            
            # í†µí•© ì ìˆ˜ ê³„ì‚° (ì—¬ëŸ¬ ì¤‘ì‹¬ì„±ì˜ í‰ê· )
            for node_id in G.nodes():
                scores = [
                    degree_centrality.get(node_id, 0),
                    closeness_centrality.get(node_id, 0),
                    betweenness_centrality.get(node_id, 0),
                    pagerank.get(node_id, 0)
                ]
                centrality_scores[node_id] = sum(scores) / len(scores)
            
            return centrality_scores
            
        except Exception as e:
            logger.error(f"ì¤‘ì‹¬ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _detect_communities(
        self, 
        nodes: List[GraphNode], 
        edges: List[GraphEdge]
    ) -> List[List[str]]:
        """ì»¤ë®¤ë‹ˆí‹° íƒì§€"""
        try:
            if not NETWORKX_AVAILABLE:
                return []
            
            # NetworkX ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()
            for node in nodes:
                G.add_node(node.id)
            for edge in edges:
                G.add_edge(edge.source, edge.target, weight=edge.weight)
            
            # ì—°ê²° ì»´í¬ë„ŒíŠ¸ ê¸°ë°˜ ì»¤ë®¤ë‹ˆí‹°
            communities = []
            for component in nx.connected_components(G):
                if len(component) >= 2:  # ìµœì†Œ 2ê°œ ë…¸ë“œ
                    communities.append(list(component))
            
            # í° ì»´í¬ë„ŒíŠ¸ì˜ ê²½ìš° ì¶”ê°€ ë¶„í•  ì‹œë„
            large_communities = []
            for community in communities:
                if len(community) > 10:
                    # ì„œë¸Œê·¸ë˜í”„ ìƒì„±
                    subG = G.subgraph(community)
                    
                    # ëª¨ë“ˆì„± ê¸°ë°˜ ë¶„í•  ì‹œë„
                    try:
                        import networkx.algorithms.community as nx_comm
                        sub_communities = nx_comm.greedy_modularity_communities(subG)
                        large_communities.extend([list(c) for c in sub_communities if len(c) >= 2])
                    except:
                        large_communities.append(community)
                else:
                    large_communities.append(community)
            
            logger.info(f"ì»¤ë®¤ë‹ˆí‹° íƒì§€: {len(large_communities)}ê°œ")
            return large_communities
            
        except Exception as e:
            logger.error(f"ì»¤ë®¤ë‹ˆí‹° íƒì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def _create_empty_graph(self) -> KnowledgeGraph:
        """ë¹ˆ ê·¸ë˜í”„ ìƒì„±"""
        return KnowledgeGraph(
            nodes=[],
            edges=[],
            created_at=datetime.now()
        )
    
    def visualize_graph(
        self, 
        knowledge_graph: KnowledgeGraph,
        output_file: Optional[str] = None,
        layout: str = "spring",
        node_size_attr: str = "word_count",
        show_labels: bool = True
    ) -> bool:
        """ì§€ì‹ ê·¸ë˜í”„ ì‹œê°í™”"""
        try:
            if not NETWORKX_AVAILABLE or not MATPLOTLIB_AVAILABLE:
                logger.error("NetworkX ë˜ëŠ” Matplotlibê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
            
            if not knowledge_graph.nodes:
                logger.warning("ì‹œê°í™”í•  ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # í•œê¸€ í°íŠ¸ ì„¤ì •
            import matplotlib.font_manager as fm
            import platform
            
            if platform.system() == 'Darwin':  # macOS
                font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'
                if os.path.exists(font_path):
                    font_prop = fm.FontProperties(fname=font_path)
                    plt.rcParams['font.family'] = font_prop.get_name()
                    plt.rcParams['axes.unicode_minus'] = False
            elif platform.system() == 'Windows':
                plt.rcParams['font.family'] = 'Malgun Gothic'
                plt.rcParams['axes.unicode_minus'] = False
            else:  # Linux
                plt.rcParams['font.family'] = 'DejaVu Sans'
            
            # NetworkX ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()
            
            # ë…¸ë“œ ì¶”ê°€
            for node in knowledge_graph.nodes:
                G.add_node(
                    node.id, 
                    title=node.title,
                    node_type=node.node_type,
                    word_count=node.word_count,
                    centrality=knowledge_graph.centrality_scores.get(node.id, 0) if knowledge_graph.centrality_scores else 0
                )
            
            # ì—£ì§€ ì¶”ê°€
            for edge in knowledge_graph.edges:
                G.add_edge(edge.source, edge.target, weight=edge.weight, rel_type=edge.relationship_type)
            
            # ë ˆì´ì•„ì›ƒ ì„ íƒ
            if layout == "spring":
                pos = nx.spring_layout(G, k=1, iterations=50)
            elif layout == "circular":
                pos = nx.circular_layout(G)
            elif layout == "kamada_kawai":
                pos = nx.kamada_kawai_layout(G)
            else:
                pos = nx.spring_layout(G)
            
            # í”Œë¡¯ ì„¤ì •
            plt.figure(figsize=(16, 12))
            
            # ë…¸ë“œ íƒ€ì…ë³„ ìƒ‰ìƒ
            node_colors = []
            node_sizes = []
            
            for node_id in G.nodes():
                node_data = G.nodes[node_id]
                
                # ë…¸ë“œ ìƒ‰ìƒ (íƒ€ì…ë³„)
                if node_data['node_type'] == 'document':
                    node_colors.append('lightblue')
                elif node_data['node_type'] == 'tag':
                    node_colors.append('lightgreen')
                else:
                    node_colors.append('lightgray')
                
                # ë…¸ë“œ í¬ê¸° (ì†ì„± ê¸°ë°˜)
                size_value = node_data.get(node_size_attr, 1)
                if node_data['node_type'] == 'document':
                    node_sizes.append(max(100, min(1000, size_value * 2)))  # ë‹¨ì–´ ìˆ˜ ê¸°ë°˜
                else:
                    node_sizes.append(max(200, min(800, size_value * 100)))  # íƒœê·¸ ë¹ˆë„ ê¸°ë°˜
            
            # ì—£ì§€ ê·¸ë¦¬ê¸° (íƒ€ì…ë³„)
            similarity_edges = [(u, v) for u, v, d in G.edges(data=True) if d.get('rel_type') == 'similarity']
            tag_edges = [(u, v) for u, v, d in G.edges(data=True) if d.get('rel_type') == 'tag']
            reference_edges = [(u, v) for u, v, d in G.edges(data=True) if d.get('rel_type') == 'reference']
            
            # ìœ ì‚¬ë„ ì—£ì§€ (íšŒìƒ‰, ì–‡ìŒ)
            if similarity_edges:
                nx.draw_networkx_edges(G, pos, edgelist=similarity_edges, 
                                     edge_color='gray', alpha=0.3, width=0.5)
            
            # íƒœê·¸ ì—£ì§€ (ì´ˆë¡, ì¤‘ê°„)
            if tag_edges:
                nx.draw_networkx_edges(G, pos, edgelist=tag_edges, 
                                     edge_color='green', alpha=0.6, width=1.0)
            
            # ì°¸ì¡° ì—£ì§€ (ë¹¨ê°•, êµµìŒ)
            if reference_edges:
                nx.draw_networkx_edges(G, pos, edgelist=reference_edges, 
                                     edge_color='red', alpha=0.8, width=2.0)
            
            # ë…¸ë“œ ê·¸ë¦¬ê¸°
            nx.draw_networkx_nodes(G, pos, node_color=node_colors, 
                                 node_size=node_sizes, alpha=0.8, linewidths=1, edgecolors='black')
            
            # ë¼ë²¨ ê·¸ë¦¬ê¸°
            if show_labels:
                # ì œëª© ë¼ë²¨ (ì§§ê²Œ ìë¥´ê¸°)
                labels = {}
                for node_id in G.nodes():
                    title = G.nodes[node_id]['title']
                    if len(title) > 20:
                        title = title[:17] + "..."
                    labels[node_id] = title
                
                # í•œê¸€ í°íŠ¸ ì ìš©ì„ ìœ„í•œ ì„¤ì •
                if platform.system() == 'Darwin' and os.path.exists('/System/Library/Fonts/Supplemental/AppleGothic.ttf'):
                    nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold', 
                                          font_family='AppleGothic')
                else:
                    nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold')
            
            # ë²”ë¡€
            legend_elements = [
                mpatches.Patch(color='lightblue', label='ë¬¸ì„œ'),
                mpatches.Patch(color='lightgreen', label='íƒœê·¸'),
                plt.Line2D([0], [0], color='gray', alpha=0.5, label='ìœ ì‚¬ë„'),
                plt.Line2D([0], [0], color='green', alpha=0.6, label='íƒœê·¸ ì—°ê²°'),
                plt.Line2D([0], [0], color='red', alpha=0.8, label='ì°¸ì¡°')
            ]
            plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0, 1))
            
            # ì œëª© ë° ì •ë³´
            metrics = knowledge_graph.graph_metrics or {}
            plt.title(f'ì§€ì‹ ê·¸ë˜í”„\në…¸ë“œ: {knowledge_graph.get_node_count()}ê°œ, '
                     f'ì—£ì§€: {knowledge_graph.get_edge_count()}ê°œ, '
                     f'ë°€ë„: {knowledge_graph.get_density():.3f}', 
                     fontsize=14, pad=20)
            
            plt.axis('off')
            plt.tight_layout()
            
            # ì €ì¥ ë˜ëŠ” í‘œì‹œ
            if output_file:
                plt.savefig(output_file, dpi=300, bbox_inches='tight')
                logger.info(f"ê·¸ë˜í”„ ì‹œê°í™” ì €ì¥: {output_file}")
            else:
                plt.show()
            
            plt.close()
            return True
            
        except Exception as e:
            logger.error(f"ê·¸ë˜í”„ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def export_graph(self, knowledge_graph: KnowledgeGraph, output_file: str) -> bool:
        """ê·¸ë˜í”„ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            export_data = {
                "graph_info": {
                    "node_count": knowledge_graph.get_node_count(),
                    "edge_count": knowledge_graph.get_edge_count(),
                    "density": knowledge_graph.get_density(),
                    "created_at": knowledge_graph.created_at.isoformat() if knowledge_graph.created_at else None,
                    "metrics": knowledge_graph.graph_metrics
                },
                "nodes": [node.to_dict() for node in knowledge_graph.nodes],
                "edges": [edge.to_dict() for edge in knowledge_graph.edges],
                "communities": knowledge_graph.communities or [],
                "centrality_scores": knowledge_graph.centrality_scores or {}
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ê·¸ë˜í”„ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"ê·¸ë˜í”„ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False


def test_knowledge_graph():
    """ì§€ì‹ ê·¸ë˜í”„ í…ŒìŠ¤íŠ¸"""
    import tempfile
    import shutil
    from ..features.advanced_search import AdvancedSearchEngine
    
    try:
        # ì„ì‹œ vault ë° ìºì‹œ ìƒì„±
        temp_vault = tempfile.mkdtemp()
        temp_cache = tempfile.mkdtemp()
        
        # ìƒí˜¸ ì—°ê²°ëœ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤ ìƒì„±
        test_docs = [
            ("tdd_basics.md", """---
tags:
  - development/tdd
  - testing/unit
---

# TDD ê¸°ë³¸ ê°œë…

TDDëŠ” í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.
[[TDD ì‹¤ìŠµ]] ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
[[ë¦¬íŒ©í† ë§]]ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ íš¨ê³¼ì ì…ë‹ˆë‹¤."""),
            
            ("tdd_practice.md", """---
tags:
  - development/tdd  
  - practice/hands-on
---

# TDD ì‹¤ìŠµ

[[TDD ê¸°ë³¸ ê°œë…]]ì—ì„œ ë°°ìš´ ë‚´ìš©ì„ ì‹¤ìŠµí•´ë´…ì‹œë‹¤.
Red-Green-Refactor ì‚¬ì´í´ì„ ë”°ë¥´ë©° ê°œë°œí•©ë‹ˆë‹¤.
[[Clean Code]] ì›ì¹™ë„ í•¨ê»˜ ì ìš©í•´ë³´ì„¸ìš”."""),
            
            ("refactoring.md", """---
tags:
  - development/refactoring
  - code-quality/improvement
---

# ë¦¬íŒ©í† ë§

ì½”ë“œì˜ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
[[TDD ê¸°ë³¸ ê°œë…]]ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ì•ˆì „í•©ë‹ˆë‹¤.
[[Clean Code]] ì›ì¹™ì— ë”°ë¼ ì§„í–‰í•˜ì„¸ìš”."""),
            
            ("clean_code.md", """---
tags:
  - development/clean-code
  - best-practices/coding
---

# Clean Code

ê¹¨ë—í•œ ì½”ë“œ ì‘ì„±ì˜ ê¸°ë³¸ ì›ì¹™ë“¤ì…ë‹ˆë‹¤.
[[TDD ì‹¤ìŠµ]]ê³¼ [[ë¦¬íŒ©í† ë§]]ì„ í†µí•´ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ê°€ë…ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì´ ì¤‘ìš”í•©ë‹ˆë‹¤."""),
            
            ("testing_principles.md", """---
tags:
  - testing/principles
  - development/testing
---

# í…ŒìŠ¤íŒ… ì›ì¹™

íš¨ê³¼ì ì¸ í…ŒìŠ¤íŠ¸ ì‘ì„±ì„ ìœ„í•œ ì›ì¹™ë“¤ì…ë‹ˆë‹¤.
ë‹¨ìœ„ í…ŒìŠ¤íŠ¸, í†µí•© í…ŒìŠ¤íŠ¸, E2E í…ŒìŠ¤íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤."""),
            
            ("architecture.md", """---
tags:
  - architecture/software
  - design/patterns
---

# ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…ì²˜

ì‹œìŠ¤í…œì˜ ì „ì²´ì ì¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.
ëª¨ë“ˆí™”ì™€ ì˜ì¡´ì„± ê´€ë¦¬ê°€ í•µì‹¬ì…ë‹ˆë‹¤.
í™•ì¥ ê°€ëŠ¥í•˜ê³  ìœ ì§€ë³´ìˆ˜ê°€ ì‰¬ìš´ êµ¬ì¡°ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.""")
        ]
        
        for filename, content in test_docs:
            with open(Path(temp_vault) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ë° ì¸ë±ì‹±
        config = {
            "model": {"name": "BAAI/bge-m3"},
            "vault": {"excluded_dirs": [], "file_extensions": [".md"]},
            "graph": {
                "similarity_threshold": 0.3,
                "max_edges_per_node": 5,
                "include_tag_nodes": True,
                "min_word_count": 10  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‚®ì¶¤
            }
        }
        
        search_engine = AdvancedSearchEngine(temp_vault, temp_cache, config)
        print("ê²€ìƒ‰ ì—”ì§„ ì¸ë±ì‹± ì¤‘...")
        if not search_engine.build_index():
            print("âŒ ì¸ë±ì‹± ì‹¤íŒ¨")
            return False
        
        # ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•ê¸° ì´ˆê¸°í™”
        graph_builder = KnowledgeGraphBuilder(search_engine, config)
        
        # ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•
        print("\nğŸ•¸ï¸ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ì¤‘...")
        knowledge_graph = graph_builder.build_graph()
        
        print(f"\nğŸ“Š ì§€ì‹ ê·¸ë˜í”„ ê²°ê³¼:")
        print(f"- ë…¸ë“œ: {knowledge_graph.get_node_count()}ê°œ")
        print(f"- ì—£ì§€: {knowledge_graph.get_edge_count()}ê°œ")
        print(f"- ë°€ë„: {knowledge_graph.get_density():.3f}")
        
        # ë©”íŠ¸ë¦­ ì¶œë ¥
        if knowledge_graph.graph_metrics:
            metrics = knowledge_graph.graph_metrics
            print(f"- ì—°ê²°ëœ ê·¸ë˜í”„: {metrics.get('is_connected', False)}")
            print(f"- ì»´í¬ë„ŒíŠ¸ ìˆ˜: {metrics.get('number_of_components', 0)}")
            print(f"- í‰ê·  í´ëŸ¬ìŠ¤í„°ë§: {metrics.get('average_clustering', 0):.3f}")
        
        # ì¤‘ì‹¬ì„± ì ìˆ˜ (ìƒìœ„ 5ê°œ)
        if knowledge_graph.centrality_scores:
            print(f"\nğŸ¯ ë†’ì€ ì¤‘ì‹¬ì„± ì ìˆ˜ ë…¸ë“œ:")
            sorted_centrality = sorted(
                knowledge_graph.centrality_scores.items(), 
                key=lambda x: x[1], reverse=True
            )[:5]
            
            for node_id, score in sorted_centrality:
                # ë…¸ë“œ ì •ë³´ ì°¾ê¸°
                node = next((n for n in knowledge_graph.nodes if n.id == node_id), None)
                if node:
                    print(f"  - {node.title} ({node.node_type}): {score:.3f}")
        
        # ì»¤ë®¤ë‹ˆí‹° ì •ë³´
        if knowledge_graph.communities:
            print(f"\nğŸ˜ï¸ íƒì§€ëœ ì»¤ë®¤ë‹ˆí‹°: {len(knowledge_graph.communities)}ê°œ")
            for i, community in enumerate(knowledge_graph.communities):
                print(f"  ì»¤ë®¤ë‹ˆí‹° {i+1}: {len(community)}ê°œ ë…¸ë“œ")
                
                # ë…¸ë“œ ì´ë¦„ ì¶œë ¥
                community_nodes = [
                    n.title for n in knowledge_graph.nodes 
                    if n.id in community
                ][:3]  # ìƒìœ„ 3ê°œë§Œ
                print(f"    - {', '.join(community_nodes)}...")
        
        # ì—£ì§€ íƒ€ì…ë³„ í†µê³„
        edge_types = {}
        for edge in knowledge_graph.edges:
            edge_types[edge.relationship_type] = edge_types.get(edge.relationship_type, 0) + 1
        
        print(f"\nğŸ”— ì—£ì§€ íƒ€ì…ë³„ ë¶„í¬:")
        for edge_type, count in edge_types.items():
            print(f"  - {edge_type}: {count}ê°œ")
        
        # JSON ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
        output_file = Path(temp_cache) / "knowledge_graph.json"
        if graph_builder.export_graph(knowledge_graph, str(output_file)):
            print(f"\nğŸ’¾ ê·¸ë˜í”„ ì €ì¥: {output_file}")
        
        # ì‹œê°í™” í…ŒìŠ¤íŠ¸ (matplotlib ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if MATPLOTLIB_AVAILABLE and NETWORKX_AVAILABLE:
            viz_file = Path(temp_cache) / "knowledge_graph.png"
            try:
                if graph_builder.visualize_graph(knowledge_graph, str(viz_file)):
                    print(f"ğŸ“ˆ ê·¸ë˜í”„ ì‹œê°í™” ì €ì¥: {viz_file}")
            except:
                print("ğŸ“ˆ ì‹œê°í™”ëŠ” ê±´ë„ˆëœ€ (ì˜ì¡´ì„± ì—†ìŒ)")
        
        # ì •ë¦¬
        shutil.rmtree(temp_vault)
        shutil.rmtree(temp_cache)
        
        print("âœ… ì§€ì‹ ê·¸ë˜í”„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì§€ì‹ ê·¸ë˜í”„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_knowledge_graph()