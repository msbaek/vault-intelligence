#!/usr/bin/env python3
"""
Topic Collector for Vault Intelligence System V2

ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ì§‘ ë° í†µí•© ë¬¸ì„œ ìƒì„± (ê¸°ì¡´ collect ê¸°ëŠ¥)
"""

import os
import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
from collections import defaultdict, Counter

from ..core.vault_processor import Document
from ..features.advanced_search import SearchResult

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class CollectionMetadata:
    """ìˆ˜ì§‘ ë©”íƒ€ë°ì´í„°"""
    topic: str
    total_documents: int
    total_word_count: int
    total_size_bytes: int
    collection_date: datetime
    search_query: str
    similarity_threshold: float
    tag_distribution: Dict[str, int]
    directory_distribution: Dict[str, int]


@dataclass
class DocumentCollection:
    """ë¬¸ì„œ ì»¬ë ‰ì…˜"""
    metadata: CollectionMetadata
    documents: List[Document]
    grouped_documents: Dict[str, List[Document]]
    statistics: Dict[str, any]


class TopicCollector:
    """ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ì§‘ê¸°"""
    
    def __init__(
        self,
        search_engine,
        config: Dict = None
    ):
        """
        Args:
            search_engine: AdvancedSearchEngine ì¸ìŠ¤í„´ìŠ¤
            config: ìˆ˜ì§‘ ì„¤ì •
        """
        self.search_engine = search_engine
        self.config = config or {}
        
        # ê¸°ë³¸ ì„¤ì •
        self.min_documents = self.config.get('collection', {}).get('min_documents', 3)
        self.group_by_tags = self.config.get('collection', {}).get('group_by_tags', True)
        self.include_statistics = self.config.get('collection', {}).get('include_statistics', True)
        self.output_format = self.config.get('collection', {}).get('output_format', 'markdown')
        
        logger.info(f"ì£¼ì œ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”")
    
    def collect_topic(
        self,
        topic: str,
        top_k: int = 100,
        threshold: float = 0.3,
        min_word_count: Optional[int] = None,
        tags_filter: Optional[List[str]] = None,
        output_file: Optional[str] = None,
        use_expansion: bool = False,
        include_synonyms: bool = True,
        include_hyde: bool = True
    ) -> DocumentCollection:
        """ì£¼ì œë³„ ë¬¸ì„œ ìˆ˜ì§‘"""
        try:
            if not self.search_engine.indexed:
                logger.warning("ê²€ìƒ‰ ì—”ì§„ì´ ì¸ë±ì‹±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return self._create_empty_collection(topic)
            
            logger.info(f"ì£¼ì œ '{topic}' ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘...")
            if use_expansion:
                expand_features = []
                if include_synonyms:
                    expand_features.append("ë™ì˜ì–´")
                if include_hyde:
                    expand_features.append("HyDE")
                logger.info(f"ğŸ“ ì¿¼ë¦¬ í™•ì¥ ëª¨ë“œ í™œì„±í™”: {', '.join(expand_features)}")
            
            # í™•ì¥ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ì˜ë¯¸ì  + í‚¤ì›Œë“œ + ì„ íƒì  í™•ì¥)
            if use_expansion:
                search_results = self.search_engine.expanded_search(
                    query=topic,
                    search_method="hybrid",
                    top_k=top_k,
                    threshold=threshold,
                    include_synonyms=include_synonyms,
                    include_hyde=include_hyde
                )
            else:
                search_results = self.search_engine.hybrid_search(
                    topic,
                    top_k=top_k,
                    threshold=threshold
                )
            
            if not search_results:
                logger.warning(f"ì£¼ì œ '{topic}'ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return self._create_empty_collection(topic)
            
            # ë¬¸ì„œ ì¶”ì¶œ
            documents = [result.document for result in search_results]
            
            # ì¶”ê°€ í•„í„°ë§
            filtered_documents = self._apply_filters(
                documents, min_word_count, tags_filter
            )
            
            if len(filtered_documents) < self.min_documents:
                logger.warning(f"í•„í„°ë§ í›„ ë¬¸ì„œê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {len(filtered_documents)}")
            
            logger.info(f"ìˆ˜ì§‘ëœ ë¬¸ì„œ: {len(filtered_documents)}ê°œ")
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = self._create_metadata(
                topic, filtered_documents, topic, threshold
            )
            
            # ë¬¸ì„œ ê·¸ë£¹í™”
            grouped_documents = self._group_documents(filtered_documents)
            
            # í†µê³„ ìƒì„±
            statistics = self._generate_statistics(filtered_documents)
            
            # ì»¬ë ‰ì…˜ ìƒì„±
            collection = DocumentCollection(
                metadata=metadata,
                documents=filtered_documents,
                grouped_documents=grouped_documents,
                statistics=statistics
            )
            
            # ì¶œë ¥ íŒŒì¼ ìƒì„±
            if output_file:
                self.export_collection(collection, output_file)
            
            logger.info(f"ì£¼ì œ ìˆ˜ì§‘ ì™„ë£Œ: {len(filtered_documents)}ê°œ ë¬¸ì„œ")
            return collection
            
        except Exception as e:
            logger.error(f"ì£¼ì œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return self._create_empty_collection(topic)
    
    def _apply_filters(
        self,
        documents: List[Document],
        min_word_count: Optional[int],
        tags_filter: Optional[List[str]]
    ) -> List[Document]:
        """ë¬¸ì„œ í•„í„°ë§"""
        filtered = documents
        
        # ìµœì†Œ ë‹¨ì–´ ìˆ˜ í•„í„°
        if min_word_count:
            filtered = [doc for doc in filtered if doc.word_count >= min_word_count]
            logger.info(f"ìµœì†Œ ë‹¨ì–´ ìˆ˜ í•„í„° ì ìš© í›„: {len(filtered)}ê°œ")
        
        # íƒœê·¸ í•„í„°
        if tags_filter:
            tag_set = set(tag.lower() for tag in tags_filter)
            filtered = [
                doc for doc in filtered
                if any(tag.lower() in tag_set for tag in doc.tags)
            ]
            logger.info(f"íƒœê·¸ í•„í„° ì ìš© í›„: {len(filtered)}ê°œ")
        
        return filtered
    
    def _create_metadata(
        self,
        topic: str,
        documents: List[Document],
        search_query: str,
        threshold: float
    ) -> CollectionMetadata:
        """ì»¬ë ‰ì…˜ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        try:
            # ê¸°ë³¸ í†µê³„
            total_documents = len(documents)
            total_word_count = sum(doc.word_count for doc in documents)
            total_size_bytes = sum(doc.file_size for doc in documents)
            
            # íƒœê·¸ ë¶„í¬
            tag_counter = Counter()
            for doc in documents:
                if doc.tags:
                    tag_counter.update(doc.tags)
            tag_distribution = dict(tag_counter.most_common(20))
            
            # ë””ë ‰í† ë¦¬ ë¶„í¬
            dir_counter = Counter()
            for doc in documents:
                dir_path = str(Path(doc.path).parent)
                if dir_path == '.':
                    dir_path = 'root'
                dir_counter[dir_path] += 1
            directory_distribution = dict(dir_counter.most_common(10))
            
            return CollectionMetadata(
                topic=topic,
                total_documents=total_documents,
                total_word_count=total_word_count,
                total_size_bytes=total_size_bytes,
                collection_date=datetime.now(),
                search_query=search_query,
                similarity_threshold=threshold,
                tag_distribution=tag_distribution,
                directory_distribution=directory_distribution
            )
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return CollectionMetadata(
                topic=topic,
                total_documents=0,
                total_word_count=0,
                total_size_bytes=0,
                collection_date=datetime.now(),
                search_query=search_query,
                similarity_threshold=threshold,
                tag_distribution={},
                directory_distribution={}
            )
    
    def _group_documents(self, documents: List[Document]) -> Dict[str, List[Document]]:
        """ë¬¸ì„œ ê·¸ë£¹í™”"""
        try:
            if not self.group_by_tags:
                return {"ì „ì²´": documents}
            
            grouped = defaultdict(list)
            untagged = []
            
            for doc in documents:
                if doc.tags:
                    # ì²« ë²ˆì§¸ íƒœê·¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
                    primary_tag = doc.tags[0]
                    
                    # ê³„ì¸µì  íƒœê·¸ì˜ ê²½ìš° ìµœìƒìœ„ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©
                    if '/' in primary_tag:
                        primary_tag = primary_tag.split('/')[0]
                    
                    grouped[primary_tag].append(doc)
                else:
                    untagged.append(doc)
            
            # íƒœê·¸ ì—†ëŠ” ë¬¸ì„œ
            if untagged:
                grouped["ê¸°íƒ€"] = untagged
            
            # ë¬¸ì„œ ìˆ˜ê°€ ì ì€ ê·¸ë£¹ë“¤ì„ "ê¸°íƒ€"ë¡œ í†µí•©
            final_grouped = {}
            misc_docs = []
            
            for tag, docs in grouped.items():
                if len(docs) >= 2 or tag == "ê¸°íƒ€":  # ìµœì†Œ 2ê°œ ë˜ëŠ” ì´ë¯¸ ê¸°íƒ€
                    final_grouped[tag] = docs
                else:
                    misc_docs.extend(docs)
            
            if misc_docs:
                if "ê¸°íƒ€" in final_grouped:
                    final_grouped["ê¸°íƒ€"].extend(misc_docs)
                else:
                    final_grouped["ê¸°íƒ€"] = misc_docs
            
            return dict(final_grouped)
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê·¸ë£¹í™” ì‹¤íŒ¨: {e}")
            return {"ì „ì²´": documents}
    
    def _generate_statistics(self, documents: List[Document]) -> Dict:
        """í†µê³„ ì •ë³´ ìƒì„±"""
        try:
            if not documents:
                return {}
            
            word_counts = [doc.word_count for doc in documents]
            file_sizes = [doc.file_size for doc in documents]
            
            statistics = {
                "document_count": len(documents),
                "word_count": {
                    "total": sum(word_counts),
                    "average": sum(word_counts) / len(word_counts),
                    "min": min(word_counts),
                    "max": max(word_counts)
                },
                "file_size": {
                    "total_bytes": sum(file_sizes),
                    "total_mb": sum(file_sizes) / (1024 * 1024),
                    "average_bytes": sum(file_sizes) / len(file_sizes)
                },
                "modification_dates": {
                    "newest": max(doc.modified_at for doc in documents),
                    "oldest": min(doc.modified_at for doc in documents)
                }
            }
            
            return statistics
            
        except Exception as e:
            logger.error(f"í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def _create_empty_collection(self, topic: str) -> DocumentCollection:
        """ë¹ˆ ì»¬ë ‰ì…˜ ìƒì„±"""
        metadata = CollectionMetadata(
            topic=topic,
            total_documents=0,
            total_word_count=0,
            total_size_bytes=0,
            collection_date=datetime.now(),
            search_query=topic,
            similarity_threshold=0.0,
            tag_distribution={},
            directory_distribution={}
        )
        
        return DocumentCollection(
            metadata=metadata,
            documents=[],
            grouped_documents={},
            statistics={}
        )
    
    def export_collection(
        self,
        collection: DocumentCollection,
        output_file: str,
        format_type: Optional[str] = None
    ) -> bool:
        """ì»¬ë ‰ì…˜ì„ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            format_type = format_type or self.output_format
            
            if format_type.lower() == "markdown":
                return self._export_as_markdown(collection, output_file)
            elif format_type.lower() == "json":
                return self._export_as_json(collection, output_file)
            else:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format_type}, ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.")
                return self._export_as_markdown(collection, output_file)
                
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def _export_as_markdown(self, collection: DocumentCollection, output_file: str) -> bool:
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            metadata = collection.metadata
            
            # ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  ìƒì„±
            content = f"""---
tags:
  - vault-intelligence/collection
  - topic/{metadata.topic.lower().replace(' ', '-')}
generated: {metadata.collection_date.isoformat()}
topic: {metadata.topic}
total_documents: {metadata.total_documents}
---

# {metadata.topic} ë¬¸ì„œ ì»¬ë ‰ì…˜

**ìƒì„±ì¼**: {metadata.collection_date.strftime('%Y-%m-%d %H:%M:%S')}  
**ìˆ˜ì§‘ëœ ë¬¸ì„œ**: {metadata.total_documents}ê°œ  
**ì´ ë‹¨ì–´ ìˆ˜**: {metadata.total_word_count:,}ê°œ  
**ê²€ìƒ‰ ì¿¼ë¦¬**: "{metadata.search_query}"

## ğŸ“Š ê°œìš”

ì´ ë¬¸ì„œëŠ” '{metadata.topic}' ì£¼ì œì™€ ê´€ë ¨ëœ ëª¨ë“  ë¬¸ì„œë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•œ ì»¬ë ‰ì…˜ì…ë‹ˆë‹¤.

### ìˆ˜ì§‘ í†µê³„
- **ì´ ë¬¸ì„œ ìˆ˜**: {metadata.total_documents}ê°œ
- **ì´ ë‹¨ì–´ ìˆ˜**: {metadata.total_word_count:,}ê°œ  
- **ì´ íŒŒì¼ í¬ê¸°**: {metadata.total_size_bytes / (1024*1024):.2f}MB
- **ìœ ì‚¬ë„ ì„ê³„ê°’**: {metadata.similarity_threshold}

"""
            
            # íƒœê·¸ ë¶„í¬ (ìƒìœ„ 10ê°œ)
            if metadata.tag_distribution:
                content += "\n### ğŸ·ï¸ ì£¼ìš” íƒœê·¸\n\n"
                for tag, count in list(metadata.tag_distribution.items())[:10]:
                    content += f"- **{tag}**: {count}ê°œ ë¬¸ì„œ\n"
            
            # ë””ë ‰í† ë¦¬ ë¶„í¬
            if metadata.directory_distribution:
                content += "\n### ğŸ“ ë””ë ‰í† ë¦¬ë³„ ë¶„í¬\n\n"
                for dir_path, count in metadata.directory_distribution.items():
                    content += f"- **{dir_path}**: {count}ê°œ ë¬¸ì„œ\n"
            
            content += "\n## ğŸ“‘ ë¬¸ì„œ ëª©ë¡\n\n"
            
            # ê·¸ë£¹ë³„ ë¬¸ì„œ ë‚˜ì—´
            for group_name, docs in collection.grouped_documents.items():
                content += f"### {group_name}\n\n"
                
                # ë‹¨ì–´ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                sorted_docs = sorted(docs, key=lambda d: d.word_count, reverse=True)
                
                for doc in sorted_docs:
                    content += f"- **[[{doc.path}]]**"
                    
                    if doc.word_count:
                        content += f" ({doc.word_count:,} ë‹¨ì–´)"
                    
                    if doc.tags:
                        tags_str = ", ".join(f"#{tag}" for tag in doc.tags[:3])
                        content += f" - {tags_str}"
                    
                    content += "\n"
                
                content += "\n"
            
            # í†µê³„ ì„¹ì…˜
            if self.include_statistics and collection.statistics:
                stats = collection.statistics
                content += f"""## ğŸ“ˆ ìƒì„¸ í†µê³„

### ë¬¸ì„œ í†µê³„
- **ì´ ë¬¸ì„œ**: {stats['document_count']}ê°œ
- **í‰ê·  ë‹¨ì–´ ìˆ˜**: {stats['word_count']['average']:.0f}ê°œ
- **ìµœëŒ€ ë‹¨ì–´ ìˆ˜**: {stats['word_count']['max']:,}ê°œ
- **ìµœì†Œ ë‹¨ì–´ ìˆ˜**: {stats['word_count']['min']:,}ê°œ

### íŒŒì¼ í¬ê¸°
- **ì´ í¬ê¸°**: {stats['file_size']['total_mb']:.2f}MB
- **í‰ê·  í¬ê¸°**: {stats['file_size']['average_bytes']/1024:.1f}KB

### ìˆ˜ì • ì¼ì
- **ê°€ì¥ ìµœì‹ **: {stats['modification_dates']['newest'].strftime('%Y-%m-%d %H:%M')}
- **ê°€ì¥ ì˜¤ë˜ëœ**: {stats['modification_dates']['oldest'].strftime('%Y-%m-%d %H:%M')}

"""
            
            content += f"""---

**ì»¬ë ‰ì…˜ ì •ë³´**  
- ìƒì„± ì‹œìŠ¤í…œ: Vault Intelligence System V2
- ìƒì„± ì¼ì‹œ: {metadata.collection_date.strftime('%Y-%m-%d %H:%M:%S')}
- ê²€ìƒ‰ ë°©ì‹: Sentence Transformers (768ì°¨ì›)
- ì„ê³„ê°’: {metadata.similarity_threshold}
"""
            
            # íŒŒì¼ ì €ì¥
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info(f"ë§ˆí¬ë‹¤ìš´ ì»¬ë ‰ì…˜ ì €ì¥ ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"ë§ˆí¬ë‹¤ìš´ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def _export_as_json(self, collection: DocumentCollection, output_file: str) -> bool:
        """JSON í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            import json
            
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            export_data = {
                "metadata": {
                    "topic": collection.metadata.topic,
                    "total_documents": collection.metadata.total_documents,
                    "total_word_count": collection.metadata.total_word_count,
                    "total_size_bytes": collection.metadata.total_size_bytes,
                    "collection_date": collection.metadata.collection_date.isoformat(),
                    "search_query": collection.metadata.search_query,
                    "similarity_threshold": collection.metadata.similarity_threshold,
                    "tag_distribution": collection.metadata.tag_distribution,
                    "directory_distribution": collection.metadata.directory_distribution
                },
                "documents": [
                    {
                        "path": doc.path,
                        "title": doc.title,
                        "word_count": doc.word_count,
                        "file_size": doc.file_size,
                        "tags": doc.tags,
                        "modified_at": doc.modified_at.isoformat()
                    }
                    for doc in collection.documents
                ],
                "grouped_documents": {
                    group: [
                        {
                            "path": doc.path,
                            "title": doc.title,
                            "word_count": doc.word_count
                        }
                        for doc in docs
                    ]
                    for group, docs in collection.grouped_documents.items()
                },
                "statistics": collection.statistics
            }
            
            # ë‚ ì§œ ê°ì²´ ì²˜ë¦¬
            if collection.statistics and 'modification_dates' in collection.statistics:
                export_data['statistics']['modification_dates'] = {
                    'newest': collection.statistics['modification_dates']['newest'].isoformat(),
                    'oldest': collection.statistics['modification_dates']['oldest'].isoformat()
                }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"JSON ì»¬ë ‰ì…˜ ì €ì¥ ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"JSON ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def suggest_related_topics(self, topic: str, top_k: int = 5) -> List[Tuple[str, int]]:
        """ê´€ë ¨ ì£¼ì œ ì œì•ˆ"""
        try:
            # ì£¼ì œ ìˆ˜ì§‘
            collection = self.collect_topic(topic, top_k=50, threshold=0.2)
            
            if not collection.documents:
                return []
            
            # íƒœê·¸ì—ì„œ ê´€ë ¨ ì£¼ì œ ì¶”ì¶œ
            tag_suggestions = []
            for tag, count in collection.metadata.tag_distribution.items():
                if tag.lower() != topic.lower() and count >= 2:
                    tag_suggestions.append((tag, count))
            
            # ë¬¸ì„œ ì œëª©ì—ì„œ ê³µí†µ í‚¤ì›Œë“œ ì¶”ì¶œ
            title_words = []
            for doc in collection.documents:
                words = doc.title.split()
                title_words.extend([w.lower().strip('.,!?') for w in words if len(w) > 3])
            
            word_counter = Counter(title_words)
            title_suggestions = [
                (word, count) for word, count in word_counter.most_common(10)
                if word.lower() != topic.lower() and count >= 2
            ]
            
            # í†µí•© ë° ì •ë ¬
            all_suggestions = tag_suggestions + title_suggestions
            all_suggestions.sort(key=lambda x: x[1], reverse=True)
            
            return all_suggestions[:top_k]
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ ì£¼ì œ ì œì•ˆ ì‹¤íŒ¨: {e}")
            return []


def test_topic_collector():
    """ì£¼ì œ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸"""
    import tempfile
    import shutil
    from ..features.advanced_search import AdvancedSearchEngine
    
    try:
        # ì„ì‹œ vault ë° ìºì‹œ ìƒì„±
        temp_vault = tempfile.mkdtemp()
        temp_cache = tempfile.mkdtemp()
        
        # ë‹¤ì–‘í•œ ì£¼ì œì˜ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤ ìƒì„±
        test_docs = [
            ("tdd_basics.md", """---
tags:
  - development/tdd
  - testing/unit
---

# TDD ê¸°ë³¸ ê°œë…

TDDëŠ” í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.
Red-Green-Refactor ì‚¬ì´í´ì„ ë”°ë¦…ë‹ˆë‹¤.
ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ê³  êµ¬í˜„í•©ë‹ˆë‹¤."""),
            
            ("tdd_practice.md", """---
tags:
  - development/tdd  
  - practice/coding
---

# TDD ì‹¤ìŠµ ê°€ì´ë“œ

í…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ëŠ” ì‹¤ìŠµì„ í•´ë´…ì‹œë‹¤.
ì‹¤íŒ¨í•˜ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê³  êµ¬í˜„ìœ¼ë¡œ í†µê³¼ì‹œí‚µë‹ˆë‹¤."""),
            
            ("refactoring_guide.md", """---
tags:
  - development/refactoring
  - code-quality/improvement
---

# ë¦¬íŒ©í† ë§ ê°€ì´ë“œ

ì½”ë“œì˜ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ëŠ” ë°©ë²•ë“¤ì…ë‹ˆë‹¤.
í…ŒìŠ¤íŠ¸ê°€ ìˆì–´ì•¼ ì•ˆì „í•˜ê²Œ ë¦¬íŒ©í† ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""),
            
            ("clean_code.md", """---
tags:
  - development/clean-code
  - best-practices/coding
---

# Clean Code ì›ì¹™

ê¹¨ë—í•œ ì½”ë“œ ì‘ì„±ì˜ ê¸°ë³¸ ì›ì¹™ë“¤ì…ë‹ˆë‹¤.
ì½ê¸° ì‰½ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì½”ë“œë¥¼ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤."""),
            
            ("architecture.md", """---
tags:
  - architecture/software
  - design/system
---

# ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…ì²˜

ì‹œìŠ¤í…œì˜ ì „ì²´ì ì¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
ëª¨ë“ˆí™”ì™€ ì˜ì¡´ì„± ê´€ë¦¬ê°€ í•µì‹¬ì…ë‹ˆë‹¤.""")
        ]
        
        for filename, content in test_docs:
            with open(Path(temp_vault) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ë° ì¸ë±ì‹±
        config = {
            "model": {"name": "paraphrase-multilingual-mpnet-base-v2"},
            "vault": {"excluded_dirs": [], "file_extensions": [".md"]},
            "collection": {
                "min_documents": 1,
                "group_by_tags": True,
                "include_statistics": True,
                "output_format": "markdown"
            }
        }
        
        search_engine = AdvancedSearchEngine(temp_vault, temp_cache, config)
        print("ê²€ìƒ‰ ì—”ì§„ ì¸ë±ì‹± ì¤‘...")
        if not search_engine.build_index():
            print("âŒ ì¸ë±ì‹± ì‹¤íŒ¨")
            return False
        
        # ì£¼ì œ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        collector = TopicCollector(search_engine, config)
        
        # TDD ì£¼ì œ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
        print("\nğŸ“š 'TDD' ì£¼ì œ ë¬¸ì„œ ìˆ˜ì§‘ ì¤‘...")
        collection = collector.collect_topic(
            topic="TDD",
            top_k=10,
            threshold=0.2
        )
        
        print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"- ì£¼ì œ: {collection.metadata.topic}")
        print(f"- ì´ ë¬¸ì„œ: {collection.metadata.total_documents}ê°œ")
        print(f"- ì´ ë‹¨ì–´: {collection.metadata.total_word_count:,}ê°œ")
        print(f"- ê²€ìƒ‰ ì¿¼ë¦¬: '{collection.metadata.search_query}'")
        
        print(f"\nğŸ·ï¸ íƒœê·¸ ë¶„í¬:")
        for tag, count in collection.metadata.tag_distribution.items():
            print(f"  - {tag}: {count}ê°œ")
        
        print(f"\nğŸ“ ê·¸ë£¹ë³„ ë¬¸ì„œ:")
        for group, docs in collection.grouped_documents.items():
            print(f"  {group}:")
            for doc in docs:
                print(f"    - {doc.path} ({doc.word_count} ë‹¨ì–´)")
        
        # ë§ˆí¬ë‹¤ìš´ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
        output_file = Path(temp_cache) / "tdd_collection.md"
        if collector.export_collection(collection, str(output_file), "markdown"):
            print(f"\nğŸ’¾ ë§ˆí¬ë‹¤ìš´ ì €ì¥: {output_file}")
            
            # ìƒì„±ëœ íŒŒì¼ ì¼ë¶€ í™•ì¸
            with open(output_file, 'r', encoding='utf-8') as f:
                content = f.read()
                print("\nğŸ“ ìƒì„±ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì¼ë¶€:")
                print(content[:500] + "...")
        
        # JSON ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
        json_file = Path(temp_cache) / "tdd_collection.json"
        if collector.export_collection(collection, str(json_file), "json"):
            print(f"\nğŸ’¾ JSON ì €ì¥: {json_file}")
        
        # ê´€ë ¨ ì£¼ì œ ì œì•ˆ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ”— 'TDD' ê´€ë ¨ ì£¼ì œ ì œì•ˆ:")
        suggestions = collector.suggest_related_topics("TDD", top_k=5)
        for topic, count in suggestions:
            print(f"  - {topic} ({count}íšŒ ì–¸ê¸‰)")
        
        # ê°œë°œ ì£¼ì œ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ“š 'development' ì£¼ì œ ë¬¸ì„œ ìˆ˜ì§‘ ì¤‘...")
        dev_collection = collector.collect_topic(
            topic="development",
            top_k=20,
            threshold=0.1
        )
        
        print(f"- ê°œë°œ ê´€ë ¨ ë¬¸ì„œ: {dev_collection.metadata.total_documents}ê°œ")
        
        # ì •ë¦¬
        shutil.rmtree(temp_vault)
        shutil.rmtree(temp_cache)
        
        print("âœ… ì£¼ì œ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ì£¼ì œ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_topic_collector()