#!/usr/bin/env python3
"""
Advanced Search Engine for Vault Intelligence System V2

Sentence Transformers ê¸°ë°˜ ì˜ë¯¸ì  ê²€ìƒ‰ ë° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„
"""

import os
import re
import logging
from typing import List, Dict, Optional, Tuple, Union
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
import numpy as np
from collections import defaultdict

try:
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

from ..core.sentence_transformer_engine import SentenceTransformerEngine
from ..core.embedding_cache import EmbeddingCache, CachedEmbedding
from ..core.vault_processor import VaultProcessor, Document

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼"""
    document: Document
    similarity_score: float
    match_type: str  # "semantic", "keyword", "hybrid"
    matched_keywords: List[str] = None
    snippet: str = ""
    rank: int = 0


@dataclass 
class SearchQuery:
    """ê²€ìƒ‰ ì¿¼ë¦¬"""
    text: str
    tags: List[str] = None
    date_from: Optional[datetime] = None
    date_to: Optional[datetime] = None
    min_word_count: Optional[int] = None
    max_word_count: Optional[int] = None
    exclude_paths: List[str] = None


class AdvancedSearchEngine:
    """ê³ ê¸‰ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(
        self,
        vault_path: str,
        cache_dir: str,
        config: Dict = None
    ):
        """
        Args:
            vault_path: Vault ê²½ë¡œ
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
            config: ê²€ìƒ‰ ì„¤ì •
        """
        self.vault_path = Path(vault_path)
        self.config = config or {}
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.engine = SentenceTransformerEngine(
            model_name=self.config.get('model', {}).get('name', 'paraphrase-multilingual-mpnet-base-v2'),
            cache_dir=self.config.get('model', {}).get('cache_folder', 'models'),
            device=self.config.get('model', {}).get('device')
        )
        
        self.cache = EmbeddingCache(cache_dir)
        
        self.processor = VaultProcessor(
            str(vault_path),
            excluded_dirs=self.config.get('vault', {}).get('excluded_dirs'),
            file_extensions=self.config.get('vault', {}).get('file_extensions')
        )
        
        # ë¬¸ì„œ ë° ì„ë² ë”© ìºì‹œ
        self.documents: List[Document] = []
        self.embeddings: Optional[np.ndarray] = None
        self.indexed = False
        
        logger.info(f"ê³ ê¸‰ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”: {vault_path}")
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ìë™ ë¡œë“œ ì‹œë„
        self.load_index()
    
    def build_index(self, force_rebuild: bool = False, progress_callback=None) -> bool:
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            logger.info("ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘...")
            
            # ë¬¸ì„œ ì²˜ë¦¬
            self.documents = self.processor.process_all_files(progress_callback)
            logger.info(f"ì²˜ë¦¬ëœ ë¬¸ì„œ: {len(self.documents)}ê°œ")
            
            if not self.documents:
                logger.warning("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # TF-IDF vectorizer í›ˆë ¨ (ëª¨ë“  ë¬¸ì„œ ë‚´ìš©)
            all_contents = [doc.content for doc in self.documents]
            all_paths = [doc.path for doc in self.documents]
            self.engine.fit_documents(all_contents, all_paths)
            logger.info("TF-IDF vectorizer í›ˆë ¨ ì™„ë£Œ")
            
            # ì„ë² ë”© ìƒì„±/ë¡œë”©
            embeddings_list = []
            cache_hits = 0
            new_embeddings = 0
            
            for i, doc in enumerate(self.documents):
                try:
                    # ìºì‹œì—ì„œ ì„ë² ë”© ì¡°íšŒ
                    cached = self.cache.get_embedding(
                        str(self.vault_path / doc.path), 
                        doc.file_hash
                    )
                    
                    if cached and not force_rebuild:
                        # ìºì‹œëœ ì„ë² ë”© ê²€ì¦
                        if (isinstance(cached.embedding, np.ndarray) and 
                            cached.embedding.size > 0 and 
                            not np.allclose(cached.embedding, 0)):
                            embeddings_list.append(cached.embedding)
                            doc.embedding = cached.embedding
                            cache_hits += 1
                        else:
                            logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ìºì‹œ ì„ë² ë”©: {doc.path}")
                            # ìƒˆë¡œ ìƒì„±
                            embedding = self.engine.encode_text(doc.content)
                            embeddings_list.append(embedding)
                            doc.embedding = embedding
                            self.cache.store_embedding(
                                str(self.vault_path / doc.path),
                                embedding,
                                self.engine.model_name,
                                doc.word_count
                            )
                            new_embeddings += 1
                    else:
                        # ìƒˆ ì„ë² ë”© ìƒì„±
                        if doc.content and doc.content.strip():
                            embedding = self.engine.encode_text(doc.content)
                            if embedding is not None and not np.allclose(embedding, 0):
                                embeddings_list.append(embedding)
                                doc.embedding = embedding
                                
                                # ìºì‹œì— ì €ì¥
                                self.cache.store_embedding(
                                    str(self.vault_path / doc.path),
                                    embedding,
                                    self.engine.model_name,
                                    doc.word_count
                                )
                                new_embeddings += 1
                            else:
                                logger.warning(f"0ì¸ ì„ë² ë”© ìƒì„±ë¨: {doc.path}")
                                empty_embedding = np.zeros(self.engine.embedding_dimension)
                                embeddings_list.append(empty_embedding)
                                doc.embedding = empty_embedding
                        else:
                            logger.warning(f"ë¹ˆ ë‚´ìš© ë¬¸ì„œ: {doc.path}")
                            empty_embedding = np.zeros(self.engine.embedding_dimension)
                            embeddings_list.append(empty_embedding)
                            doc.embedding = empty_embedding
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback and (i + 1) % 50 == 0:
                        progress_callback(i + 1, len(self.documents))
                
                except Exception as e:
                    logger.error(f"ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {doc.path}, {e}")
                    # ë¹ˆ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´
                    empty_embedding = np.zeros(self.engine.embedding_dimension)
                    embeddings_list.append(empty_embedding)
                    doc.embedding = empty_embedding
            
            # ì„ë² ë”© ë°°ì—´ ìƒì„±
            if embeddings_list:
                self.embeddings = np.array(embeddings_list)
                self.indexed = True
                
                logger.info(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ:")
                logger.info(f"- ë¬¸ì„œ: {len(self.documents)}ê°œ")
                logger.info(f"- ìºì‹œ íˆíŠ¸: {cache_hits}ê°œ")
                logger.info(f"- ì‹ ê·œ ì„ë² ë”©: {new_embeddings}ê°œ")
                logger.info(f"- ì„ë² ë”© í˜•íƒœ: {self.embeddings.shape}")
                
                # ì¸ë±ìŠ¤ ì €ì¥
                self.save_index()
                
                return True
            
            return False
        
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            return False
    
    def load_index(self) -> bool:
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            # TF-IDF ëª¨ë¸ ë¡œë“œ
            model_path = os.path.join(self.cache.cache_dir, "tfidf_model.pkl")
            if os.path.exists(model_path):
                self.engine.load_model(model_path)
                logger.info("TF-IDF ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
                # ë¬¸ì„œ ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ
                logger.info("ë¬¸ì„œ ë°ì´í„° ë¡œë”© ì¤‘...")
                self.documents = self.processor.process_all_files()
                logger.info(f"ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(self.documents)}ê°œ")
                
                # TF-IDF ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆì§€ë§Œ ìƒˆ ë¬¸ì„œë“¤ì— ëŒ€í•´ ë‹¤ì‹œ í›ˆë ¨ í•„ìš”
                # (ìºì‹œëœ ì„ë² ë”©ì´ í˜„ì¬ TF-IDF ëª¨ë¸ê³¼ í˜¸í™˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
                logger.info("TF-IDF ëª¨ë¸ì„ í˜„ì¬ ë¬¸ì„œë“¤ë¡œ ì¬í›ˆë ¨ ì¤‘...")
                all_contents = [doc.content for doc in self.documents]
                all_paths = [doc.path for doc in self.documents]
                self.engine.fit_documents(all_contents, all_paths)
                logger.info("TF-IDF ì¬í›ˆë ¨ ì™„ë£Œ")
                
                # ì„ë² ë”© ë°°ì—´ ì¬ìƒì„± (TF-IDF ì¬í›ˆë ¨ í›„ ëª¨ë“  ì„ë² ë”© ìƒˆë¡œ ìƒì„±)
                embeddings_list = []
                logger.info("ëª¨ë“  ì„ë² ë”©ì„ ìƒˆë¡œ ìƒì„± ì¤‘...")
                for i, doc in enumerate(self.documents):
                    # ìºì‹œ ë¬´ì‹œí•˜ê³  ëª¨ë“  ì„ë² ë”© ìƒˆë¡œ ìƒì„± 
                    # (TF-IDF ì¬í›ˆë ¨ìœ¼ë¡œ ì¸í•´ ê¸°ì¡´ ìºì‹œê°€ ë¬´íš¨í™”ë¨)
                    embedding = self.engine.encode_text(doc.content)
                    embeddings_list.append(embedding)
                    doc.embedding = embedding
                    
                    # ìƒˆ ì„ë² ë”©ì„ ìºì‹œì— ì €ì¥
                    self.cache.store_embedding(
                        str(self.vault_path / doc.path),
                        embedding,
                        self.engine.model_name,
                        doc.word_count
                    )
                    
                    # ì§„í–‰ë¥  í‘œì‹œ
                    if (i + 1) % 100 == 0:
                        logger.info(f"ì„ë² ë”© ìƒì„± ì§„í–‰ë¥ : {i + 1}/{len(self.documents)}")
                
                logger.info("ëª¨ë“  ì„ë² ë”© ìƒì„± ì™„ë£Œ")
                
                if embeddings_list:
                    self.embeddings = np.array(embeddings_list)
                    self.indexed = True
                    logger.info(f"ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ, {self.embeddings.shape}")
                    return True
                else:
                    logger.warning("ì„ë² ë”© ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
            else:
                logger.warning("ì €ì¥ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def save_index(self) -> bool:
        """ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            if not self.indexed:
                logger.warning("ì €ì¥í•  ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # TF-IDF ëª¨ë¸ ì €ì¥
            model_path = os.path.join(self.cache.cache_dir, "tfidf_model.pkl")
            self.engine.save_model(model_path)
            logger.info("ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def semantic_search(
        self,
        query: str,
        top_k: int = 10,
        threshold: float = 0.0
    ) -> List[SearchResult]:
        """ì˜ë¯¸ì  ê²€ìƒ‰"""
        if not self.indexed:
            # ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            if not self.load_index():
                logger.warning("ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.engine.encode_text(query)
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self.engine.calculate_similarities(
                query_embedding, 
                self.embeddings
            )
            
            # ìƒìœ„ ê²°ê³¼ ì„ íƒ
            results = self.engine.find_most_similar(
                query_embedding,
                self.embeddings,
                top_k=min(top_k, len(self.documents))
            )
            
            # SearchResult ê°ì²´ ìƒì„±
            search_results = []
            for rank, (idx, similarity) in enumerate(results):
                if similarity >= threshold:
                    snippet = self._generate_snippet(self.documents[idx], query)
                    
                    result = SearchResult(
                        document=self.documents[idx],
                        similarity_score=similarity,
                        match_type="semantic",
                        snippet=snippet,
                        rank=rank + 1
                    )
                    search_results.append(result)
            
            logger.info(f"ì˜ë¯¸ì  ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
        
        except Exception as e:
            logger.error(f"ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def keyword_search(
        self,
        query: str,
        top_k: int = 10,
        case_sensitive: bool = False
    ) -> List[SearchResult]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰"""
        if not self.documents:
            logger.warning("ë¬¸ì„œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            keywords = self._extract_keywords(query)
            results = []
            
            for doc in self.documents:
                match_score, matched_kw = self._calculate_keyword_match(
                    doc, keywords, case_sensitive
                )
                
                if match_score > 0:
                    snippet = self._generate_snippet(doc, query)
                    
                    result = SearchResult(
                        document=doc,
                        similarity_score=match_score,
                        match_type="keyword",
                        matched_keywords=matched_kw,
                        snippet=snippet
                    )
                    results.append(result)
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            # ìˆœìœ„ í• ë‹¹ ë° ìƒìœ„ kê°œ ì„ íƒ
            for rank, result in enumerate(results[:top_k]):
                result.rank = rank + 1
            
            logger.info(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(results[:top_k])}ê°œ ê²°ê³¼")
            return results[:top_k]
        
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def hybrid_search(
        self,
        query: str,
        top_k: int = 10,
        semantic_weight: float = 0.7,
        keyword_weight: float = 0.3,
        threshold: float = 0.0
    ) -> List[SearchResult]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì˜ë¯¸ì  + í‚¤ì›Œë“œ)"""
        try:
            # ê°ê°ì˜ ê²€ìƒ‰ ìˆ˜í–‰
            semantic_results = self.semantic_search(query, top_k * 2, 0.0)
            keyword_results = self.keyword_search(query, top_k * 2)
            
            # ê²°ê³¼ í†µí•©
            combined_scores = defaultdict(float)
            all_results = {}
            
            # ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
            for result in semantic_results:
                doc_path = result.document.path
                combined_scores[doc_path] += result.similarity_score * semantic_weight
                all_results[doc_path] = result
                all_results[doc_path].match_type = "hybrid"
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬  
            for result in keyword_results:
                doc_path = result.document.path
                combined_scores[doc_path] += result.similarity_score * keyword_weight
                
                if doc_path in all_results:
                    # í‚¤ì›Œë“œ ì •ë³´ ì¶”ê°€
                    all_results[doc_path].matched_keywords = result.matched_keywords
                else:
                    all_results[doc_path] = result
                    all_results[doc_path].match_type = "hybrid"
            
            # í†µí•© ì ìˆ˜ë¡œ ì •ë ¬
            final_results = []
            for doc_path, combined_score in combined_scores.items():
                if combined_score >= threshold:
                    result = all_results[doc_path]
                    result.similarity_score = combined_score
                    final_results.append(result)
            
            final_results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            # ìˆœìœ„ í• ë‹¹
            for rank, result in enumerate(final_results[:top_k]):
                result.rank = rank + 1
            
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results[:top_k])}ê°œ ê²°ê³¼")
            return final_results[:top_k]
        
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def advanced_search(self, search_query: SearchQuery) -> List[SearchResult]:
        """ê³ ê¸‰ ê²€ìƒ‰ (í•„í„°ë§ í¬í•¨)"""
        try:
            # ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
            results = self.hybrid_search(
                search_query.text,
                top_k=100,  # ë„‰ë„‰í•˜ê²Œ ê°€ì ¸ì˜¨ í›„ í•„í„°ë§
                threshold=self.config.get('search', {}).get('similarity_threshold', 0.0)
            )
            
            # í•„í„°ë§ ì ìš©
            filtered_results = self._apply_filters(results, search_query)
            
            logger.info(f"ê³ ê¸‰ ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ê²°ê³¼")
            return filtered_results
        
        except Exception as e:
            logger.error(f"ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _apply_filters(
        self, 
        results: List[SearchResult], 
        query: SearchQuery
    ) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ì— í•„í„° ì ìš©"""
        filtered = results
        
        # íƒœê·¸ í•„í„°
        if query.tags:
            tag_set = set(tag.lower() for tag in query.tags)
            filtered = [
                r for r in filtered
                if any(tag.lower() in tag_set for tag in r.document.tags)
            ]
        
        # ë‚ ì§œ í•„í„°
        if query.date_from:
            filtered = [
                r for r in filtered
                if r.document.modified_at >= query.date_from
            ]
        
        if query.date_to:
            filtered = [
                r for r in filtered  
                if r.document.modified_at <= query.date_to
            ]
        
        # ë‹¨ì–´ ìˆ˜ í•„í„°
        if query.min_word_count:
            filtered = [
                r for r in filtered
                if r.document.word_count >= query.min_word_count
            ]
        
        if query.max_word_count:
            filtered = [
                r for r in filtered
                if r.document.word_count <= query.max_word_count
            ]
        
        # ê²½ë¡œ ì œì™¸ í•„í„°
        if query.exclude_paths:
            exclude_set = set(query.exclude_paths)
            filtered = [
                r for r in filtered
                if r.document.path not in exclude_set
            ]
        
        return filtered
    
    def _extract_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ë¶„ë¦¬ (ê³µë°±, êµ¬ë‘ì  ê¸°ì¤€)
        keywords = re.findall(r'[a-zA-Zê°€-í£0-9]+', query)
        return [kw.lower() for kw in keywords if len(kw) >= 2]
    
    def _calculate_keyword_match(
        self,
        document: Document,
        keywords: List[str],
        case_sensitive: bool = False
    ) -> Tuple[float, List[str]]:
        """ë¬¸ì„œì™€ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        if not keywords:
            return 0.0, []
        
        content = document.content if case_sensitive else document.content.lower()
        title = document.title if case_sensitive else document.title.lower()
        tags = document.tags if case_sensitive else [tag.lower() for tag in document.tags]
        
        matched_keywords = []
        total_score = 0.0
        
        for keyword in keywords:
            kw = keyword if case_sensitive else keyword.lower()
            
            # ì œëª© ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 3.0)
            if kw in title:
                total_score += 3.0
                matched_keywords.append(keyword)
            
            # íƒœê·¸ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 2.0)
            elif any(kw in tag for tag in tags):
                total_score += 2.0
                matched_keywords.append(keyword)
            
            # ë³¸ë¬¸ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 1.0)
            elif kw in content:
                # ë³¸ë¬¸ì—ì„œì˜ ë¹ˆë„ìˆ˜ ê³ ë ¤
                frequency = content.count(kw)
                total_score += min(frequency * 1.0, 5.0)  # ìµœëŒ€ 5ì 
                matched_keywords.append(keyword)
        
        # ì •ê·œí™” (ë§¤ì¹­ëœ í‚¤ì›Œë“œ ë¹„ìœ¨)
        match_ratio = len(matched_keywords) / len(keywords)
        final_score = total_score * match_ratio
        
        return final_score, matched_keywords
    
    def _generate_snippet(self, document: Document, query: str, max_length: int = 150) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí« ìƒì„±"""
        try:
            keywords = self._extract_keywords(query)
            content = document.content
            
            if not keywords:
                return content[:max_length] + "..." if len(content) > max_length else content
            
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°
            sentences = re.split(r'[.!?]\s+', content)
            best_sentence = ""
            best_score = 0
            
            for sentence in sentences:
                score = sum(1 for kw in keywords if kw.lower() in sentence.lower())
                if score > best_score:
                    best_score = score
                    best_sentence = sentence
            
            if best_sentence:
                snippet = best_sentence.strip()
                if len(snippet) > max_length:
                    snippet = snippet[:max_length] + "..."
                return snippet
            
            # ê¸°ë³¸ ìŠ¤ë‹ˆí«
            return content[:max_length] + "..." if len(content) > max_length else content
        
        except Exception as e:
            logger.error(f"ìŠ¤ë‹ˆí« ìƒì„± ì‹¤íŒ¨: {e}")
            return document.content[:max_length] + "..."
    
    def get_search_statistics(self) -> Dict:
        """ê²€ìƒ‰ ì—”ì§„ í†µê³„"""
        try:
            cache_stats = self.cache.get_statistics()
            vault_stats = self.processor.get_vault_statistics()
            
            return {
                "indexed_documents": len(self.documents),
                "embedding_dimension": self.engine.embedding_dimension,
                "model_name": self.engine.model_name,
                "cache_statistics": cache_stats,
                "vault_statistics": vault_stats,
                "indexed": self.indexed
            }
        
        except Exception as e:
            logger.error(f"í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}


def test_search_engine():
    """ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    import tempfile
    import shutil
    
    try:
        # ì„ì‹œ vault ìƒì„±
        temp_vault = tempfile.mkdtemp()
        temp_cache = tempfile.mkdtemp()
        
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤ ìƒì„±
        test_docs = [
            ("tdd.md", "# TDD ê¸°ë³¸ ê°œë…\n\nTDDëŠ” í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.\n## Red-Green-Refactor\ní…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ê³  êµ¬í˜„í•©ë‹ˆë‹¤."),
            ("refactoring.md", "# ë¦¬íŒ©í† ë§\n\në¦¬íŒ©í† ë§ì€ ì½”ë“œì˜ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\ní…ŒìŠ¤íŠ¸ê°€ ìˆì–´ì•¼ ì•ˆì „í•˜ê²Œ ë¦¬íŒ©í† ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
            ("clean_code.md", "# Clean Code\n\nê¹¨ë—í•œ ì½”ë“œ ì‘ì„±ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\nì¢‹ì€ í…ŒìŠ¤íŠ¸ëŠ” ê¹¨ë—í•œ ì½”ë“œì˜ ê¸°ë°˜ì…ë‹ˆë‹¤.")
        ]
        
        for filename, content in test_docs:
            with open(Path(temp_vault) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        config = {
            "model": {"name": "paraphrase-multilingual-mpnet-base-v2"},
            "vault": {"excluded_dirs": [], "file_extensions": [".md"]}
        }
        
        engine = AdvancedSearchEngine(temp_vault, temp_cache, config)
        
        print("ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        if not engine.build_index():
            print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
            return False
        
        # ì˜ë¯¸ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\nğŸ” ì˜ë¯¸ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: 'í…ŒìŠ¤íŠ¸ ë°©ë²•ë¡ '")
        results = engine.semantic_search("í…ŒìŠ¤íŠ¸ ë°©ë²•ë¡ ", top_k=3)
        for result in results:
            print(f"  {result.rank}. {result.document.title} (ìœ ì‚¬ë„: {result.similarity_score:.4f})")
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸  
        print("\nğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: 'TDD ë¦¬íŒ©í† ë§'")
        results = engine.keyword_search("TDD ë¦¬íŒ©í† ë§", top_k=3)
        for result in results:
            print(f"  {result.rank}. {result.document.title} (ì ìˆ˜: {result.similarity_score:.2f}) - {result.matched_keywords}")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\nğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: 'í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±'")
        results = engine.hybrid_search("í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±", top_k=3)
        for result in results:
            print(f"  {result.rank}. {result.document.title} (ì ìˆ˜: {result.similarity_score:.4f})")
        
        # í†µê³„ í™•ì¸
        stats = engine.get_search_statistics()
        print(f"\nğŸ“Š í†µê³„: {stats['indexed_documents']}ê°œ ë¬¸ì„œ, {stats['embedding_dimension']}ì°¨ì›")
        
        # ì •ë¦¬
        shutil.rmtree(temp_vault)
        shutil.rmtree(temp_cache)
        
        print("âœ… ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_search_engine()