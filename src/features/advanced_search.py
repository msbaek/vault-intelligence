#!/usr/bin/env python3
"""
Advanced Search Engine for Vault Intelligence System V2

Sentence Transformers ê¸°ë°˜ ì˜ë¯¸ì  ê²€ìƒ‰ ë° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„
"""

import os
import re
import logging
from typing import List, Dict, Optional, Tuple, Union
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
import numpy as np
from collections import defaultdict

try:
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

from ..core.sentence_transformer_engine import SentenceTransformerEngine
from ..core.embedding_cache import EmbeddingCache, CachedEmbedding
from ..core.vault_processor import VaultProcessor, Document

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼"""
    document: Document
    similarity_score: float
    match_type: str  # "semantic", "keyword", "hybrid"
    matched_keywords: List[str] = None
    snippet: str = ""
    rank: int = 0


@dataclass 
class SearchQuery:
    """ê²€ìƒ‰ ì¿¼ë¦¬"""
    text: str
    tags: List[str] = None
    date_from: Optional[datetime] = None
    date_to: Optional[datetime] = None
    min_word_count: Optional[int] = None
    max_word_count: Optional[int] = None
    exclude_paths: List[str] = None


class AdvancedSearchEngine:
    """ê³ ê¸‰ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(
        self,
        vault_path: str,
        cache_dir: str,
        config: Dict = None
    ):
        """
        Args:
            vault_path: Vault ê²½ë¡œ
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
            config: ê²€ìƒ‰ ì„¤ì •
        """
        self.vault_path = Path(vault_path)
        self.config = config or {}
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ì„±ëŠ¥ ìµœì í™” ì„¤ì •)
        self.engine = SentenceTransformerEngine(
            model_name=self.config.get('model', {}).get('name', 'BAAI/bge-m3'),
            cache_dir=self.config.get('model', {}).get('cache_folder', 'models'),
            device=self.config.get('model', {}).get('device'),
            use_fp16=self.config.get('model', {}).get('use_fp16', False),
            batch_size=self.config.get('model', {}).get('batch_size', 4),
            max_length=self.config.get('model', {}).get('max_length', 4096),
            num_workers=self.config.get('model', {}).get('num_workers', 6)
        )
        
        self.cache = EmbeddingCache(cache_dir)
        
        self.processor = VaultProcessor(
            str(vault_path),
            excluded_dirs=self.config.get('vault', {}).get('excluded_dirs'),
            excluded_files=self.config.get('vault', {}).get('excluded_files'),
            file_extensions=self.config.get('vault', {}).get('file_extensions'),
            include_folders=self.config.get('vault', {}).get('include_folders'),
            exclude_folders=self.config.get('vault', {}).get('exclude_folders')
        )
        
        # ë¬¸ì„œ ë° ì„ë² ë”© ìºì‹œ
        self.documents: List[Document] = []
        self.embeddings: Optional[np.ndarray] = None
        self.indexed = False
        self.is_sampled = False
        self.sample_size = None
        
        logger.info(f"ê³ ê¸‰ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”: {vault_path}")
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ìë™ ë¡œë“œ ì‹œë„
        self.load_index()
    
    def build_index(self, force_rebuild: bool = False, progress_callback=None, sample_size: Optional[int] = None) -> bool:
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•
        
        Args:
            force_rebuild: ê°•ì œ ì¬êµ¬ì¶• ì—¬ë¶€
            progress_callback: ì§„í–‰ë¥  ì½œë°±
            sample_size: ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ì „ì²´ ì²˜ë¦¬)
        """
        try:
            logger.info("ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘...")
            
            # ë¬¸ì„œ ì²˜ë¦¬
            self.documents = self.processor.process_all_files(progress_callback)
            logger.info(f"ì²˜ë¦¬ëœ ë¬¸ì„œ: {len(self.documents)}ê°œ")
            
            if not self.documents:
                logger.warning("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ëŒ€ê·œëª¨ vaultì— ëŒ€í•œ ìë™ ìƒ˜í”Œë§ ê¶Œì¥
            if sample_size is None and len(self.documents) > 1000:
                recommended_size = min(500, len(self.documents) // 2)
                logger.warning(f"âš ï¸  ëŒ€ê·œëª¨ vault ê°ì§€ ({len(self.documents)}ê°œ ë¬¸ì„œ)")
                logger.warning(f"ğŸ“Š ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ --sample-size {recommended_size} ì˜µì…˜ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤")
            
            # BGE-M3 ì„ë² ë”© ì—”ì§„ í›ˆë ¨ (ìƒ˜í”Œë§ ì§€ì›)
            all_contents = [doc.content for doc in self.documents]
            all_paths = [doc.path for doc in self.documents]
            self.engine.fit_documents(all_contents, all_paths, sample_size=sample_size)
            logger.info("BGE-M3 ì„ë² ë”© ì—”ì§„ í›ˆë ¨ ì™„ë£Œ")
            
            # ìƒ˜í”Œë§ ëª¨ë“œì¼ ë•ŒëŠ” BGE-M3 ì—”ì§„ì˜ ì„ë² ë”©ì„ ì§ì ‘ ì‚¬ìš©
            if sample_size and sample_size < len(self.documents):
                logger.info("ğŸ“Š ìƒ˜í”Œë§ ëª¨ë“œ: BGE-M3 ì—”ì§„ì˜ ì„ë² ë”©ì„ ì§ì ‘ ì‚¬ìš©")
                embeddings_list = []
                
                # BGE-M3 ì—”ì§„ì—ì„œ ìƒ˜í”Œë§ëœ ì¸ë±ìŠ¤ ê³„ì‚°
                step = len(self.documents) // sample_size
                sample_indices = list(range(0, len(self.documents), step))[:sample_size]
                
                # ìƒ˜í”Œë§ëœ ë¬¸ì„œë“¤ë§Œ ì„ íƒ
                sampled_documents = [self.documents[i] for i in sample_indices]
                
                # BGE-M3 ì—”ì§„ì—ì„œ ìƒì„±ëœ ì„ë² ë”© ì§ì ‘ ì‚¬ìš©
                if hasattr(self.engine, 'dense_embeddings') and self.engine.dense_embeddings is not None:
                    for i, doc in enumerate(sampled_documents):
                        if i < len(self.engine.dense_embeddings):
                            doc.embedding = self.engine.dense_embeddings[i]
                            embeddings_list.append(self.engine.dense_embeddings[i])
                        else:
                            # í´ë°±: ì œë¡œ ë²¡í„°
                            doc.embedding = np.zeros(self.engine.embedding_dimension)
                            embeddings_list.append(doc.embedding)
                    
                    self.documents = sampled_documents
                    self.embeddings = np.array(embeddings_list)
                    self.indexed = True
                    self.is_sampled = True
                    self.sample_size = len(sampled_documents)
                    
                    logger.info(f"âœ… ìƒ˜í”Œë§ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(sampled_documents)}ê°œ ë¬¸ì„œ")
                    
                    # ìƒ˜í”Œë§ ì¸ë±ìŠ¤ ì €ì¥
                    self.save_index()
                    
                    return True
            
            # ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
            embeddings_list = []
            cache_hits = 0
            new_embeddings = 0
            
            for i, doc in enumerate(self.documents):
                try:
                    # ìºì‹œì—ì„œ ì„ë² ë”© ì¡°íšŒ
                    cached = self.cache.get_embedding(
                        str(self.vault_path / doc.path), 
                        doc.file_hash
                    )
                    
                    if cached and not force_rebuild:
                        # ìºì‹œëœ ì„ë² ë”© ê²€ì¦
                        if (isinstance(cached.embedding, np.ndarray) and 
                            cached.embedding.size > 0 and 
                            not np.allclose(cached.embedding, 0)):
                            embeddings_list.append(cached.embedding)
                            doc.embedding = cached.embedding
                            cache_hits += 1
                        else:
                            logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ìºì‹œ ì„ë² ë”©: {doc.path}")
                            # ìƒˆë¡œ ìƒì„±
                            embedding = self.engine.encode_text(doc.content)
                            embeddings_list.append(embedding)
                            doc.embedding = embedding
                            self.cache.store_embedding(
                                str(self.vault_path / doc.path),
                                embedding,
                                self.engine.model_name,
                                doc.word_count
                            )
                            new_embeddings += 1
                    else:
                        # ìƒˆ ì„ë² ë”© ìƒì„±
                        if doc.content and doc.content.strip():
                            embedding = self.engine.encode_text(doc.content)
                            if embedding is not None and not np.allclose(embedding, 0):
                                embeddings_list.append(embedding)
                                doc.embedding = embedding
                                
                                # ìºì‹œì— ì €ì¥
                                self.cache.store_embedding(
                                    str(self.vault_path / doc.path),
                                    embedding,
                                    self.engine.model_name,
                                    doc.word_count
                                )
                                new_embeddings += 1
                            else:
                                logger.warning(f"0ì¸ ì„ë² ë”© ìƒì„±ë¨: {doc.path}")
                                empty_embedding = np.zeros(self.engine.embedding_dimension)
                                embeddings_list.append(empty_embedding)
                                doc.embedding = empty_embedding
                        else:
                            logger.warning(f"ë¹ˆ ë‚´ìš© ë¬¸ì„œ: {doc.path}")
                            empty_embedding = np.zeros(self.engine.embedding_dimension)
                            embeddings_list.append(empty_embedding)
                            doc.embedding = empty_embedding
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback and (i + 1) % 50 == 0:
                        progress_callback(i + 1, len(self.documents))
                
                except Exception as e:
                    logger.error(f"ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {doc.path}, {e}")
                    # ë¹ˆ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´
                    empty_embedding = np.zeros(self.engine.embedding_dimension)
                    embeddings_list.append(empty_embedding)
                    doc.embedding = empty_embedding
            
            # ì„ë² ë”© ë°°ì—´ ìƒì„±
            if embeddings_list:
                self.embeddings = np.array(embeddings_list)
                self.indexed = True
                
                logger.info(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ:")
                logger.info(f"- ë¬¸ì„œ: {len(self.documents)}ê°œ")
                logger.info(f"- ìºì‹œ íˆíŠ¸: {cache_hits}ê°œ")
                logger.info(f"- ì‹ ê·œ ì„ë² ë”©: {new_embeddings}ê°œ")
                logger.info(f"- ì„ë² ë”© í˜•íƒœ: {self.embeddings.shape}")
                
                # ì¸ë±ìŠ¤ ì €ì¥
                self.save_index()
                
                return True
            
            return False
        
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            return False
    
    def load_index(self) -> bool:
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            # ìƒ˜í”Œë§ ë©”íƒ€ë°ì´í„° í™•ì¸
            metadata_path = os.path.join(self.cache.cache_dir, "index_metadata.json")
            if os.path.exists(metadata_path):
                import json
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    index_metadata = json.load(f)
                
                if index_metadata.get('is_sampled', False):
                    logger.info(f"ğŸ“Š ì´ì „ ìƒ˜í”Œë§ ì¸ë±ìŠ¤ ë°œê²¬: {index_metadata.get('sample_size', 'unknown')}ê°œ ë¬¸ì„œ")
                    logger.info("ğŸ’¡ ìƒ˜í”Œë§ ì¸ë±ìŠ¤ëŠ” ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ìš©ì…ë‹ˆë‹¤.")
                    self.is_sampled = True
                    self.sample_size = index_metadata.get('sample_size')
                else:
                    self.is_sampled = False
                    self.sample_size = None
            
            # ìºì‹œëœ ì„ë² ë”© í™œìš©í•˜ì—¬ ì¸ë±ìŠ¤ ë³µì›
            logger.info("ğŸ“‚ ìºì‹œëœ ì„ë² ë”©ìœ¼ë¡œ ì¸ë±ìŠ¤ ë³µì› ì‹œë„...")
            
            # ë¬¸ì„œ ë¡œë“œ
            self.documents = self.processor.process_all_files()
            if not self.documents:
                logger.warning("ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ìºì‹œëœ ì„ë² ë”©ê³¼ ëˆ„ë½ ë¬¸ì„œ ë¶„ë¦¬
            cached_embeddings = []
            missing_docs = []
            cached_docs = []
            
            for doc in self.documents:
                cached = self.cache.get_embedding(doc.path)
                if cached is None:
                    missing_docs.append(doc)
                else:
                    cached_docs.append(doc)
                    cached_embeddings.append(cached.embedding)
            
            logger.info(f"ğŸ“Š ìºì‹œ ìƒíƒœ: {len(cached_docs)}ê°œ ìˆìŒ, {len(missing_docs)}ê°œ ëˆ„ë½")
            
            # ëˆ„ë½ ë¬¸ì„œê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì „ì²´ ì¬êµ¬ì¶•
            if len(missing_docs) > len(self.documents) * 0.1:  # 10% ì´ìƒ ëˆ„ë½
                logger.warning(f"âš ï¸  ëˆ„ë½ ë¬¸ì„œê°€ ë§ì•„ ì „ì²´ ì¬êµ¬ì¶• í•„ìš”: {len(missing_docs)}ê°œ")
                return False
            
            # ëˆ„ë½ ë¬¸ì„œë§Œ ì„ë² ë”© ìƒì„±
            if missing_docs:
                logger.info(f"ğŸ”„ ëˆ„ë½ëœ {len(missing_docs)}ê°œ ë¬¸ì„œë§Œ ì„ë² ë”© ìƒì„±...")
                missing_texts = [doc.content for doc in missing_docs]
                missing_embeddings = self.engine.encode_texts(missing_texts)
                
                # ìºì‹œì— ì €ì¥
                for doc, embedding in zip(missing_docs, missing_embeddings):
                    # cache.store_embeddingì„ ì§ì ‘ í˜¸ì¶œ
                    self.cache.store_embedding(
                        file_path=doc.path,
                        embedding=embedding,
                        model_name=self.engine.model_name,
                        word_count=doc.word_count
                    )
                
                # ëª¨ë“  ì„ë² ë”© í†µí•©
                all_embeddings = cached_embeddings + missing_embeddings.tolist()
                all_documents = cached_docs + missing_docs
            else:
                all_embeddings = cached_embeddings
                all_documents = cached_docs
            
            # ì„ë² ë”© ë°°ì—´ êµ¬ì„±
            self.embeddings = np.array(all_embeddings)
            self.documents = all_documents
            
            # BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• (ë¹ ë¦„)
            from rank_bm25 import BM25Okapi
            tokenized_docs = [doc.content.split() for doc in self.documents]
            self.bm25 = BM25Okapi(tokenized_docs)
            
            self.indexed = True
            logger.info(f"âœ… ì ì§„ì  ì¸ë±ìŠ¤ ë³µì› ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ ({len(missing_docs)}ê°œ ìƒˆë¡œ ì¶”ê°€)")
            return True
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def save_index(self) -> bool:
        """ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            if not self.indexed:
                logger.warning("ì €ì¥í•  ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # BGE-M3 ëª¨ë¸ ì €ì¥ (BGE-M3ëŠ” ìë™ìœ¼ë¡œ ìºì‹œë¨)
            # ìƒ˜í”Œë§ ë©”íƒ€ë°ì´í„° ì €ì¥
            index_metadata = {
                'is_sampled': getattr(self, 'is_sampled', False),
                'sample_size': getattr(self, 'sample_size', None),
                'total_documents': len(self.documents),
                'embedding_dimension': self.engine.embedding_dimension,
                'model_name': self.engine.model_name
            }
            
            metadata_path = os.path.join(self.cache.cache_dir, "index_metadata.json")
            import json
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(index_metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ - ìƒ˜í”Œë§: {index_metadata['is_sampled']}, ë¬¸ì„œ: {index_metadata['total_documents']}ê°œ")
            return True
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def semantic_search(
        self,
        query: str,
        top_k: int = 10,
        threshold: float = 0.0
    ) -> List[SearchResult]:
        """ì˜ë¯¸ì  ê²€ìƒ‰"""
        if not self.indexed:
            # ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            if not self.load_index():
                logger.warning("ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.engine.encode_text(query)
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self.engine.calculate_similarities(
                query_embedding, 
                self.embeddings
            )
            
            # ìƒìœ„ ê²°ê³¼ ì„ íƒ
            results = self.engine.find_most_similar(
                query_embedding,
                self.embeddings,
                top_k=min(top_k, len(self.documents))
            )
            
            # SearchResult ê°ì²´ ìƒì„±
            search_results = []
            for rank, (idx, similarity) in enumerate(results):
                if similarity >= threshold:
                    snippet = self._generate_snippet(self.documents[idx], query)
                    
                    result = SearchResult(
                        document=self.documents[idx],
                        similarity_score=similarity,
                        match_type="semantic",
                        snippet=snippet,
                        rank=rank + 1
                    )
                    search_results.append(result)
            
            logger.info(f"ì˜ë¯¸ì  ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
        
        except Exception as e:
            logger.error(f"ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def keyword_search(
        self,
        query: str,
        top_k: int = 10,
        case_sensitive: bool = False
    ) -> List[SearchResult]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰"""
        if not self.documents:
            logger.warning("ë¬¸ì„œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            keywords = self._extract_keywords(query)
            results = []
            
            for doc in self.documents:
                match_score, matched_kw = self._calculate_keyword_match(
                    doc, keywords, case_sensitive
                )
                
                if match_score > 0:
                    snippet = self._generate_snippet(doc, query)
                    
                    result = SearchResult(
                        document=doc,
                        similarity_score=match_score,
                        match_type="keyword",
                        matched_keywords=matched_kw,
                        snippet=snippet
                    )
                    results.append(result)
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            # ìˆœìœ„ í• ë‹¹ ë° ìƒìœ„ kê°œ ì„ íƒ
            for rank, result in enumerate(results[:top_k]):
                result.rank = rank + 1
            
            logger.info(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(results[:top_k])}ê°œ ê²°ê³¼")
            return results[:top_k]
        
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def hybrid_search(
        self,
        query: str,
        top_k: int = 10,
        semantic_weight: float = 0.7,
        keyword_weight: float = 0.3,
        threshold: float = 0.0
    ) -> List[SearchResult]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì˜ë¯¸ì  + í‚¤ì›Œë“œ)"""
        try:
            # ê°ê°ì˜ ê²€ìƒ‰ ìˆ˜í–‰
            semantic_results = self.semantic_search(query, top_k * 2, 0.0)
            keyword_results = self.keyword_search(query, top_k * 2)
            
            # ê²°ê³¼ í†µí•©
            combined_scores = defaultdict(float)
            all_results = {}
            
            # ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
            for result in semantic_results:
                doc_path = result.document.path
                combined_scores[doc_path] += result.similarity_score * semantic_weight
                all_results[doc_path] = result
                all_results[doc_path].match_type = "hybrid"
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬  
            for result in keyword_results:
                doc_path = result.document.path
                combined_scores[doc_path] += result.similarity_score * keyword_weight
                
                if doc_path in all_results:
                    # í‚¤ì›Œë“œ ì •ë³´ ì¶”ê°€
                    all_results[doc_path].matched_keywords = result.matched_keywords
                else:
                    all_results[doc_path] = result
                    all_results[doc_path].match_type = "hybrid"
            
            # í†µí•© ì ìˆ˜ë¡œ ì •ë ¬
            final_results = []
            for doc_path, combined_score in combined_scores.items():
                if combined_score >= threshold:
                    result = all_results[doc_path]
                    result.similarity_score = combined_score
                    final_results.append(result)
            
            final_results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            # ìˆœìœ„ í• ë‹¹
            for rank, result in enumerate(final_results[:top_k]):
                result.rank = rank + 1
            
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results[:top_k])}ê°œ ê²°ê³¼")
            return final_results[:top_k]
        
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def advanced_search(self, search_query: SearchQuery) -> List[SearchResult]:
        """ê³ ê¸‰ ê²€ìƒ‰ (í•„í„°ë§ í¬í•¨)"""
        try:
            # ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
            results = self.hybrid_search(
                search_query.text,
                top_k=100,  # ë„‰ë„‰í•˜ê²Œ ê°€ì ¸ì˜¨ í›„ í•„í„°ë§
                threshold=self.config.get('search', {}).get('similarity_threshold', 0.0)
            )
            
            # í•„í„°ë§ ì ìš©
            filtered_results = self._apply_filters(results, search_query)
            
            logger.info(f"ê³ ê¸‰ ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results)}ê°œ ê²°ê³¼")
            return filtered_results
        
        except Exception as e:
            logger.error(f"ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _apply_filters(
        self, 
        results: List[SearchResult], 
        query: SearchQuery
    ) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ì— í•„í„° ì ìš©"""
        filtered = results
        
        # íƒœê·¸ í•„í„°
        if query.tags:
            tag_set = set(tag.lower() for tag in query.tags)
            filtered = [
                r for r in filtered
                if any(tag.lower() in tag_set for tag in r.document.tags)
            ]
        
        # ë‚ ì§œ í•„í„°
        if query.date_from:
            filtered = [
                r for r in filtered
                if r.document.modified_at >= query.date_from
            ]
        
        if query.date_to:
            filtered = [
                r for r in filtered  
                if r.document.modified_at <= query.date_to
            ]
        
        # ë‹¨ì–´ ìˆ˜ í•„í„°
        if query.min_word_count:
            filtered = [
                r for r in filtered
                if r.document.word_count >= query.min_word_count
            ]
        
        if query.max_word_count:
            filtered = [
                r for r in filtered
                if r.document.word_count <= query.max_word_count
            ]
        
        # ê²½ë¡œ ì œì™¸ í•„í„°
        if query.exclude_paths:
            exclude_set = set(query.exclude_paths)
            filtered = [
                r for r in filtered
                if r.document.path not in exclude_set
            ]
        
        return filtered
    
    def _extract_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ë¶„ë¦¬ (ê³µë°±, êµ¬ë‘ì  ê¸°ì¤€)
        keywords = re.findall(r'[a-zA-Zê°€-í£0-9]+', query)
        return [kw.lower() for kw in keywords if len(kw) >= 2]
    
    def _calculate_keyword_match(
        self,
        document: Document,
        keywords: List[str],
        case_sensitive: bool = False
    ) -> Tuple[float, List[str]]:
        """ë¬¸ì„œì™€ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        if not keywords:
            return 0.0, []
        
        content = document.content if case_sensitive else document.content.lower()
        title = document.title if case_sensitive else document.title.lower()
        tags = document.tags if case_sensitive else [tag.lower() for tag in document.tags]
        
        matched_keywords = []
        total_score = 0.0
        
        for keyword in keywords:
            kw = keyword if case_sensitive else keyword.lower()
            
            # ì œëª© ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 3.0)
            if kw in title:
                total_score += 3.0
                matched_keywords.append(keyword)
            
            # íƒœê·¸ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 2.0)
            elif any(kw in tag for tag in tags):
                total_score += 2.0
                matched_keywords.append(keyword)
            
            # ë³¸ë¬¸ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 1.0)
            elif kw in content:
                # ë³¸ë¬¸ì—ì„œì˜ ë¹ˆë„ìˆ˜ ê³ ë ¤
                frequency = content.count(kw)
                total_score += min(frequency * 1.0, 5.0)  # ìµœëŒ€ 5ì 
                matched_keywords.append(keyword)
        
        # ì •ê·œí™” (ë§¤ì¹­ëœ í‚¤ì›Œë“œ ë¹„ìœ¨)
        match_ratio = len(matched_keywords) / len(keywords)
        final_score = total_score * match_ratio
        
        return final_score, matched_keywords
    
    def _generate_snippet(self, document: Document, query: str, max_length: int = 150) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí« ìƒì„±"""
        try:
            keywords = self._extract_keywords(query)
            content = document.content
            
            if not keywords:
                return content[:max_length] + "..." if len(content) > max_length else content
            
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°
            sentences = re.split(r'[.!?]\s+', content)
            best_sentence = ""
            best_score = 0
            
            for sentence in sentences:
                score = sum(1 for kw in keywords if kw.lower() in sentence.lower())
                if score > best_score:
                    best_score = score
                    best_sentence = sentence
            
            if best_sentence:
                snippet = best_sentence.strip()
                if len(snippet) > max_length:
                    snippet = snippet[:max_length] + "..."
                return snippet
            
            # ê¸°ë³¸ ìŠ¤ë‹ˆí«
            return content[:max_length] + "..." if len(content) > max_length else content
        
        except Exception as e:
            logger.error(f"ìŠ¤ë‹ˆí« ìƒì„± ì‹¤íŒ¨: {e}")
            return document.content[:max_length] + "..."
    
    def get_search_statistics(self) -> Dict:
        """ê²€ìƒ‰ ì—”ì§„ í†µê³„"""
        try:
            cache_stats = self.cache.get_statistics()
            vault_stats = self.processor.get_vault_statistics()
            
            return {
                "indexed_documents": len(self.documents),
                "embedding_dimension": self.engine.embedding_dimension,
                "model_name": self.engine.model_name,
                "cache_statistics": cache_stats,
                "vault_statistics": vault_stats,
                "indexed": self.indexed
            }
        
        except Exception as e:
            logger.error(f"í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def search_with_reranking(
        self,
        query: str,
        search_method: str = "hybrid",
        initial_k: int = 100,
        final_k: int = 10,
        threshold: float = 0.0,
        use_reranker: bool = True,
        **search_kwargs
    ) -> List[SearchResult]:
        """
        ì¬ìˆœìœ„í™”ë¥¼ í¬í•¨í•œ ê³ ê¸‰ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            search_method: ê²€ìƒ‰ ë°©ë²• ("semantic", "keyword", "hybrid")
            initial_k: 1ì°¨ ê²€ìƒ‰ì—ì„œ ê°€ì ¸ì˜¬ í›„ë³´ ìˆ˜
            final_k: ìµœì¢… ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            use_reranker: ì¬ìˆœìœ„í™” ì‚¬ìš© ì—¬ë¶€
            **search_kwargs: ì¶”ê°€ ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜
            
        Returns:
            ì¬ìˆœìœ„í™”ëœ ê²€ìƒ‰ ê²°ê³¼ (SearchResult í˜•íƒœë¡œ ë³€í™˜)
        """
        # Rerankerê°€ ìš”ì²­ë˜ì—ˆì§€ë§Œ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
        if use_reranker:
            try:
                from .reranker import BGEReranker, RerankerPipeline
                
                # ì„¤ì •ì—ì„œ reranker ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                reranker_config = self.config.get('reranker', {})
                
                # Reranker ì´ˆê¸°í™”
                reranker = BGEReranker(
                    model_name=reranker_config.get('model_name', 'BAAI/bge-reranker-v2-m3'),
                    use_fp16=reranker_config.get('use_fp16', True),
                    cache_folder=reranker_config.get('cache_folder', self.config.get('model', {}).get('cache_folder')),
                    device=reranker_config.get('device', self.config.get('model', {}).get('device'))
                )
                
                if reranker.is_available():
                    # íŒŒì´í”„ë¼ì¸ ìƒì„± ë° ì‹¤í–‰
                    pipeline = RerankerPipeline(self, reranker, self.config)
                    rerank_results = pipeline.search_and_rerank(
                        query=query,
                        search_method=search_method,
                        initial_k=initial_k,
                        final_k=final_k,
                        similarity_threshold=threshold,
                        **search_kwargs
                    )
                    
                    # RerankResultë¥¼ SearchResultë¡œ ë³€í™˜
                    search_results = []
                    for rerank_result in rerank_results:
                        # ì›ë³¸ SearchResultë¥¼ ë³µì‚¬í•˜ê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
                        search_result = rerank_result.search_result
                        search_result.similarity_score = rerank_result.rerank_score
                        search_result.rank = rerank_result.new_rank + 1
                        search_result.match_type = f"{search_result.match_type}_reranked"
                        search_results.append(search_result)
                    
                    logger.info(f"ì¬ìˆœìœ„í™” ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
                    return search_results
                else:
                    logger.warning("Rerankerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                    use_reranker = False
                    
            except ImportError:
                logger.warning("Reranker ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                use_reranker = False
            except Exception as e:
                logger.warning(f"Reranker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                use_reranker = False
        
        # ì¼ë°˜ ê²€ìƒ‰ ìˆ˜í–‰ (reranker ì—†ì´)
        if search_method == "semantic":
            return self.semantic_search(query, top_k=final_k, threshold=threshold, **search_kwargs)
        elif search_method == "keyword":
            return self.keyword_search(query, top_k=final_k, **search_kwargs)
        elif search_method == "hybrid":
            return self.hybrid_search(query, top_k=final_k, threshold=threshold, **search_kwargs)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ë²•: {search_method}")
    
    def colbert_search(
        self,
        query: str,
        top_k: int = 10,
        threshold: float = 0.0
    ) -> List[SearchResult]:
        """
        ColBERT ê¸°ë°˜ í† í° ìˆ˜ì¤€ late interaction ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ColBERT ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            from .colbert_search import ColBERTSearchEngine
            
            # ColBERT ì—”ì§„ ì„¤ì •
            colbert_config = self.config.get('colbert', {})
            
            # ColBERT ì—”ì§„ ì´ˆê¸°í™” (ìºì‹œ í¬í•¨)
            colbert_engine = ColBERTSearchEngine(
                model_name=colbert_config.get('model_name', 'BAAI/bge-m3'),
                device=colbert_config.get('device', self.config.get('model', {}).get('device')),
                use_fp16=colbert_config.get('use_fp16', True),
                cache_folder=colbert_config.get('cache_folder', self.config.get('model', {}).get('cache_folder')),
                max_length=colbert_config.get('max_length', self.config.get('model', {}).get('max_length', 4096)),
                cache_dir=self.cache_dir,
                enable_cache=colbert_config.get('enable_cache', True)
            )
            
            if not colbert_engine.is_available():
                logger.warning("ColBERT ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ë¯¸ì  ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                return self.semantic_search(query, top_k, threshold)
            
            # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ êµ¬ì¶• (ìºì‹œë¥¼ í™œìš©í•˜ì—¬ ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ ê°€ëŠ¥)
            if not colbert_engine.is_indexed:
                logger.info("ColBERT ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
                max_docs = colbert_config.get('max_documents', None)  # Noneì´ë©´ ì „ì²´ ë¬¸ì„œ
                force_rebuild = False  # ê¸°ë³¸ì ìœ¼ë¡œ ìºì‹œ í™œìš©
                
                if not colbert_engine.build_index(
                    self.documents, 
                    max_documents=max_docs,
                    force_rebuild=force_rebuild
                ):
                    logger.error("ColBERT ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
                    return self.semantic_search(query, top_k, threshold)
            
            # ColBERT ê²€ìƒ‰ ìˆ˜í–‰
            colbert_results = colbert_engine.search(query, top_k, threshold)
            
            # SearchResult í˜•íƒœë¡œ ë³€í™˜
            search_results = colbert_engine.convert_to_search_results(colbert_results)
            
            logger.info(f"ColBERT ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
            
        except ImportError:
            logger.warning("ColBERT ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ë¯¸ì  ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return self.semantic_search(query, top_k, threshold)
        except Exception as e:
            logger.error(f"ColBERT ê²€ìƒ‰ ì‹¤íŒ¨: {e}. ì˜ë¯¸ì  ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return self.semantic_search(query, top_k, threshold)
    
    def expanded_search(
        self,
        query: str,
        search_method: str = "hybrid",
        top_k: int = 10,
        threshold: float = 0.0,
        include_synonyms: bool = True,
        include_hyde: bool = True,
        **search_kwargs
    ) -> List[SearchResult]:
        """
        ì¿¼ë¦¬ í™•ì¥ì„ í¬í•¨í•œ ê³ ê¸‰ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            search_method: ê²€ìƒ‰ ë°©ë²• ("semantic", "keyword", "hybrid", "colbert")
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            include_synonyms: ë™ì˜ì–´ í¬í•¨ ì—¬ë¶€
            include_hyde: HyDE í¬í•¨ ì—¬ë¶€
            **search_kwargs: ì¶”ê°€ ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜
            
        Returns:
            í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•œ ê²°ê³¼
        """
        try:
            from .query_expansion import QueryExpansionEngine
            
            # ì¿¼ë¦¬ í™•ì¥ ì„¤ì •
            expansion_config = self.config.get('query_expansion', {})
            
            # ì¿¼ë¦¬ í™•ì¥ ì—”ì§„ ì´ˆê¸°í™”
            expansion_engine = QueryExpansionEngine(
                model_name=expansion_config.get('model_name', 'BAAI/bge-m3'),
                device=expansion_config.get('device', self.config.get('model', {}).get('device')),
                use_fp16=expansion_config.get('use_fp16', True),
                enable_hyde=expansion_config.get('enable_hyde', True)
            )
            
            # ì¿¼ë¦¬ í™•ì¥ ì‹¤í–‰
            expanded_query = expansion_engine.expand_query(
                query=query,
                include_synonyms=include_synonyms,
                include_hyde=include_hyde,
                max_synonyms=expansion_config.get('max_synonyms', 3)
            )
            
            logger.info(f"ì¿¼ë¦¬ í™•ì¥ ì™„ë£Œ: {expanded_query.expansion_method}")
            
            # ì—¬ëŸ¬ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            search_queries = expansion_engine.create_expanded_search_queries(expanded_query)
            
            # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ìˆ˜í–‰ ë° ê²°ê³¼ í†µí•©
            all_results = []
            seen_docs = set()  # ì¤‘ë³µ ë¬¸ì„œ ì œê±°ìš©
            
            for i, search_query in enumerate(search_queries):
                try:
                    # ê° ì¿¼ë¦¬ë³„ ê°€ì¤‘ì¹˜ (ì›ë³¸ ì¿¼ë¦¬ê°€ ê°€ì¥ ë†’ìŒ)
                    weight = 1.0 - (i * 0.1)  # 0.9, 0.8, 0.7, ...
                    weight = max(weight, 0.3)  # ìµœì†Œ 0.3
                    
                    # ê²€ìƒ‰ ì‹¤í–‰
                    if search_method == "semantic":
                        results = self.semantic_search(search_query, top_k * 2, threshold, **search_kwargs)
                    elif search_method == "keyword":
                        results = self.keyword_search(search_query, top_k * 2, **search_kwargs)
                    elif search_method == "hybrid":
                        results = self.hybrid_search(search_query, top_k * 2, threshold, **search_kwargs)
                    elif search_method == "colbert":
                        results = self.colbert_search(search_query, top_k * 2, threshold, **search_kwargs)
                    else:
                        continue
                    
                    # ê²°ê³¼ ê°€ì¤‘ì¹˜ ì ìš© ë° ì¤‘ë³µ ì œê±°
                    for result in results:
                        doc_id = result.document.path
                        if doc_id not in seen_docs:
                            # ì ìˆ˜ì— ê°€ì¤‘ì¹˜ ì ìš©
                            result.similarity_score *= weight
                            result.match_type = f"{result.match_type}_expanded"
                            
                            # í™•ì¥ ì •ë³´ ì¶”ê°€
                            if i == 0:
                                result.match_type += "_original"
                            elif search_query in expanded_query.synonyms:
                                result.match_type += "_synonym"
                            elif search_query == expanded_query.hypothetical_doc:
                                result.match_type += "_hyde"
                            else:
                                result.match_type += "_related"
                            
                            all_results.append(result)
                            seen_docs.add(doc_id)
                
                except Exception as e:
                    logger.warning(f"í™•ì¥ ì¿¼ë¦¬ '{search_query[:50]}...' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            # í†µí•© ê²°ê³¼ ì •ë ¬ ë° ìƒìœ„ Kê°œ ì„ íƒ
            all_results.sort(key=lambda x: x.similarity_score, reverse=True)
            final_results = all_results[:top_k]
            
            # ìˆœìœ„ ì¬í• ë‹¹
            for rank, result in enumerate(final_results):
                result.rank = rank + 1
            
            logger.info(f"í™•ì¥ ê²€ìƒ‰ ì™„ë£Œ: {len(search_queries)}ê°œ ì¿¼ë¦¬ë¡œ {len(final_results)}ê°œ ê²°ê³¼")
            
            return final_results
            
        except ImportError:
            logger.warning("ì¿¼ë¦¬ í™•ì¥ ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            # í´ë°±: ì¼ë°˜ ê²€ìƒ‰
            if search_method == "semantic":
                return self.semantic_search(query, top_k, threshold, **search_kwargs)
            elif search_method == "keyword":
                return self.keyword_search(query, top_k, **search_kwargs)
            elif search_method == "hybrid":
                return self.hybrid_search(query, top_k, threshold, **search_kwargs)
            elif search_method == "colbert":
                return self.colbert_search(query, top_k, threshold, **search_kwargs)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ë²•: {search_method}")
        except Exception as e:
            logger.error(f"í™•ì¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}. ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            # í´ë°±: ì¼ë°˜ ê²€ìƒ‰
            if search_method == "semantic":
                return self.semantic_search(query, top_k, threshold, **search_kwargs)
            elif search_method == "keyword":
                return self.keyword_search(query, top_k, **search_kwargs)
            elif search_method == "hybrid":
                return self.hybrid_search(query, top_k, threshold, **search_kwargs)
            elif search_method == "colbert":
                return self.colbert_search(query, top_k, threshold, **search_kwargs)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ë²•: {search_method}")
    
    def get_related_documents(
        self,
        document_path: str,
        top_k: int = 5,
        include_centrality_boost: bool = True,
        similarity_threshold: float = 0.3
    ) -> List[SearchResult]:
        """
        íŠ¹ì • ë¬¸ì„œì™€ ê´€ë ¨ëœ ë¬¸ì„œë“¤ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
        
        Args:
            document_path: ê¸°ì¤€ì´ ë˜ëŠ” ë¬¸ì„œ ê²½ë¡œ
            top_k: ë°˜í™˜í•  ê´€ë ¨ ë¬¸ì„œ ìˆ˜
            include_centrality_boost: ì¤‘ì‹¬ì„± ì ìˆ˜ë¥¼ ë°˜ì˜í• ì§€ ì—¬ë¶€
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ê´€ë ¨ ë¬¸ì„œ ëª©ë¡ (SearchResult)
        """
        try:
            if not self.indexed:
                logger.warning("ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            # ê¸°ì¤€ ë¬¸ì„œ ì°¾ê¸°
            base_document = None
            for doc in self.documents:
                if doc.path == document_path or doc.title == document_path:
                    base_document = doc
                    break
            
            if base_document is None:
                logger.warning(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_path}")
                return []
            
            # ê¸°ì¤€ ë¬¸ì„œì˜ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
            base_cached = self.cache.get_embedding(base_document.path)
            if base_cached is None:
                logger.warning(f"ë¬¸ì„œì— ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤: {document_path}")
                return []
            base_embedding = base_cached.embedding
            
            # ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ì˜µì…˜)
            centrality_scores = {}
            if include_centrality_boost:
                centrality_scores = self._get_centrality_scores()
            
            # ìœ ì‚¬ë„ ê¸°ë°˜ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
            related_results = []
            base_embedding = base_embedding.reshape(1, -1)
            
            for doc in self.documents:
                # ìê¸° ìì‹  ì œì™¸
                if doc.path == base_document.path:
                    continue
                    
                # ë¬¸ì„œì˜ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
                doc_cached = self.cache.get_embedding(doc.path)
                if doc_cached is None:
                    continue
                doc_embedding = doc_cached.embedding
                
                # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
                doc_embedding = doc_embedding.reshape(1, -1)
                if SKLEARN_AVAILABLE:
                    similarity = cosine_similarity(base_embedding, doc_embedding)[0][0]
                else:
                    # NumPyë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    similarity = np.dot(base_embedding[0], doc_embedding[0]) / (
                        np.linalg.norm(base_embedding[0]) * np.linalg.norm(doc_embedding[0])
                    )
                
                if similarity < similarity_threshold:
                    continue
                
                # íƒœê·¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¶”ê°€
                tag_boost = self._calculate_tag_similarity(base_document, doc)
                
                # ì¤‘ì‹¬ì„± ì ìˆ˜ ê°€ì¤‘ì¹˜ (ì˜µì…˜)
                centrality_boost = 0.0
                if include_centrality_boost and doc.path in centrality_scores:
                    centrality_boost = centrality_scores[doc.path] * 0.2  # 20% ê°€ì¤‘ì¹˜
                
                # ìµœì¢… ì ìˆ˜ ê³„ì‚°
                final_score = similarity + tag_boost + centrality_boost
                
                # SearchResult ìƒì„±
                result = SearchResult(
                    document=doc,
                    similarity_score=final_score,
                    match_type="related_semantic",
                    matched_keywords=[],
                    snippet=doc.content[:150] + "..." if len(doc.content) > 150 else doc.content
                )
                
                related_results.append(result)
            
            # ì ìˆ˜ë³„ ì •ë ¬ ë° ìƒìœ„ Kê°œ ì„ íƒ
            related_results.sort(key=lambda x: x.similarity_score, reverse=True)
            final_results = related_results[:top_k]
            
            # ìˆœìœ„ í• ë‹¹
            for rank, result in enumerate(final_results):
                result.rank = rank + 1
            
            logger.info(f"ê´€ë ¨ ë¬¸ì„œ ì¶”ì²œ ì™„ë£Œ: {base_document.title}ì— ëŒ€í•œ {len(final_results)}ê°œ ë¬¸ì„œ")
            return final_results
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ ë¬¸ì„œ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            return []
    
    def search_with_related(
        self,
        query: str,
        search_method: str = "hybrid", 
        top_k: int = 10,
        include_related: int = 3,
        **search_kwargs
    ) -> Tuple[List[SearchResult], List[SearchResult]]:
        """
        ê²€ìƒ‰ ê²°ê³¼ì™€ í•¨ê»˜ ê´€ë ¨ ë¬¸ì„œë“¤ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            search_method: ê²€ìƒ‰ ë°©ë²•
            top_k: ì£¼ìš” ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            include_related: ê° ê²°ê³¼ë³„ ê´€ë ¨ ë¬¸ì„œ ìˆ˜
            **search_kwargs: ì¶”ê°€ ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜
            
        Returns:
            (ì£¼ìš” ê²€ìƒ‰ ê²°ê³¼, ê´€ë ¨ ë¬¸ì„œ ëª©ë¡) íŠœí”Œ
        """
        try:
            # ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰
            if search_method == "semantic":
                main_results = self.semantic_search(query, top_k, **search_kwargs)
            elif search_method == "keyword":
                main_results = self.keyword_search(query, top_k, **search_kwargs)
            elif search_method == "hybrid":
                main_results = self.hybrid_search(query, top_k, **search_kwargs)
            elif search_method == "colbert":
                main_results = self.colbert_search(query, top_k, **search_kwargs)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ë²•: {search_method}")
            
            if not main_results:
                return [], []
            
            # ìƒìœ„ ê²°ê³¼ë“¤ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘
            all_related = []
            seen_docs = {result.document.path for result in main_results}  # ì¤‘ë³µ ë°©ì§€
            
            for main_result in main_results[:3]:  # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
                related_docs = self.get_related_documents(
                    main_result.document.path, 
                    top_k=include_related + 2,  # ì—¬ìœ ë¶„ ì¶”ê°€
                    similarity_threshold=0.2  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ ë¬¸ì„œ í¬í•¨
                )
                
                # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
                for related in related_docs:
                    if related.document.path not in seen_docs:
                        # ê´€ë ¨ ë¬¸ì„œì„ì„ í‘œì‹œ
                        related.match_type = f"related_to_{main_result.document.title[:20]}"
                        all_related.append(related)
                        seen_docs.add(related.document.path)
                        
                        if len(all_related) >= include_related * 3:  # ì ì ˆí•œ ìˆ˜ì¤€ì—ì„œ ì¤‘ë‹¨
                            break
                
                if len(all_related) >= include_related * 3:
                    break
            
            # ê´€ë ¨ ë¬¸ì„œë“¤ì„ ì ìˆ˜ë³„ë¡œ ì¬ì •ë ¬
            all_related.sort(key=lambda x: x.similarity_score, reverse=True)
            final_related = all_related[:include_related * 2]  # ìµœì¢… ê´€ë ¨ ë¬¸ì„œ ìˆ˜
            
            # ìˆœìœ„ ì¬í• ë‹¹
            for rank, result in enumerate(final_related):
                result.rank = rank + 1
            
            logger.info(f"ê²€ìƒ‰+ê´€ë ¨ë¬¸ì„œ ì™„ë£Œ: ì£¼ìš” {len(main_results)}ê°œ, ê´€ë ¨ {len(final_related)}ê°œ")
            return main_results, final_related
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰+ê´€ë ¨ë¬¸ì„œ ì‹¤íŒ¨: {e}")
            return [], []
    
    def _get_centrality_scores(self) -> Dict[str, float]:
        """ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ì¤‘ì‹¬ì„± ì ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            from .knowledge_graph import KnowledgeGraphBuilder
            
            # ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• (ë‚®ì€ ì„ê³„ê°’ ì‚¬ìš©)
            graph_config = self.config.get('graph', {})
            if 'min_word_count' not in graph_config:
                graph_config['min_word_count'] = 5  # ë‚®ì€ ì„ê³„ê°’ ì„¤ì •
            graph_builder = KnowledgeGraphBuilder(self, graph_config)
            knowledge_graph = graph_builder.build_graph()
            
            if knowledge_graph.centrality_scores:
                # ë…¸ë“œ IDë¥¼ ë¬¸ì„œ ê²½ë¡œë¡œ ë§¤í•‘
                path_scores = {}
                for node in knowledge_graph.nodes:
                    if node.node_type == "document":
                        node_id = node.id
                        centrality = knowledge_graph.centrality_scores.get(node_id, 0.0)
                        path_scores[node.path] = centrality
                
                return path_scores
            
            return {}
            
        except Exception as e:
            logger.debug(f"ì¤‘ì‹¬ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _calculate_tag_similarity(self, doc1: Document, doc2: Document) -> float:
        """ë‘ ë¬¸ì„œ ê°„ì˜ íƒœê·¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if not doc1.tags or not doc2.tags:
            return 0.0
        
        tags1 = set(doc1.tags)
        tags2 = set(doc2.tags)
        
        # Jaccard ìœ ì‚¬ë„
        intersection = len(tags1 & tags2)
        union = len(tags1 | tags2)
        
        if union == 0:
            return 0.0
        
        # íƒœê·¸ ìœ ì‚¬ë„ì— ê°€ì¤‘ì¹˜ ì ìš© (ìµœëŒ€ 0.2)
        tag_similarity = (intersection / union) * 0.2
        
        return tag_similarity
    
    def search_with_centrality_boost(
        self,
        query: str,
        search_method: str = "hybrid",
        top_k: int = 10,
        centrality_weight: float = 0.2,
        **search_kwargs
    ) -> List[SearchResult]:
        """
        ì¤‘ì‹¬ì„± ì ìˆ˜ë¥¼ ë°˜ì˜í•œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            search_method: ê²€ìƒ‰ ë°©ë²• ("semantic", "keyword", "hybrid", "colbert")
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            centrality_weight: ì¤‘ì‹¬ì„± ì ìˆ˜ ê°€ì¤‘ì¹˜ (0.0 - 1.0)
            **search_kwargs: ì¶”ê°€ ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜
            
        Returns:
            ì¤‘ì‹¬ì„± ì ìˆ˜ê°€ ë°˜ì˜ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰
            if search_method == "semantic":
                results = self.semantic_search(query, top_k * 2, **search_kwargs)
            elif search_method == "keyword":
                results = self.keyword_search(query, top_k * 2, **search_kwargs)
            elif search_method == "hybrid":
                results = self.hybrid_search(query, top_k * 2, **search_kwargs)
            elif search_method == "colbert":
                results = self.colbert_search(query, top_k * 2, **search_kwargs)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ë²•: {search_method}")
            
            if not results:
                return []
            
            # ì¤‘ì‹¬ì„± ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
            centrality_scores = self._get_centrality_scores()
            
            if not centrality_scores:
                logger.info("ì¤‘ì‹¬ì„± ì ìˆ˜ê°€ ì—†ì–´ ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return results[:top_k]
            
            # ê²°ê³¼ì— ì¤‘ì‹¬ì„± ì ìˆ˜ ì ìš©
            boosted_results = []
            for result in results:
                doc_path = result.document.path
                centrality_boost = centrality_scores.get(doc_path, 0.0) * centrality_weight
                
                # ìƒˆë¡œìš´ ì ìˆ˜ = ì›ë˜ ì ìˆ˜ + ì¤‘ì‹¬ì„± ë¶€ìŠ¤íŠ¸
                new_score = result.similarity_score + centrality_boost
                
                # ìƒˆë¡œìš´ SearchResult ìƒì„±
                boosted_result = SearchResult(
                    document=result.document,
                    similarity_score=new_score,
                    match_type=f"{result.match_type}_centrality_boosted",
                    matched_keywords=result.matched_keywords,
                    snippet=result.snippet,
                    rank=0  # ì¬ì •ë ¬ í›„ í• ë‹¹
                )
                
                boosted_results.append(boosted_result)
            
            # ìƒˆë¡œìš´ ì ìˆ˜ë¡œ ì¬ì •ë ¬
            boosted_results.sort(key=lambda x: x.similarity_score, reverse=True)
            final_results = boosted_results[:top_k]
            
            # ìˆœìœ„ ì¬í• ë‹¹
            for rank, result in enumerate(final_results):
                result.rank = rank + 1
            
            # ìˆœìœ„ ë³€í™” ë¡œê¹…
            original_order = [r.document.title for r in results[:top_k]]
            new_order = [r.document.title for r in final_results]
            
            if original_order != new_order:
                logger.info(f"ì¤‘ì‹¬ì„± ë¶€ìŠ¤íŒ…ìœ¼ë¡œ ìˆœìœ„ ë³€ê²½ë¨:")
                for i, (orig, new) in enumerate(zip(original_order, new_order)):
                    if orig != new:
                        logger.info(f"  ìˆœìœ„ {i+1}: {orig} â†’ {new}")
            
            logger.info(f"ì¤‘ì‹¬ì„± ë¶€ìŠ¤íŒ… ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼")
            return final_results
            
        except Exception as e:
            logger.error(f"ì¤‘ì‹¬ì„± ë¶€ìŠ¤íŒ… ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # í´ë°±: ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜
            if search_method == "semantic":
                return self.semantic_search(query, top_k, **search_kwargs)
            elif search_method == "keyword":
                return self.keyword_search(query, top_k, **search_kwargs)
            elif search_method == "hybrid":
                return self.hybrid_search(query, top_k, **search_kwargs)
            elif search_method == "colbert":
                return self.colbert_search(query, top_k, **search_kwargs)
            else:
                return []
    
    def analyze_knowledge_gaps(
        self,
        similarity_threshold: float = 0.3,
        min_connections: int = 2
    ) -> Dict[str, any]:
        """
        ì§€ì‹ ê³µë°±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            similarity_threshold: ì—°ê²° íŒì • ìœ ì‚¬ë„ ì„ê³„ê°’
            min_connections: ìµœì†Œ ì—°ê²° ìˆ˜ (ì´ë³´ë‹¤ ì ìœ¼ë©´ ê³ ë¦½ìœ¼ë¡œ íŒì •)
            
        Returns:
            ì§€ì‹ ê³µë°± ë¶„ì„ ê²°ê³¼
        """
        try:
            if not self.indexed:
                logger.warning("ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return {}
            
            # ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•
            centrality_scores = self._get_centrality_scores()
            
            # ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            doc_similarities = {}
            isolated_docs = []
            weakly_connected_docs = []
            
            for i, doc1 in enumerate(self.documents):
                if doc1.embedding is None:
                    continue
                
                connections = 0
                related_docs = []
                
                for j, doc2 in enumerate(self.documents):
                    if i == j or doc2.embedding is None:
                        continue
                    
                    # ìœ ì‚¬ë„ ê³„ì‚°
                    if SKLEARN_AVAILABLE:
                        similarity = cosine_similarity(
                            doc1.embedding.reshape(1, -1),
                            doc2.embedding.reshape(1, -1)
                        )[0][0]
                    else:
                        similarity = np.dot(doc1.embedding, doc2.embedding) / (
                            np.linalg.norm(doc1.embedding) * np.linalg.norm(doc2.embedding)
                        )
                    
                    if similarity >= similarity_threshold:
                        connections += 1
                        related_docs.append({
                            'title': doc2.title,
                            'similarity': float(similarity)
                        })
                
                doc_similarities[doc1.title] = {
                    'connections': connections,
                    'related_docs': related_docs,
                    'centrality': centrality_scores.get(doc1.path, 0.0)
                }
                
                # ê³ ë¦½/ì•½í•œ ì—°ê²° ë¬¸ì„œ ë¶„ë¥˜
                if connections == 0:
                    isolated_docs.append({
                        'title': doc1.title,
                        'path': doc1.path,
                        'word_count': doc1.word_count,
                        'tags': doc1.tags or []
                    })
                elif connections < min_connections:
                    weakly_connected_docs.append({
                        'title': doc1.title,
                        'path': doc1.path,
                        'connections': connections,
                        'word_count': doc1.word_count,
                        'tags': doc1.tags or []
                    })
            
            # íƒœê·¸ë³„ ë¬¸ì„œ ë¶„í¬ ë¶„ì„
            tag_distribution = {}
            for doc in self.documents:
                if doc.tags:
                    for tag in doc.tags:
                        if tag not in tag_distribution:
                            tag_distribution[tag] = []
                        tag_distribution[tag].append(doc.title)
            
            # ê³ ë¦½ëœ íƒœê·¸ ì°¾ê¸°
            isolated_tags = {
                tag: docs for tag, docs in tag_distribution.items()
                if len(docs) == 1
            }
            
            # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
            analysis_result = {
                'total_documents': len(self.documents),
                'isolated_documents': isolated_docs,
                'weakly_connected_documents': weakly_connected_docs,
                'isolated_tags': isolated_tags,
                'tag_distribution': tag_distribution,
                'document_similarities': doc_similarities,
                'summary': {
                    'isolated_count': len(isolated_docs),
                    'weakly_connected_count': len(weakly_connected_docs),
                    'isolated_tag_count': len(isolated_tags),
                    'total_tags': len(tag_distribution),
                    'isolation_rate': len(isolated_docs) / len(self.documents) if self.documents else 0
                }
            }
            
            logger.info(f"ì§€ì‹ ê³µë°± ë¶„ì„ ì™„ë£Œ:")
            logger.info(f"  - ê³ ë¦½ ë¬¸ì„œ: {len(isolated_docs)}ê°œ")
            logger.info(f"  - ì•½í•œ ì—°ê²° ë¬¸ì„œ: {len(weakly_connected_docs)}ê°œ")
            logger.info(f"  - ê³ ë¦½ íƒœê·¸: {len(isolated_tags)}ê°œ")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"ì§€ì‹ ê³µë°± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}


def test_search_engine():
    """ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    import tempfile
    import shutil
    
    try:
        # ì„ì‹œ vault ìƒì„±
        temp_vault = tempfile.mkdtemp()
        temp_cache = tempfile.mkdtemp()
        
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤ ìƒì„±
        test_docs = [
            ("tdd.md", "# TDD ê¸°ë³¸ ê°œë…\n\nTDDëŠ” í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.\n## Red-Green-Refactor\ní…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì‘ì„±í•˜ê³  êµ¬í˜„í•©ë‹ˆë‹¤."),
            ("refactoring.md", "# ë¦¬íŒ©í† ë§\n\në¦¬íŒ©í† ë§ì€ ì½”ë“œì˜ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\ní…ŒìŠ¤íŠ¸ê°€ ìˆì–´ì•¼ ì•ˆì „í•˜ê²Œ ë¦¬íŒ©í† ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
            ("clean_code.md", "# Clean Code\n\nê¹¨ë—í•œ ì½”ë“œ ì‘ì„±ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\nì¢‹ì€ í…ŒìŠ¤íŠ¸ëŠ” ê¹¨ë—í•œ ì½”ë“œì˜ ê¸°ë°˜ì…ë‹ˆë‹¤.")
        ]
        
        for filename, content in test_docs:
            with open(Path(temp_vault) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        config = {
            "model": {"name": "paraphrase-multilingual-mpnet-base-v2"},
            "vault": {"excluded_dirs": [], "file_extensions": [".md"]}
        }
        
        engine = AdvancedSearchEngine(temp_vault, temp_cache, config)
        
        print("ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        if not engine.build_index():
            print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
            return False
        
        # ì˜ë¯¸ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\nğŸ” ì˜ë¯¸ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: 'í…ŒìŠ¤íŠ¸ ë°©ë²•ë¡ '")
        results = engine.semantic_search("í…ŒìŠ¤íŠ¸ ë°©ë²•ë¡ ", top_k=3)
        for result in results:
            print(f"  {result.rank}. {result.document.title} (ìœ ì‚¬ë„: {result.similarity_score:.4f})")
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸  
        print("\nğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: 'TDD ë¦¬íŒ©í† ë§'")
        results = engine.keyword_search("TDD ë¦¬íŒ©í† ë§", top_k=3)
        for result in results:
            print(f"  {result.rank}. {result.document.title} (ì ìˆ˜: {result.similarity_score:.2f}) - {result.matched_keywords}")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\nğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: 'í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±'")
        results = engine.hybrid_search("í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±", top_k=3)
        for result in results:
            print(f"  {result.rank}. {result.document.title} (ì ìˆ˜: {result.similarity_score:.4f})")
        
        # í†µê³„ í™•ì¸
        stats = engine.get_search_statistics()
        print(f"\nğŸ“Š í†µê³„: {stats['indexed_documents']}ê°œ ë¬¸ì„œ, {stats['embedding_dimension']}ì°¨ì›")
        
        # ì •ë¦¬
        shutil.rmtree(temp_vault)
        shutil.rmtree(temp_cache)
        
        print("âœ… ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì—”ì§„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_search_engine()