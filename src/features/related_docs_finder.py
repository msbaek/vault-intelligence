#!/usr/bin/env python3
"""
Related Documents Finder for Vault Intelligence System V2

íŠ¹ì • Obsidian ë¬¸ì„œì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•„ "ê´€ë ¨ ë¬¸ì„œ" ì„¹ì…˜ì„ ìë™ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import re
import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime

from ..core.vault_processor import Document
from .advanced_search import AdvancedSearchEngine, SearchResult

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class RelatedDocResult:
    """ê´€ë ¨ ë¬¸ì„œ ê²°ê³¼"""
    target_file_path: str
    related_docs: List[SearchResult]
    success: bool
    error_message: str = ""
    processing_time: float = 0.0
    section_added: bool = False
    existing_section_updated: bool = False


class RelatedDocsFinder:
    """ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° ë° ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        search_engine: AdvancedSearchEngine,
        config: Dict = None
    ):
        """
        Args:
            search_engine: êµ¬ì¶•ëœ AdvancedSearchEngine ì¸ìŠ¤í„´ìŠ¤
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.search_engine = search_engine
        self.config = config or {}
        
        # ê¸°ë³¸ ì„¤ì •
        self.default_top_k = self.config.get('related_docs', {}).get('default_top_k', 5)
        self.default_threshold = self.config.get('related_docs', {}).get('default_threshold', 0.3)
        self.section_title = self.config.get('related_docs', {}).get('section_title', '## ê´€ë ¨ ë¬¸ì„œ')
        self.show_similarity = self.config.get('related_docs', {}).get('show_similarity', True)
        self.show_snippet = self.config.get('related_docs', {}).get('show_snippet', False)
        
        logger.info("ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def find_related_docs(
        self,
        file_path: str,
        top_k: int = None,
        threshold: float = None,
        include_centrality: bool = True
    ) -> List[SearchResult]:
        """
        íŠ¹ì • ë¬¸ì„œì˜ ê´€ë ¨ ë¬¸ì„œë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.
        
        Args:
            file_path: ëŒ€ìƒ ë¬¸ì„œ ê²½ë¡œ
            top_k: ë°˜í™˜í•  ê´€ë ¨ ë¬¸ì„œ ìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            include_centrality: ì¤‘ì‹¬ì„± ì ìˆ˜ í¬í•¨ ì—¬ë¶€
            
        Returns:
            ê´€ë ¨ ë¬¸ì„œ ëª©ë¡ (SearchResult)
        """
        try:
            if top_k is None:
                top_k = self.default_top_k
            if threshold is None:
                threshold = self.default_threshold
            
            logger.info(f"ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰: {file_path}")
            
            # ê¸°ì¡´ AdvancedSearchEngineì˜ get_related_documents ë©”ì„œë“œ í™œìš©
            related_results = self.search_engine.get_related_documents(
                document_path=file_path,
                top_k=top_k,
                include_centrality_boost=include_centrality,
                similarity_threshold=threshold
            )
            
            logger.info(f"ê´€ë ¨ ë¬¸ì„œ {len(related_results)}ê°œ ë°œê²¬")
            return related_results
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def format_related_section(
        self,
        related_docs: List[SearchResult],
        show_similarity: bool = None,
        show_snippet: bool = None,
        format_style: str = "detailed"
    ) -> str:
        """
        ê´€ë ¨ ë¬¸ì„œ ëª©ë¡ì„ ë§ˆí¬ë‹¤ìš´ ì„¹ì…˜ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
        
        Args:
            related_docs: ê´€ë ¨ ë¬¸ì„œ ëª©ë¡
            show_similarity: ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ ì—¬ë¶€
            show_snippet: ìŠ¤ë‹ˆí« í‘œì‹œ ì—¬ë¶€  
            format_style: í¬ë§· ìŠ¤íƒ€ì¼ ("simple", "detailed")
            
        Returns:
            ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜
        """
        try:
            if not related_docs:
                return ""
            
            if show_similarity is None:
                show_similarity = self.show_similarity
            if show_snippet is None:
                show_snippet = self.show_snippet
            
            lines = [self.section_title, ""]
            
            for result in related_docs:
                doc = result.document
                
                # ìœ„í‚¤ë§í¬ í˜•ì‹ìœ¼ë¡œ ë¬¸ì„œ ì œëª© ìƒì„±
                link_text = f"[[{doc.title}]]"
                
                if format_style == "simple":
                    # ê°„ë‹¨í•œ í˜•ì‹: - [[ë¬¸ì„œì œëª©]]
                    lines.append(f"- {link_text}")
                    
                elif format_style == "detailed":
                    # ìƒì„¸ í˜•ì‹: - [[ë¬¸ì„œì œëª©]] (ìœ ì‚¬ë„: 0.75)
                    line = f"- {link_text}"
                    
                    if show_similarity:
                        line += f" (ìœ ì‚¬ë„: {result.similarity_score:.3f})"
                    
                    lines.append(line)
                    
                    # ìŠ¤ë‹ˆí« ì¶”ê°€ (ì„ íƒì )
                    if show_snippet and result.snippet:
                        # ìŠ¤ë‹ˆí«ì„ ë“¤ì—¬ì“°ê¸°ë¡œ ì¶”ê°€
                        snippet_lines = result.snippet.split('\n')
                        for snippet_line in snippet_lines:
                            if snippet_line.strip():
                                lines.append(f"  - {snippet_line.strip()}")
                    
                    # íƒœê·¸ ì •ë³´ ì¶”ê°€ (ì„ íƒì )
                    if doc.tags and len(doc.tags) > 0:
                        tags_str = ", ".join([f"#{tag}" for tag in doc.tags[:3]])  # ìµœëŒ€ 3ê°œê¹Œì§€
                        lines.append(f"  - íƒœê·¸: {tags_str}")
            
            lines.append("")  # ë§ˆì§€ë§‰ ë¹ˆ ì¤„
            return "\n".join(lines)
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            return ""
    
    def update_document(
        self,
        file_path: str,
        related_docs: List[SearchResult] = None,
        top_k: int = None,
        threshold: float = None,
        update_existing: bool = True,
        backup: bool = False,
        dry_run: bool = False
    ) -> RelatedDocResult:
        """
        ë¬¸ì„œì— ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ì„ ì¶”ê°€í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            file_path: ëŒ€ìƒ ë¬¸ì„œ ê²½ë¡œ
            related_docs: ë¯¸ë¦¬ ê³„ì‚°ëœ ê´€ë ¨ ë¬¸ì„œ (Noneì´ë©´ ìƒˆë¡œ ê³„ì‚°)
            top_k: ê´€ë ¨ ë¬¸ì„œ ìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            update_existing: ê¸°ì¡´ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì—¬ë¶€
            backup: ë°±ì—… ìƒì„± ì—¬ë¶€
            dry_run: ì‹¤ì œ ìˆ˜ì • ì—†ì´ ë¯¸ë¦¬ë³´ê¸°ë§Œ
            
        Returns:
            RelatedDocResult: ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = datetime.now()
        
        try:
            file_path = Path(file_path)
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not file_path.exists():
                return RelatedDocResult(
                    target_file_path=str(file_path),
                    related_docs=[],
                    success=False,
                    error_message=f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}"
                )
            
            # ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° (í•„ìš”í•œ ê²½ìš°)
            if related_docs is None:
                related_docs = self.find_related_docs(
                    str(file_path), 
                    top_k=top_k, 
                    threshold=threshold
                )
            
            if not related_docs:
                return RelatedDocResult(
                    target_file_path=str(file_path),
                    related_docs=[],
                    success=False,
                    error_message="ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                )
            
            # íŒŒì¼ ë‚´ìš© ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ë°±ì—… ìƒì„± (ì˜µì…˜)
            if backup and not dry_run:
                backup_path = file_path.with_suffix(f'.backup.{datetime.now().strftime("%Y%m%d_%H%M%S")}.md')
                with open(backup_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                logger.info(f"ë°±ì—… ìƒì„±: {backup_path}")
            
            # ê¸°ì¡´ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ì°¾ê¸°
            section_pattern = r'^## ê´€ë ¨ ë¬¸ì„œ.*?(?=^##|\Z)'
            existing_match = re.search(section_pattern, content, re.MULTILINE | re.DOTALL)
            
            # ìƒˆ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ìƒì„±
            new_section = self.format_related_section(related_docs)
            
            existing_section_updated = False
            section_added = False
            
            if existing_match:
                if update_existing:
                    # ê¸°ì¡´ ì„¹ì…˜ êµì²´
                    new_content = re.sub(section_pattern, new_section, content, flags=re.MULTILINE | re.DOTALL)
                    existing_section_updated = True
                    logger.info("ê¸°ì¡´ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ì„ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤")
                else:
                    logger.info("ê¸°ì¡´ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ì´ ìˆì–´ ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    return RelatedDocResult(
                        target_file_path=str(file_path),
                        related_docs=related_docs,
                        success=True,
                        error_message="ê¸°ì¡´ ì„¹ì…˜ ì¡´ì¬ (ì—…ë°ì´íŠ¸ ì•ˆí•¨)",
                        processing_time=(datetime.now() - start_time).total_seconds()
                    )
            else:
                # íŒŒì¼ ëì— ìƒˆ ì„¹ì…˜ ì¶”ê°€
                new_content = content.rstrip() + "\n\n" + new_section
                section_added = True
                logger.info("ìƒˆë¡œìš´ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤")
            
            # ì‹¤ì œ íŒŒì¼ ì“°ê¸° (dry_runì´ ì•„ë‹Œ ê²½ìš°)
            if not dry_run:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                logger.info(f"íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {file_path}")
            else:
                logger.info(f"[DRY RUN] íŒŒì¼ ì—…ë°ì´íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {file_path}")
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return RelatedDocResult(
                target_file_path=str(file_path),
                related_docs=related_docs,
                success=True,
                processing_time=processing_time,
                section_added=section_added,
                existing_section_updated=existing_section_updated
            )
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return RelatedDocResult(
                target_file_path=str(file_path),
                related_docs=related_docs or [],
                success=False,
                error_message=str(e),
                processing_time=processing_time
            )
    
    def batch_process(
        self,
        file_patterns: List[str],
        top_k: int = None,
        threshold: float = None,
        update_existing: bool = True,
        backup: bool = False,
        dry_run: bool = False,
        progress_callback=None
    ) -> List[RelatedDocResult]:
        """
        ì—¬ëŸ¬ ë¬¸ì„œì— ëŒ€í•´ ì¼ê´„ì ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ì„ ì¶”ê°€/ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            file_patterns: íŒŒì¼ íŒ¨í„´ ëª©ë¡ (glob íŒ¨í„´ ì§€ì›)
            top_k: ê´€ë ¨ ë¬¸ì„œ ìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            update_existing: ê¸°ì¡´ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì—¬ë¶€
            backup: ë°±ì—… ìƒì„± ì—¬ë¶€
            dry_run: ì‹¤ì œ ìˆ˜ì • ì—†ì´ ë¯¸ë¦¬ë³´ê¸°ë§Œ
            progress_callback: ì§„í–‰ë¥  ì½œë°±
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ëª©ë¡
        """
        try:
            # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
            all_files = []
            vault_path = Path(self.search_engine.vault_path)
            
            for pattern in file_patterns:
                if "/" in pattern:
                    # ê²½ë¡œê°€ í¬í•¨ëœ íŒ¨í„´
                    pattern_path = Path(pattern)
                    if pattern_path.is_absolute():
                        matches = list(Path("/").glob(str(pattern_path.relative_to("/"))))
                    else:
                        matches = list(vault_path.glob(pattern))
                else:
                    # íŒŒì¼ëª…ë§Œ ìˆëŠ” íŒ¨í„´ - vault ì „ì²´ì—ì„œ ê²€ìƒ‰
                    matches = list(vault_path.rglob(pattern))
                
                # .md íŒŒì¼ë§Œ í•„í„°ë§
                md_files = [f for f in matches if f.suffix.lower() == '.md']
                all_files.extend(md_files)
            
            # ì¤‘ë³µ ì œê±°
            all_files = list(set(all_files))
            
            logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ëŒ€ìƒ: {len(all_files)}ê°œ íŒŒì¼")
            
            if not all_files:
                logger.warning("ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            results = []
            
            for i, file_path in enumerate(all_files):
                try:
                    logger.info(f"ì²˜ë¦¬ ì¤‘ ({i+1}/{len(all_files)}): {file_path.name}")
                    
                    # ê° íŒŒì¼ ì²˜ë¦¬
                    result = self.update_document(
                        str(file_path),
                        top_k=top_k,
                        threshold=threshold,
                        update_existing=update_existing,
                        backup=backup,
                        dry_run=dry_run
                    )
                    
                    results.append(result)
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback:
                        progress_callback(i + 1, len(all_files))
                
                except Exception as e:
                    logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                    results.append(RelatedDocResult(
                        target_file_path=str(file_path),
                        related_docs=[],
                        success=False,
                        error_message=str(e)
                    ))
            
            # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
            successful = sum(1 for r in results if r.success)
            logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {successful}/{len(results)}ê°œ ì„±ê³µ")
            
            return results
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    def analyze_related_docs_coverage(
        self,
        file_patterns: List[str] = None
    ) -> Dict[str, any]:
        """
        ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ì»¤ë²„ë¦¬ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            file_patterns: ë¶„ì„í•  íŒŒì¼ íŒ¨í„´ ëª©ë¡ (Noneì´ë©´ ì „ì²´ vault)
            
        Returns:
            ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ê²°ê³¼
        """
        try:
            vault_path = Path(self.search_engine.vault_path)
            
            # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
            if file_patterns is None:
                all_files = list(vault_path.rglob("*.md"))
            else:
                all_files = []
                for pattern in file_patterns:
                    matches = list(vault_path.rglob(pattern))
                    md_files = [f for f in matches if f.suffix.lower() == '.md']
                    all_files.extend(md_files)
                all_files = list(set(all_files))
            
            logger.info(f"ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ëŒ€ìƒ: {len(all_files)}ê°œ íŒŒì¼")
            
            has_related_section = 0
            no_related_section = 0
            empty_related_section = 0
            
            files_with_sections = []
            files_without_sections = []
            
            section_pattern = r'^## ê´€ë ¨ ë¬¸ì„œ.*?(?=^##|\Z)'
            
            for file_path in all_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    match = re.search(section_pattern, content, re.MULTILINE | re.DOTALL)
                    
                    if match:
                        section_content = match.group(0)
                        # ì‹¤ì œ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
                        if re.search(r'\[\[.*?\]\]', section_content):
                            has_related_section += 1
                            files_with_sections.append(str(file_path.relative_to(vault_path)))
                        else:
                            empty_related_section += 1
                    else:
                        no_related_section += 1
                        files_without_sections.append(str(file_path.relative_to(vault_path)))
                
                except Exception as e:
                    logger.warning(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")
                    continue
            
            total_files = len(all_files)
            coverage_rate = has_related_section / total_files if total_files > 0 else 0
            
            analysis_result = {
                'total_files': total_files,
                'has_related_section': has_related_section,
                'no_related_section': no_related_section,
                'empty_related_section': empty_related_section,
                'coverage_rate': coverage_rate,
                'files_with_sections': files_with_sections,
                'files_without_sections': files_without_sections[:20]  # ìµœëŒ€ 20ê°œë§Œ í‘œì‹œ
            }
            
            logger.info(f"ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì™„ë£Œ:")
            logger.info(f"  - ì´ íŒŒì¼: {total_files}ê°œ")
            logger.info(f"  - ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ìˆìŒ: {has_related_section}ê°œ")
            logger.info(f"  - ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ ì—†ìŒ: {no_related_section}ê°œ")
            logger.info(f"  - ë¹ˆ ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜: {empty_related_section}ê°œ")
            logger.info(f"  - ì»¤ë²„ë¦¬ì§€: {coverage_rate:.1%}")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}


def test_related_docs_finder():
    """ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    import tempfile
    import shutil
    
    try:
        # ì„ì‹œ vault ìƒì„±
        temp_vault = tempfile.mkdtemp()
        temp_cache = tempfile.mkdtemp()
        
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤ ìƒì„±
        test_docs = [
            ("tdd-basics.md", """# TDD ê¸°ë³¸ ê°œë…

TDD(Test-Driven Development)ëŠ” í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.

## Red-Green-Refactor ì‚¬ì´í´
1. Red: ì‹¤íŒ¨í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì‘ì„±
2. Green: í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í•˜ëŠ” ìµœì†Œí•œì˜ ì½”ë“œ ì‘ì„±  
3. Refactor: ì½”ë“œ ê°œì„ 

#development #tdd #testing"""),
            
            ("refactoring-guide.md", """# ë¦¬íŒ©í† ë§ ê°€ì´ë“œ

ë¦¬íŒ©í† ë§ì€ ì½”ë“œì˜ êµ¬ì¡°ë¥¼ ê°œì„ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

## ì•ˆì „í•œ ë¦¬íŒ©í† ë§
í…ŒìŠ¤íŠ¸ê°€ ìˆì–´ì•¼ ì•ˆì „í•˜ê²Œ ë¦¬íŒ©í† ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
TDDì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤.

#development #refactoring #testing"""),
            
            ("clean-code.md", """# Clean Code ì›ì¹™

ê¹¨ë—í•œ ì½”ë“œ ì‘ì„±ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## í•µì‹¬ ì›ì¹™
- ì˜ë¯¸ ìˆëŠ” ì´ë¦„
- ì‘ì€ í•¨ìˆ˜
- ì¢‹ì€ ì£¼ì„

ì¢‹ì€ í…ŒìŠ¤íŠ¸ëŠ” ê¹¨ë—í•œ ì½”ë“œì˜ ê¸°ë°˜ì…ë‹ˆë‹¤.

#development #clean-code #best-practices""")
        ]
        
        for filename, content in test_docs:
            with open(Path(temp_vault) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        config = {
            "model": {"name": "paraphrase-multilingual-mpnet-base-v2"},
            "vault": {"excluded_dirs": [], "file_extensions": [".md"]}
        }
        
        from .advanced_search import AdvancedSearchEngine
        search_engine = AdvancedSearchEngine(temp_vault, temp_cache, config)
        
        print("ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        if not search_engine.build_index():
            print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨")
            return False
        
        # ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        finder = RelatedDocsFinder(search_engine, config)
        
        # í…ŒìŠ¤íŠ¸ 1: ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        print("\nğŸ” ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° í…ŒìŠ¤íŠ¸")
        target_file = str(Path(temp_vault) / "tdd-basics.md")
        related_docs = finder.find_related_docs(target_file, top_k=3)
        
        print(f"'{target_file}'ì˜ ê´€ë ¨ ë¬¸ì„œ:")
        for doc in related_docs:
            print(f"  - {doc.document.title} (ìœ ì‚¬ë„: {doc.similarity_score:.3f})")
        
        # í…ŒìŠ¤íŠ¸ 2: ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ í¬ë§·íŒ…
        print("\nğŸ“ ì„¹ì…˜ í¬ë§·íŒ… í…ŒìŠ¤íŠ¸")
        section = finder.format_related_section(related_docs, format_style="detailed")
        print("ìƒì„±ëœ ì„¹ì…˜:")
        print(section)
        
        # í…ŒìŠ¤íŠ¸ 3: ë¬¸ì„œ ì—…ë°ì´íŠ¸ (ë“œë¼ì´ëŸ°)
        print("\nğŸ“„ ë¬¸ì„œ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸ (ë“œë¼ì´ëŸ°)")
        result = finder.update_document(target_file, related_docs, dry_run=True)
        print(f"ê²°ê³¼: ì„±ê³µ={result.success}, ì²˜ë¦¬ì‹œê°„={result.processing_time:.2f}ì´ˆ")
        
        # í…ŒìŠ¤íŠ¸ 4: ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        print("\nğŸ“Š ì»¤ë²„ë¦¬ì§€ ë¶„ì„ í…ŒìŠ¤íŠ¸")
        coverage = finder.analyze_related_docs_coverage()
        print(f"ì»¤ë²„ë¦¬ì§€: {coverage.get('coverage_rate', 0):.1%}")
        
        # ì •ë¦¬
        shutil.rmtree(temp_vault)
        shutil.rmtree(temp_cache)
        
        print("âœ… ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_related_docs_finder()